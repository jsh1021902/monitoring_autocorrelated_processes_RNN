{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pytorchtools import EarlyStopping\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA 사용 및 EarlyStopping 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "early_stopping = EarlyStopping(patience = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맥북 M2 MAX는 NVIDEA GPU를 사용하지 않아 CUDA 지원 안 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    torch.save(model.state_dict(), f'../model_b_arma11_l24_v5.pt') # 모델의 학습된 매개변수 파일에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "length = 24            # 윈도우 사이즈 (생성할 시계열 데이터의 길이)\n",
    "hidden_size1 = 48      # 은닉층 1 크기\n",
    "hidden_size2 = 24      # 은닉층 2 크기\n",
    "hidden_size3 = 12      # 은닉층 3 크기\n",
    "hidden_size4 = 6       # 은닉층 4 크기\n",
    "learning_rate = 1e-6   # 학습률\n",
    "epoch = 400\n",
    "trainrun = 50          # 생성할 학습 데이터 시퀀스의 수 \n",
    "testrun = 25           # 생성할 평가 데이터 시퀀스의 수 \n",
    "validrun = 25          # 생성할 검증 데이터 시퀀스의 수 \n",
    "\n",
    "# 시계열 데이터 생성을 위한 매개변수\n",
    "# 자기상관계수 (phi값을 0.2, 0.2, 0.4, 0.8, 0.8별로 생성)\n",
    "phi1 = np.array([np.repeat(0.2,8),\n",
    "                 np.repeat(0.2,8),\n",
    "                 np.repeat(0.4,8),\n",
    "                 np.repeat(0.8,8),\n",
    "                 np.repeat(0.8,8)])\n",
    "phi1 = np.concatenate(phi1)\n",
    "\n",
    "# theta 값을 -0.2, -0.8, -0.4, -0.2, -0.8별로 생성\n",
    "theta = np.array([np.repeat(-0.2,8),\n",
    "                 np.repeat(-0.8,8),\n",
    "                 np.repeat(-0.4,8),\n",
    "                 np.repeat(-0.2,8),\n",
    "                 np.repeat(-0.8,8)])\n",
    "theta = np.concatenate(theta)\n",
    "\n",
    "# 변화율 크기 (= 이상상태 포함 정도, psi) * 논문과 수치가 약간 변동이 있음\n",
    "psi1 = np.array([0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,])\n",
    "\n",
    "# 공정의 수준 변화율 (delta)\n",
    "de1 = np.array([0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "                0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "                0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "                0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "                0, 0.5, 1, 2, 3, 0, 0, 0,])\n",
    "\n",
    "# 공정의 분산 변화율 (gamma)\n",
    "ga = np.array([1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시계열 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# AR(1) 시계열 데이터 생성 함수\n",
    "def arma(phi1, theta, delta, gamma, psi, length, run):\n",
    "    # 초기 설정\n",
    "    y = np.zeros(shape=(run, length))                           # 생성될 시계열 데이터를 저장할 빈 배열을 초기화. 배열의 크기는 (생성할 데이터 시퀀스의 수, 각 시퀀스의 길이) \n",
    "    sigma = math.sqrt((1 + 2 * phi1 * theta + pow(theta, 2)) / (1 - pow(phi1, 2)))                    # AR(1)모델의 표준 편차\n",
    "    \n",
    "    # 데이터 시퀀스 생성\n",
    "    for j in range(0, run):                                     # 각 run 마다 랜덤 노이즈(e)를 정규분포에서 추출하여 시계열의 기본 노이즈 생성 (과적합 방지 차원)\n",
    "        e = np.random.normal(loc=0, scale=1, size=length)       \n",
    "        x = np.array(np.repeat(0, length), dtype=np.float64)\n",
    "        \n",
    "        x[0] = e[0]                                             # x 배열 초기화하고, 첫 번째 시점의 값은 첫 번째 노이즈 값으로 설정 (시계열의 시작점에서 발생할 수 있는 임의성 반영 및 자기상관 구조 구현)\n",
    "\n",
    "        # psi 시점 이전의 데이터 생성\n",
    "        for i in range(1, psi):                                 # psi 시점 이전까지는 관리상태 데이터\n",
    "            x[i] = phi1 * x[i - 1] + e[i] - theta * e[i-1]      # 각 시점에서의 값은 이전 시점의 값에 자기상관 계수 ar1을 곱한 것과 현재 시점의 노이즈를 더한 값으로 설정\n",
    "            \n",
    "        # psi 시점 이후의 데이터 생성 및 변동성 적용\n",
    "        for i in range(psi,len(x)):                             # psi 시점 이후에는 각 에러 항에 gamma 값을 곱하여 에러 항의 변동성을 조절 \n",
    "            e[i] = gamma * e[i]\n",
    "            x[i] = phi1 * x[i-1] + e[i] - theta * e[i-1]\n",
    "        for i in range(psi,len(x)):                             # delta(변동성 크기 조절하는 매개변수)를 통한 추가 변동성 적용\n",
    "            x[i] = x[i] + delta*sigma\n",
    "        \n",
    "        # 최종 데이터 반환 (각 run에 대해 생성된 시계열 데이터를 저장)  \n",
    "        y[j] = x\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# 다양한 매개변수 조합에 대한 시계열 데이터 세트 생성\n",
    "def totaldat(run,length):\n",
    "    # 빈 데이터 배열 초기화\n",
    "    y = np.zeros(shape=(len(phi1), run, length))\n",
    "    # 매개변수 조합별 데이터 생성\n",
    "    for i in range(len(phi1)):\n",
    "        y[i]= arma(phi1[i], theta[i], de1[i], ga[i], psi1[i], length, run)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# 훈련용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "train_x = totaldat(trainrun,length)                             # 훈련용 시계열 데이터 생성\n",
    "train_x = train_x.reshape(trainrun*len(phi1),length)            # 생성된 훈련용 데이터를 적절한 형태로 재배열\n",
    "\n",
    "# 레이블 생성 및 변형\n",
    "train_y =  [np.repeat(0,trainrun),np.repeat(1,trainrun*7),      # 관리상태(1가지)는 0, 이상상태(7가지)는 1로 가정\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),]\n",
    "train_y =  np.concatenate(train_y)\n",
    "train_y = train_y.reshape(2000,1)                               # 최종 레이블 배열의 형태를 조정 (학습용 데이터 세트 2000개)\n",
    "\n",
    "# PyTorch 텐서로 변환 및 장치 할당\n",
    "# train_x = torch.FloatTensor(train_x).to(device)\n",
    "# train_y = torch.FloatTensor(train_y).to(device)\n",
    "\n",
    "\n",
    "# 평가용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "test_x = totaldat(run = testrun, length = length)\n",
    "test_x = test_x.reshape(testrun*len(phi1),length)\n",
    "\n",
    "# 레이블 생성 및 변형\n",
    "test_y = [np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),]\n",
    "test_y = np.concatenate(test_y)\n",
    "test_y = test_y.reshape(1000,1)                                 # 최종 레이블 배열의 형태를 조정 (평가용 데이터 세트 1000개)\n",
    "\n",
    "# PyTorch 텐서로 변환 및 장치 할당\n",
    "# test_x = torch.FloatTensor(test_x).to(device)\n",
    "# test_y = torch.FloatTensor(test_y).to(device)\n",
    "\n",
    "\n",
    "# 검증용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "valid_x = totaldat(run = validrun, length = length)\n",
    "valid_x = valid_x.reshape(validrun*len(phi1),length)\n",
    "\n",
    "# 레이블 생성 및 변형\n",
    "valid_y = [np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),]\n",
    "valid_y = np.concatenate(valid_y)\n",
    "valid_y = valid_y.reshape(1000,1)                                 # 최종 레이블 배열의 형태를 조정 (검증용 데이터 세트 1000개)\n",
    "\n",
    "\n",
    "# PyTorch 텐서로 변환 및 장치 할당\n",
    "train_x = torch.FloatTensor(train_x).to(device)\n",
    "train_y = torch.FloatTensor(train_y).to(device)\n",
    "test_x = torch.FloatTensor(test_x).to(device)\n",
    "test_y = torch.FloatTensor(test_y).to(device)\n",
    "valid_x = torch.FloatTensor(valid_x).to(device)\n",
    "valid_y = torch.FloatTensor(valid_y).to(device)\n",
    "\n",
    "# DataLoader 설정\n",
    "trainset = TensorDataset(train_x, train_y)                        # 데이터와 레이블 쌍을 포함하는 데이터셋 생성\n",
    "trainloader = DataLoader(trainset, shuffle=True)                  # 데이터셋에서 미니배치 자동으로 생성 후 모델 학습 및 평가 시 배치 처리를 용이하게 함 (훈련에서는 데이터를 섞어 학습 과정에서의 일반화 능력 향상)\n",
    "testset = TensorDataset(test_x, test_y)\n",
    "testloader = DataLoader(testset,shuffle = False)                  # 학습 및 검증에서는 데이터 순서 유지\n",
    "validset = TensorDataset(valid_x, valid_y)\n",
    "validloader = DataLoader(validset,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    # 클래스 초기화\n",
    "    def __init__(self, input_size, hidden_size, num_layers, device):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.device = device                                          \n",
    "        self.hidden_size = hidden_size                                # RNN 모델의 은닉층 크기 \n",
    "        self.num_layers = num_layers                                  # RNN 모델의 층 개수\n",
    "        \n",
    "        # 기본 RNN 레이어 생성\n",
    "        self.rnn = nn.RNN(input_size= input_size,\n",
    "                          hidden_size = hidden_size1,\n",
    "                          num_layers = num_layers,\n",
    "                          nonlinearity= \"relu\",                       # 활성화 함수로 relu를 사용\n",
    "                          batch_first= True)                          # 입력 텐서의 첫 번째 차원이 배치 크기임을 나타냄\n",
    "        \n",
    "        # 완전 연결 레이어 (여러 레이어를 연속적으로 적용할 수 있게 하기 위해 nn.Sequential을 사용)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size,hidden_size2),  # 선형 레이어\n",
    "                                nn.GELU(),                            # GELU 활성화 함수\n",
    "                                nn.Linear(hidden_size2,hidden_size3),\n",
    "                                nn.GELU(),\n",
    "                                nn.Linear(hidden_size3,hidden_size4),\n",
    "                                nn.GELU(),\n",
    "                                nn.Linear(hidden_size4,1),\n",
    "                                nn.Sigmoid()                          # 시그모이드 활성화함수를 사용해 출력을 [0, 1] 범위로 조정\n",
    "                                )\n",
    "\n",
    "    # 순전파 (forward)\n",
    "    def forward(self, x):\n",
    "        # 초기 hidden state 설정\n",
    "        h0 = torch.zeros(x.size()[0], self.hidden_size).to(device)    # 각 배치에 대한 초기 은닉층을 0으로 설정 (= RNN의 첫 번째 시점에서 이전 상태가 없음을 의미)\n",
    "        \n",
    "        # RNN 레이어 실행\n",
    "        out, _ = self.rnn(x, h0)                                      # 입력 데이터 x와 초기 은닉층 h0를 RNN 층에 전달함. out: RNN의 마지막 레이어로부터 나온 output feature 를 반환 (hn: hidden state를 반환)\n",
    "        \n",
    "        # 데이터 재구성 및 완전 연결 레이어 실행\n",
    "        out = out.reshape(out.shape[0], -1)                           # many to many 전략 : 시퀀스의 각 타임 스텝에 대응하는 출력 생성 \n",
    "        out = self.fc(out)                                            # out을 재구성하여 모든 시간 단계의 출력을 하나의 벡터로 평탄화하고 완전 연결 레이어를 통해 최종 예측값을 계산\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "model = NeuralNetwork(input_size = length, hidden_size = hidden_size1, num_layers = 1, device = device).to(device)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 손실 기록을 위한 리스트 초기화\n",
    "loss_ = []                                                            # 훈련 과정에서의 손실값 기록\n",
    "n = len(trainloader)\n",
    "valoss_ = []                                                          # 검증 과정에서의 손실값 기록\n",
    "logger = {\"train_loss\": list(),\n",
    "          \"validation_loss\": list()\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "train loss : 0.22502085939794778, validation loss : 0.22458135436475277\n",
      "Validation loss decreased (inf --> 0.224581).  Saving model ...\n",
      "epoch 2\n",
      "train loss : 0.22455048925802112, validation loss : 0.22411351966857912\n",
      "Validation loss decreased (0.224581 --> 0.224114).  Saving model ...\n",
      "epoch 3\n",
      "train loss : 0.22408226210375629, validation loss : 0.22364683355391027\n",
      "Validation loss decreased (0.224114 --> 0.223647).  Saving model ...\n",
      "epoch 4\n",
      "train loss : 0.22361620122566822, validation loss : 0.22318125458434226\n",
      "Validation loss decreased (0.223647 --> 0.223181).  Saving model ...\n",
      "epoch 5\n",
      "train loss : 0.22315113574415446, validation loss : 0.22271648085415366\n",
      "Validation loss decreased (0.223181 --> 0.222716).  Saving model ...\n",
      "epoch 6\n",
      "train loss : 0.22268632160375515, validation loss : 0.22225176403423152\n",
      "Validation loss decreased (0.222716 --> 0.222252).  Saving model ...\n",
      "epoch 7\n",
      "train loss : 0.22222126597166061, validation loss : 0.2217865736995425\n",
      "Validation loss decreased (0.222252 --> 0.221787).  Saving model ...\n",
      "epoch 8\n",
      "train loss : 0.2217556372601539, validation loss : 0.22132042151130737\n",
      "Validation loss decreased (0.221787 --> 0.221320).  Saving model ...\n",
      "epoch 9\n",
      "train loss : 0.22128911130047507, validation loss : 0.2208530609243446\n",
      "Validation loss decreased (0.221320 --> 0.220853).  Saving model ...\n",
      "epoch 10\n",
      "train loss : 0.22082141387611628, validation loss : 0.22038475529104473\n",
      "Validation loss decreased (0.220853 --> 0.220385).  Saving model ...\n",
      "epoch 11\n",
      "train loss : 0.22035241041129286, validation loss : 0.21991498902981935\n",
      "Validation loss decreased (0.220385 --> 0.219915).  Saving model ...\n",
      "epoch 12\n",
      "train loss : 0.21988180048701664, validation loss : 0.2194433765759071\n",
      "Validation loss decreased (0.219915 --> 0.219443).  Saving model ...\n",
      "epoch 13\n",
      "train loss : 0.21940948357089207, validation loss : 0.21897017490176057\n",
      "Validation loss decreased (0.219443 --> 0.218970).  Saving model ...\n",
      "epoch 14\n",
      "train loss : 0.21893522131549462, validation loss : 0.2184949462445719\n",
      "Validation loss decreased (0.218970 --> 0.218495).  Saving model ...\n",
      "epoch 15\n",
      "train loss : 0.21845872243841488, validation loss : 0.2180172699302435\n",
      "Validation loss decreased (0.218495 --> 0.218017).  Saving model ...\n",
      "epoch 16\n",
      "train loss : 0.21797981889965012, validation loss : 0.21753719810489563\n",
      "Validation loss decreased (0.218017 --> 0.217537).  Saving model ...\n",
      "epoch 17\n",
      "train loss : 0.21749836505467401, validation loss : 0.21705438686819636\n",
      "Validation loss decreased (0.217537 --> 0.217054).  Saving model ...\n",
      "epoch 18\n",
      "train loss : 0.2170141301846339, validation loss : 0.2165687169300185\n",
      "Validation loss decreased (0.217054 --> 0.216569).  Saving model ...\n",
      "epoch 19\n",
      "train loss : 0.21652703718488156, validation loss : 0.2160801969915628\n",
      "Validation loss decreased (0.216569 --> 0.216080).  Saving model ...\n",
      "epoch 20\n",
      "train loss : 0.2160368878480047, validation loss : 0.2155884948529303\n",
      "Validation loss decreased (0.216080 --> 0.215588).  Saving model ...\n",
      "epoch 21\n",
      "train loss : 0.21554350301126637, validation loss : 0.21509355094035468\n",
      "Validation loss decreased (0.215588 --> 0.215094).  Saving model ...\n",
      "epoch 22\n",
      "train loss : 0.21504675532301717, validation loss : 0.2145951651239937\n",
      "Validation loss decreased (0.215094 --> 0.214595).  Saving model ...\n",
      "epoch 23\n",
      "train loss : 0.21454640957648338, validation loss : 0.214093070615245\n",
      "Validation loss decreased (0.214595 --> 0.214093).  Saving model ...\n",
      "epoch 24\n",
      "train loss : 0.21404241912284247, validation loss : 0.21358722777105865\n",
      "Validation loss decreased (0.214093 --> 0.213587).  Saving model ...\n",
      "epoch 25\n",
      "train loss : 0.2135346214681864, validation loss : 0.21307763255178927\n",
      "Validation loss decreased (0.213587 --> 0.213078).  Saving model ...\n",
      "epoch 26\n",
      "train loss : 0.21302278065910707, validation loss : 0.21256377169661797\n",
      "Validation loss decreased (0.213078 --> 0.212564).  Saving model ...\n",
      "epoch 27\n",
      "train loss : 0.2125065303913973, validation loss : 0.21204542721256062\n",
      "Validation loss decreased (0.212564 --> 0.212045).  Saving model ...\n",
      "epoch 28\n",
      "train loss : 0.21198568390602515, validation loss : 0.2115222526980298\n",
      "Validation loss decreased (0.212045 --> 0.211522).  Saving model ...\n",
      "epoch 29\n",
      "train loss : 0.21146009188530776, validation loss : 0.21099433784700672\n",
      "Validation loss decreased (0.211522 --> 0.210994).  Saving model ...\n",
      "epoch 30\n",
      "train loss : 0.2109294755034149, validation loss : 0.21046128565967084\n",
      "Validation loss decreased (0.210994 --> 0.210461).  Saving model ...\n",
      "epoch 31\n",
      "train loss : 0.21039372906112866, validation loss : 0.20992312952539616\n",
      "Validation loss decreased (0.210461 --> 0.209923).  Saving model ...\n",
      "epoch 32\n",
      "train loss : 0.20985273718764072, validation loss : 0.20937955855485052\n",
      "Validation loss decreased (0.209923 --> 0.209380).  Saving model ...\n",
      "epoch 33\n",
      "train loss : 0.20930638885746397, validation loss : 0.20883058771129812\n",
      "Validation loss decreased (0.209380 --> 0.208831).  Saving model ...\n",
      "epoch 34\n",
      "train loss : 0.20875465870276091, validation loss : 0.20827618829511543\n",
      "Validation loss decreased (0.208831 --> 0.208276).  Saving model ...\n",
      "epoch 35\n",
      "train loss : 0.20819731993313345, validation loss : 0.2077161581294877\n",
      "Validation loss decreased (0.208276 --> 0.207716).  Saving model ...\n",
      "epoch 36\n",
      "train loss : 0.20763440856772164, validation loss : 0.2071504871038099\n",
      "Validation loss decreased (0.207716 --> 0.207150).  Saving model ...\n",
      "epoch 37\n",
      "train loss : 0.20706594781456764, validation loss : 0.20657933503771955\n",
      "Validation loss decreased (0.207150 --> 0.206579).  Saving model ...\n",
      "epoch 38\n",
      "train loss : 0.20649195651317895, validation loss : 0.2060027051128839\n",
      "Validation loss decreased (0.206579 --> 0.206003).  Saving model ...\n",
      "epoch 39\n",
      "train loss : 0.20591236605504762, validation loss : 0.20542036537520394\n",
      "Validation loss decreased (0.206003 --> 0.205420).  Saving model ...\n",
      "epoch 40\n",
      "train loss : 0.20532729936130342, validation loss : 0.2048325198078528\n",
      "Validation loss decreased (0.205420 --> 0.204833).  Saving model ...\n",
      "epoch 41\n",
      "train loss : 0.20473677820921304, validation loss : 0.20423938253812673\n",
      "Validation loss decreased (0.204833 --> 0.204239).  Saving model ...\n",
      "epoch 42\n",
      "train loss : 0.2041409285135035, validation loss : 0.20364089636362734\n",
      "Validation loss decreased (0.204239 --> 0.203641).  Saving model ...\n",
      "epoch 43\n",
      "train loss : 0.20353983034522724, validation loss : 0.2030371831187675\n",
      "Validation loss decreased (0.203641 --> 0.203037).  Saving model ...\n",
      "epoch 44\n",
      "train loss : 0.20293360912291838, validation loss : 0.20242848146266557\n",
      "Validation loss decreased (0.203037 --> 0.202428).  Saving model ...\n",
      "epoch 45\n",
      "train loss : 0.20232232047807838, validation loss : 0.20181467038591702\n",
      "Validation loss decreased (0.202428 --> 0.201815).  Saving model ...\n",
      "epoch 46\n",
      "train loss : 0.20170608711088803, validation loss : 0.20119611756762731\n",
      "Validation loss decreased (0.201815 --> 0.201196).  Saving model ...\n",
      "epoch 47\n",
      "train loss : 0.20108504241451303, validation loss : 0.20057269506108888\n",
      "Validation loss decreased (0.201196 --> 0.200573).  Saving model ...\n",
      "epoch 48\n",
      "train loss : 0.20045939577215663, validation loss : 0.1999449035053452\n",
      "Validation loss decreased (0.200573 --> 0.199945).  Saving model ...\n",
      "epoch 49\n",
      "train loss : 0.19982929429259835, validation loss : 0.1993127420535501\n",
      "Validation loss decreased (0.199945 --> 0.199313).  Saving model ...\n",
      "epoch 50\n",
      "train loss : 0.19919487376637757, validation loss : 0.19867645905271172\n",
      "Validation loss decreased (0.199313 --> 0.198676).  Saving model ...\n",
      "epoch 51\n",
      "train loss : 0.19855629731554025, validation loss : 0.19803609418021698\n",
      "Validation loss decreased (0.198676 --> 0.198036).  Saving model ...\n",
      "epoch 52\n",
      "train loss : 0.19791373075817068, validation loss : 0.19739198798027177\n",
      "Validation loss decreased (0.198036 --> 0.197392).  Saving model ...\n",
      "epoch 53\n",
      "train loss : 0.19726731454503707, validation loss : 0.19674418574494293\n",
      "Validation loss decreased (0.197392 --> 0.196744).  Saving model ...\n",
      "epoch 54\n",
      "train loss : 0.1966173522486179, validation loss : 0.1960930773433022\n",
      "Validation loss decreased (0.196744 --> 0.196093).  Saving model ...\n",
      "epoch 55\n",
      "train loss : 0.1959640252975578, validation loss : 0.19543876942145552\n",
      "Validation loss decreased (0.196093 --> 0.195439).  Saving model ...\n",
      "epoch 56\n",
      "train loss : 0.19530747597132408, validation loss : 0.19478148343580376\n",
      "Validation loss decreased (0.195439 --> 0.194781).  Saving model ...\n",
      "epoch 57\n",
      "train loss : 0.1946479036834156, validation loss : 0.19412128267326115\n",
      "Validation loss decreased (0.194781 --> 0.194121).  Saving model ...\n",
      "epoch 58\n",
      "train loss : 0.19398554124037637, validation loss : 0.19345861175258097\n",
      "Validation loss decreased (0.194121 --> 0.193459).  Saving model ...\n",
      "epoch 59\n",
      "train loss : 0.1933206561305639, validation loss : 0.1927937756551398\n",
      "Validation loss decreased (0.193459 --> 0.192794).  Saving model ...\n",
      "epoch 60\n",
      "train loss : 0.19265348194977266, validation loss : 0.19212684494821977\n",
      "Validation loss decreased (0.192794 --> 0.192127).  Saving model ...\n",
      "epoch 61\n",
      "train loss : 0.19198423637379508, validation loss : 0.19145815915034198\n",
      "Validation loss decreased (0.192127 --> 0.191458).  Saving model ...\n",
      "epoch 62\n",
      "train loss : 0.19131324138729683, validation loss : 0.19078809755815254\n",
      "Validation loss decreased (0.191458 --> 0.190788).  Saving model ...\n",
      "epoch 63\n",
      "train loss : 0.1906407084964393, validation loss : 0.19011679793645941\n",
      "Validation loss decreased (0.190788 --> 0.190117).  Saving model ...\n",
      "epoch 64\n",
      "train loss : 0.1899669188595144, validation loss : 0.18944451024374576\n",
      "Validation loss decreased (0.190117 --> 0.189445).  Saving model ...\n",
      "epoch 65\n",
      "train loss : 0.18929225004639186, validation loss : 0.18877177203091292\n",
      "Validation loss decreased (0.189445 --> 0.188772).  Saving model ...\n",
      "epoch 66\n",
      "train loss : 0.18861687855143097, validation loss : 0.1880986026059554\n",
      "Validation loss decreased (0.188772 --> 0.188099).  Saving model ...\n",
      "epoch 67\n",
      "train loss : 0.18794116798229513, validation loss : 0.18742554029594385\n",
      "Validation loss decreased (0.188099 --> 0.187426).  Saving model ...\n",
      "epoch 68\n",
      "train loss : 0.18726539028845451, validation loss : 0.18675275564801824\n",
      "Validation loss decreased (0.187426 --> 0.186753).  Saving model ...\n",
      "epoch 69\n",
      "train loss : 0.18658983543809446, validation loss : 0.18608064248106454\n",
      "Validation loss decreased (0.186753 --> 0.186081).  Saving model ...\n",
      "epoch 70\n",
      "train loss : 0.1859149089980072, validation loss : 0.18540950766668787\n",
      "Validation loss decreased (0.186081 --> 0.185410).  Saving model ...\n",
      "epoch 71\n",
      "train loss : 0.18524083716801407, validation loss : 0.18473960080626092\n",
      "Validation loss decreased (0.185410 --> 0.184740).  Saving model ...\n",
      "epoch 72\n",
      "train loss : 0.1845680327218595, validation loss : 0.18407140402133682\n",
      "Validation loss decreased (0.184740 --> 0.184071).  Saving model ...\n",
      "epoch 73\n",
      "train loss : 0.18389676428262197, validation loss : 0.18340506138588775\n",
      "Validation loss decreased (0.184071 --> 0.183405).  Saving model ...\n",
      "epoch 74\n",
      "train loss : 0.18322733923045265, validation loss : 0.1827409639352228\n",
      "Validation loss decreased (0.183405 --> 0.182741).  Saving model ...\n",
      "epoch 75\n",
      "train loss : 0.18256001966445398, validation loss : 0.18207929347373544\n",
      "Validation loss decreased (0.182741 --> 0.182079).  Saving model ...\n",
      "epoch 76\n",
      "train loss : 0.18189513364734475, validation loss : 0.18142050452726452\n",
      "Validation loss decreased (0.182079 --> 0.181421).  Saving model ...\n",
      "epoch 77\n",
      "train loss : 0.181233077561126, validation loss : 0.18076493506322847\n",
      "Validation loss decreased (0.181421 --> 0.180765).  Saving model ...\n",
      "epoch 78\n",
      "train loss : 0.180574100062269, validation loss : 0.18011280275600106\n",
      "Validation loss decreased (0.180765 --> 0.180113).  Saving model ...\n",
      "epoch 79\n",
      "train loss : 0.17991851137032075, validation loss : 0.17946438745979834\n",
      "Validation loss decreased (0.180113 --> 0.179464).  Saving model ...\n",
      "epoch 80\n",
      "train loss : 0.17926652762025477, validation loss : 0.17881993741184704\n",
      "Validation loss decreased (0.179464 --> 0.178820).  Saving model ...\n",
      "epoch 81\n",
      "train loss : 0.17861835245534857, validation loss : 0.17817955048749637\n",
      "Validation loss decreased (0.178820 --> 0.178180).  Saving model ...\n",
      "epoch 82\n",
      "train loss : 0.1779743209446059, validation loss : 0.17754372219377335\n",
      "Validation loss decreased (0.178180 --> 0.177544).  Saving model ...\n",
      "epoch 83\n",
      "train loss : 0.17733465493300996, validation loss : 0.17691246601249966\n",
      "Validation loss decreased (0.177544 --> 0.176912).  Saving model ...\n",
      "epoch 84\n",
      "train loss : 0.17669956899674086, validation loss : 0.1762861413650348\n",
      "Validation loss decreased (0.176912 --> 0.176286).  Saving model ...\n",
      "epoch 85\n",
      "train loss : 0.17606930798467463, validation loss : 0.17566489988962902\n",
      "Validation loss decreased (0.176286 --> 0.175665).  Saving model ...\n",
      "epoch 86\n",
      "train loss : 0.17544399239139108, validation loss : 0.17504883960305667\n",
      "Validation loss decreased (0.175665 --> 0.175049).  Saving model ...\n",
      "epoch 87\n",
      "train loss : 0.1748238769244314, validation loss : 0.17443830323937062\n",
      "Validation loss decreased (0.175049 --> 0.174438).  Saving model ...\n",
      "epoch 88\n",
      "train loss : 0.17420915118547217, validation loss : 0.1738333786276079\n",
      "Validation loss decreased (0.174438 --> 0.173833).  Saving model ...\n",
      "epoch 89\n",
      "train loss : 0.1735999302381474, validation loss : 0.17323415006417517\n",
      "Validation loss decreased (0.173833 --> 0.173234).  Saving model ...\n",
      "epoch 90\n",
      "train loss : 0.1729963645060482, validation loss : 0.17264077906488026\n",
      "Validation loss decreased (0.173234 --> 0.172641).  Saving model ...\n",
      "epoch 91\n",
      "train loss : 0.17239854693841594, validation loss : 0.17205337203278173\n",
      "Validation loss decreased (0.172641 --> 0.172053).  Saving model ...\n",
      "epoch 92\n",
      "train loss : 0.17180668371385238, validation loss : 0.17147213244824303\n",
      "Validation loss decreased (0.172053 --> 0.171472).  Saving model ...\n",
      "epoch 93\n",
      "train loss : 0.17122082222911517, validation loss : 0.17089702566578777\n",
      "Validation loss decreased (0.171472 --> 0.170897).  Saving model ...\n",
      "epoch 94\n",
      "train loss : 0.1706409739430156, validation loss : 0.17032804829088669\n",
      "Validation loss decreased (0.170897 --> 0.170328).  Saving model ...\n",
      "epoch 95\n",
      "train loss : 0.17006726227619767, validation loss : 0.16976536463544104\n",
      "Validation loss decreased (0.170328 --> 0.169765).  Saving model ...\n",
      "epoch 96\n",
      "train loss : 0.1694997920072668, validation loss : 0.16920912778057726\n",
      "Validation loss decreased (0.169765 --> 0.169209).  Saving model ...\n",
      "epoch 97\n",
      "train loss : 0.16893863158612563, validation loss : 0.1686592986341302\n",
      "Validation loss decreased (0.169209 --> 0.168659).  Saving model ...\n",
      "epoch 98\n",
      "train loss : 0.16838376586017323, validation loss : 0.1681158541517097\n",
      "Validation loss decreased (0.168659 --> 0.168116).  Saving model ...\n",
      "epoch 99\n",
      "train loss : 0.1678352714611501, validation loss : 0.16757886258382973\n",
      "Validation loss decreased (0.168116 --> 0.167579).  Saving model ...\n",
      "epoch 100\n",
      "train loss : 0.16729312578166866, validation loss : 0.167048306302462\n",
      "Validation loss decreased (0.167579 --> 0.167048).  Saving model ...\n",
      "epoch 101\n",
      "train loss : 0.1667573863064025, validation loss : 0.1665242720030206\n",
      "Validation loss decreased (0.167048 --> 0.166524).  Saving model ...\n",
      "epoch 102\n",
      "train loss : 0.16622807093307276, validation loss : 0.16600670926208314\n",
      "Validation loss decreased (0.166524 --> 0.166007).  Saving model ...\n",
      "epoch 103\n",
      "train loss : 0.16570515325489638, validation loss : 0.16549560430173146\n",
      "Validation loss decreased (0.166007 --> 0.165496).  Saving model ...\n",
      "epoch 104\n",
      "train loss : 0.1651886002280469, validation loss : 0.16499091090664045\n",
      "Validation loss decreased (0.165496 --> 0.164991).  Saving model ...\n",
      "epoch 105\n",
      "train loss : 0.16467842775273794, validation loss : 0.16449262083939684\n",
      "Validation loss decreased (0.164991 --> 0.164493).  Saving model ...\n",
      "epoch 106\n",
      "train loss : 0.164174582474126, validation loss : 0.16400070545187903\n",
      "Validation loss decreased (0.164493 --> 0.164001).  Saving model ...\n",
      "epoch 107\n",
      "train loss : 0.16367705825555384, validation loss : 0.16351515172935735\n",
      "Validation loss decreased (0.164001 --> 0.163515).  Saving model ...\n",
      "epoch 108\n",
      "train loss : 0.16318582816921495, validation loss : 0.16303588957458265\n",
      "Validation loss decreased (0.163515 --> 0.163036).  Saving model ...\n",
      "epoch 109\n",
      "train loss : 0.16270084298335294, validation loss : 0.1625629079142166\n",
      "Validation loss decreased (0.163036 --> 0.162563).  Saving model ...\n",
      "epoch 110\n",
      "train loss : 0.16222205675231183, validation loss : 0.16209609987664814\n",
      "Validation loss decreased (0.162563 --> 0.162096).  Saving model ...\n",
      "epoch 111\n",
      "train loss : 0.16174941702391996, validation loss : 0.161635458542926\n",
      "Validation loss decreased (0.162096 --> 0.161635).  Saving model ...\n",
      "epoch 112\n",
      "train loss : 0.1612828846907352, validation loss : 0.16118091530310188\n",
      "Validation loss decreased (0.161635 --> 0.161181).  Saving model ...\n",
      "epoch 113\n",
      "train loss : 0.16082240119526164, validation loss : 0.16073240763547292\n",
      "Validation loss decreased (0.161181 --> 0.160732).  Saving model ...\n",
      "epoch 114\n",
      "train loss : 0.16036790767620834, validation loss : 0.16028988610340814\n",
      "Validation loss decreased (0.160732 --> 0.160290).  Saving model ...\n",
      "epoch 115\n",
      "train loss : 0.15991934412891587, validation loss : 0.15985327834739596\n",
      "Validation loss decreased (0.160290 --> 0.159853).  Saving model ...\n",
      "epoch 116\n",
      "train loss : 0.15947665371237202, validation loss : 0.15942249924399865\n",
      "Validation loss decreased (0.159853 --> 0.159422).  Saving model ...\n",
      "epoch 117\n",
      "train loss : 0.15903976364085104, validation loss : 0.1589975000053223\n",
      "Validation loss decreased (0.159422 --> 0.158998).  Saving model ...\n",
      "epoch 118\n",
      "train loss : 0.15860859634584457, validation loss : 0.15857818415908412\n",
      "Validation loss decreased (0.158998 --> 0.158578).  Saving model ...\n",
      "epoch 119\n",
      "train loss : 0.1581830834531297, validation loss : 0.15816451884460594\n",
      "Validation loss decreased (0.158578 --> 0.158165).  Saving model ...\n",
      "epoch 120\n",
      "train loss : 0.15776316173166574, validation loss : 0.1577564078322289\n",
      "Validation loss decreased (0.158165 --> 0.157756).  Saving model ...\n",
      "epoch 121\n",
      "train loss : 0.15734875946801077, validation loss : 0.15735378170935113\n",
      "Validation loss decreased (0.157756 --> 0.157354).  Saving model ...\n",
      "epoch 122\n",
      "train loss : 0.15693981164073142, validation loss : 0.1569565730846689\n",
      "Validation loss decreased (0.157354 --> 0.156957).  Saving model ...\n",
      "epoch 123\n",
      "train loss : 0.15653624537131566, validation loss : 0.1565646980638175\n",
      "Validation loss decreased (0.156957 --> 0.156565).  Saving model ...\n",
      "epoch 124\n",
      "train loss : 0.15613798052014327, validation loss : 0.15617807833742625\n",
      "Validation loss decreased (0.156565 --> 0.156178).  Saving model ...\n",
      "epoch 125\n",
      "train loss : 0.15574494651182963, validation loss : 0.15579664371445817\n",
      "Validation loss decreased (0.156178 --> 0.155797).  Saving model ...\n",
      "epoch 126\n",
      "train loss : 0.1553570631780514, validation loss : 0.15542029866559318\n",
      "Validation loss decreased (0.155797 --> 0.155420).  Saving model ...\n",
      "epoch 127\n",
      "train loss : 0.15497426622693472, validation loss : 0.1550490050758323\n",
      "Validation loss decreased (0.155420 --> 0.155049).  Saving model ...\n",
      "epoch 128\n",
      "train loss : 0.154596473005657, validation loss : 0.15468265809073828\n",
      "Validation loss decreased (0.155049 --> 0.154683).  Saving model ...\n",
      "epoch 129\n",
      "train loss : 0.15422360110809705, validation loss : 0.15432116708052068\n",
      "Validation loss decreased (0.154683 --> 0.154321).  Saving model ...\n",
      "epoch 130\n",
      "train loss : 0.15385558498138477, validation loss : 0.15396451093900007\n",
      "Validation loss decreased (0.154321 --> 0.153965).  Saving model ...\n",
      "epoch 131\n",
      "train loss : 0.153492349683332, validation loss : 0.15361257318440394\n",
      "Validation loss decreased (0.153965 --> 0.153613).  Saving model ...\n",
      "epoch 132\n",
      "train loss : 0.15313382513344304, validation loss : 0.15326530045444905\n",
      "Validation loss decreased (0.153613 --> 0.153265).  Saving model ...\n",
      "epoch 133\n",
      "train loss : 0.15277993764244954, validation loss : 0.15292260805566213\n",
      "Validation loss decreased (0.153265 --> 0.152923).  Saving model ...\n",
      "epoch 134\n",
      "train loss : 0.15243061711044953, validation loss : 0.15258443355836318\n",
      "Validation loss decreased (0.152923 --> 0.152584).  Saving model ...\n",
      "epoch 135\n",
      "train loss : 0.15208578977550427, validation loss : 0.15225069411871944\n",
      "Validation loss decreased (0.152584 --> 0.152251).  Saving model ...\n",
      "epoch 136\n",
      "train loss : 0.1517453832352269, validation loss : 0.15192131635920247\n",
      "Validation loss decreased (0.152251 --> 0.151921).  Saving model ...\n",
      "epoch 137\n",
      "train loss : 0.15140932539051344, validation loss : 0.15159624440970385\n",
      "Validation loss decreased (0.151921 --> 0.151596).  Saving model ...\n",
      "epoch 138\n",
      "train loss : 0.15107755127520028, validation loss : 0.15127540392508027\n",
      "Validation loss decreased (0.151596 --> 0.151275).  Saving model ...\n",
      "epoch 139\n",
      "train loss : 0.15074999130447742, validation loss : 0.1509587121755892\n",
      "Validation loss decreased (0.151275 --> 0.150959).  Saving model ...\n",
      "epoch 140\n",
      "train loss : 0.1504265763385543, validation loss : 0.1506461070832006\n",
      "Validation loss decreased (0.150959 --> 0.150646).  Saving model ...\n",
      "epoch 141\n",
      "train loss : 0.15010723803216117, validation loss : 0.1503375342347452\n",
      "Validation loss decreased (0.150646 --> 0.150338).  Saving model ...\n",
      "epoch 142\n",
      "train loss : 0.14979191272611012, validation loss : 0.1500329209249693\n",
      "Validation loss decreased (0.150338 --> 0.150033).  Saving model ...\n",
      "epoch 143\n",
      "train loss : 0.14948053261943683, validation loss : 0.1497321948909481\n",
      "Validation loss decreased (0.150033 --> 0.149732).  Saving model ...\n",
      "epoch 144\n",
      "train loss : 0.14917303541259588, validation loss : 0.1494352954352554\n",
      "Validation loss decreased (0.149732 --> 0.149435).  Saving model ...\n",
      "epoch 145\n",
      "train loss : 0.14886935459739314, validation loss : 0.1491421528127575\n",
      "Validation loss decreased (0.149435 --> 0.149142).  Saving model ...\n",
      "epoch 146\n",
      "train loss : 0.1485694277785077, validation loss : 0.1488527182442237\n",
      "Validation loss decreased (0.149142 --> 0.148853).  Saving model ...\n",
      "epoch 147\n",
      "train loss : 0.1482731950688331, validation loss : 0.1485669204511083\n",
      "Validation loss decreased (0.148853 --> 0.148567).  Saving model ...\n",
      "epoch 148\n",
      "train loss : 0.14798059673120822, validation loss : 0.14828470741941302\n",
      "Validation loss decreased (0.148567 --> 0.148285).  Saving model ...\n",
      "epoch 149\n",
      "train loss : 0.14769157086962492, validation loss : 0.1480060041648441\n",
      "Validation loss decreased (0.148285 --> 0.148006).  Saving model ...\n",
      "epoch 150\n",
      "train loss : 0.147406059801638, validation loss : 0.1477307758316515\n",
      "Validation loss decreased (0.148006 --> 0.147731).  Saving model ...\n",
      "epoch 151\n",
      "train loss : 0.14712400698443764, validation loss : 0.14745895060476036\n",
      "Validation loss decreased (0.147731 --> 0.147459).  Saving model ...\n",
      "epoch 152\n",
      "train loss : 0.1468453538428319, validation loss : 0.14719046482592663\n",
      "Validation loss decreased (0.147459 --> 0.147190).  Saving model ...\n",
      "epoch 153\n",
      "train loss : 0.14657003921100742, validation loss : 0.14692527565532892\n",
      "Validation loss decreased (0.147190 --> 0.146925).  Saving model ...\n",
      "epoch 154\n",
      "train loss : 0.14629801707474013, validation loss : 0.1466633237095385\n",
      "Validation loss decreased (0.146925 --> 0.146663).  Saving model ...\n",
      "epoch 155\n",
      "train loss : 0.14602922870424817, validation loss : 0.14640455320295542\n",
      "Validation loss decreased (0.146663 --> 0.146405).  Saving model ...\n",
      "epoch 156\n",
      "train loss : 0.14576362031823326, validation loss : 0.14614891151417198\n",
      "Validation loss decreased (0.146405 --> 0.146149).  Saving model ...\n",
      "epoch 157\n",
      "train loss : 0.14550114261574554, validation loss : 0.14589634931487347\n",
      "Validation loss decreased (0.146149 --> 0.145896).  Saving model ...\n",
      "epoch 158\n",
      "train loss : 0.14524174170559231, validation loss : 0.14564681560619158\n",
      "Validation loss decreased (0.145896 --> 0.145647).  Saving model ...\n",
      "epoch 159\n",
      "train loss : 0.144985368560185, validation loss : 0.14540026083111676\n",
      "Validation loss decreased (0.145647 --> 0.145400).  Saving model ...\n",
      "epoch 160\n",
      "train loss : 0.14473197184273534, validation loss : 0.1451566329924561\n",
      "Validation loss decreased (0.145400 --> 0.145157).  Saving model ...\n",
      "epoch 161\n",
      "train loss : 0.14448150536947643, validation loss : 0.14491588256552987\n",
      "Validation loss decreased (0.145157 --> 0.144916).  Saving model ...\n",
      "epoch 162\n",
      "train loss : 0.14423391965167448, validation loss : 0.14467797107434024\n",
      "Validation loss decreased (0.144916 --> 0.144678).  Saving model ...\n",
      "epoch 163\n",
      "train loss : 0.14398916998534866, validation loss : 0.14444284712906222\n",
      "Validation loss decreased (0.144678 --> 0.144443).  Saving model ...\n",
      "epoch 164\n",
      "train loss : 0.143747207595708, validation loss : 0.14421046532894805\n",
      "Validation loss decreased (0.144443 --> 0.144210).  Saving model ...\n",
      "epoch 165\n",
      "train loss : 0.14350798950964078, validation loss : 0.14398077885378152\n",
      "Validation loss decreased (0.144210 --> 0.143981).  Saving model ...\n",
      "epoch 166\n",
      "train loss : 0.14327146858579728, validation loss : 0.14375374551528558\n",
      "Validation loss decreased (0.143981 --> 0.143754).  Saving model ...\n",
      "epoch 167\n",
      "train loss : 0.14303760350944605, validation loss : 0.14352932417859085\n",
      "Validation loss decreased (0.143754 --> 0.143529).  Saving model ...\n",
      "epoch 168\n",
      "train loss : 0.14280634740899567, validation loss : 0.14330746838494124\n",
      "Validation loss decreased (0.143529 --> 0.143307).  Saving model ...\n",
      "epoch 169\n",
      "train loss : 0.1425776618126493, validation loss : 0.1430881394048546\n",
      "Validation loss decreased (0.143307 --> 0.143088).  Saving model ...\n",
      "epoch 170\n",
      "train loss : 0.14235150346866304, validation loss : 0.14287129239204832\n",
      "Validation loss decreased (0.143088 --> 0.142871).  Saving model ...\n",
      "epoch 171\n",
      "train loss : 0.14212783419308728, validation loss : 0.14265689117395797\n",
      "Validation loss decreased (0.142871 --> 0.142657).  Saving model ...\n",
      "epoch 172\n",
      "train loss : 0.14190661359862547, validation loss : 0.14244489991612538\n",
      "Validation loss decreased (0.142657 --> 0.142445).  Saving model ...\n",
      "epoch 173\n",
      "train loss : 0.14168780376799414, validation loss : 0.1422352772045927\n",
      "Validation loss decreased (0.142445 --> 0.142235).  Saving model ...\n",
      "epoch 174\n",
      "train loss : 0.14147136355692824, validation loss : 0.1420279827905297\n",
      "Validation loss decreased (0.142235 --> 0.142028).  Saving model ...\n",
      "epoch 175\n",
      "train loss : 0.14125725699962688, validation loss : 0.1418229802870488\n",
      "Validation loss decreased (0.142028 --> 0.141823).  Saving model ...\n",
      "epoch 176\n",
      "train loss : 0.14104544893468005, validation loss : 0.14162023850465805\n",
      "Validation loss decreased (0.141823 --> 0.141620).  Saving model ...\n",
      "epoch 177\n",
      "train loss : 0.14083590255712683, validation loss : 0.1414197137378559\n",
      "Validation loss decreased (0.141620 --> 0.141420).  Saving model ...\n",
      "epoch 178\n",
      "train loss : 0.1406285818566631, validation loss : 0.14122137933050805\n",
      "Validation loss decreased (0.141420 --> 0.141221).  Saving model ...\n",
      "epoch 179\n",
      "train loss : 0.1404234519758468, validation loss : 0.14102519774698982\n",
      "Validation loss decreased (0.141221 --> 0.141025).  Saving model ...\n",
      "epoch 180\n",
      "train loss : 0.14022048055618208, validation loss : 0.14083113236573983\n",
      "Validation loss decreased (0.141025 --> 0.140831).  Saving model ...\n",
      "epoch 181\n",
      "train loss : 0.14001962974025806, validation loss : 0.14063915636506436\n",
      "Validation loss decreased (0.140831 --> 0.140639).  Saving model ...\n",
      "epoch 182\n",
      "train loss : 0.13982087133525864, validation loss : 0.140449235646836\n",
      "Validation loss decreased (0.140639 --> 0.140449).  Saving model ...\n",
      "epoch 183\n",
      "train loss : 0.13962417318681383, validation loss : 0.1402613361548119\n",
      "Validation loss decreased (0.140449 --> 0.140261).  Saving model ...\n",
      "epoch 184\n",
      "train loss : 0.13942950147078872, validation loss : 0.1400754287960958\n",
      "Validation loss decreased (0.140261 --> 0.140075).  Saving model ...\n",
      "epoch 185\n",
      "train loss : 0.13923682607725293, validation loss : 0.13989147838272778\n",
      "Validation loss decreased (0.140075 --> 0.139891).  Saving model ...\n",
      "epoch 186\n",
      "train loss : 0.13904611552794371, validation loss : 0.13970946250667537\n",
      "Validation loss decreased (0.139891 --> 0.139709).  Saving model ...\n",
      "epoch 187\n",
      "train loss : 0.13885734082676313, validation loss : 0.13952934736347997\n",
      "Validation loss decreased (0.139709 --> 0.139529).  Saving model ...\n",
      "epoch 188\n",
      "train loss : 0.1386704733521154, validation loss : 0.13935110637183307\n",
      "Validation loss decreased (0.139529 --> 0.139351).  Saving model ...\n",
      "epoch 189\n",
      "train loss : 0.1384854829206042, validation loss : 0.13917470948798388\n",
      "Validation loss decreased (0.139351 --> 0.139175).  Saving model ...\n",
      "epoch 190\n",
      "train loss : 0.1383023408278983, validation loss : 0.13900012918249333\n",
      "Validation loss decreased (0.139175 --> 0.139000).  Saving model ...\n",
      "epoch 191\n",
      "train loss : 0.13812102134948284, validation loss : 0.13882733636907046\n",
      "Validation loss decreased (0.139000 --> 0.138827).  Saving model ...\n",
      "epoch 192\n",
      "train loss : 0.13794149533559344, validation loss : 0.1386563073230567\n",
      "Validation loss decreased (0.138827 --> 0.138656).  Saving model ...\n",
      "epoch 193\n",
      "train loss : 0.13776373689409083, validation loss : 0.13848701493000673\n",
      "Validation loss decreased (0.138656 --> 0.138487).  Saving model ...\n",
      "epoch 194\n",
      "train loss : 0.13758772138674424, validation loss : 0.13831943363265914\n",
      "Validation loss decreased (0.138487 --> 0.138319).  Saving model ...\n",
      "epoch 195\n",
      "train loss : 0.13741342279217122, validation loss : 0.13815353908181216\n",
      "Validation loss decreased (0.138319 --> 0.138154).  Saving model ...\n",
      "epoch 196\n",
      "train loss : 0.1372408139827963, validation loss : 0.1379893055891472\n",
      "Validation loss decreased (0.138154 --> 0.137989).  Saving model ...\n",
      "epoch 197\n",
      "train loss : 0.13706987490570366, validation loss : 0.13782670899118152\n",
      "Validation loss decreased (0.137989 --> 0.137827).  Saving model ...\n",
      "epoch 198\n",
      "train loss : 0.13690057790000038, validation loss : 0.13766572508572286\n",
      "Validation loss decreased (0.137827 --> 0.137666).  Saving model ...\n",
      "epoch 199\n",
      "train loss : 0.13673289693593427, validation loss : 0.13750633098942508\n",
      "Validation loss decreased (0.137666 --> 0.137506).  Saving model ...\n",
      "epoch 200\n",
      "train loss : 0.1365668118771848, validation loss : 0.13734850275761978\n",
      "Validation loss decreased (0.137506 --> 0.137349).  Saving model ...\n",
      "epoch 201\n",
      "train loss : 0.1364022988060061, validation loss : 0.1371922194425639\n",
      "Validation loss decreased (0.137349 --> 0.137192).  Saving model ...\n",
      "epoch 202\n",
      "train loss : 0.13623933657723114, validation loss : 0.1370374567857922\n",
      "Validation loss decreased (0.137192 --> 0.137037).  Saving model ...\n",
      "epoch 203\n",
      "train loss : 0.13607789998128575, validation loss : 0.13688419515656797\n",
      "Validation loss decreased (0.137037 --> 0.136884).  Saving model ...\n",
      "epoch 204\n",
      "train loss : 0.135917972675326, validation loss : 0.13673241122677315\n",
      "Validation loss decreased (0.136884 --> 0.136732).  Saving model ...\n",
      "epoch 205\n",
      "train loss : 0.13575952838758423, validation loss : 0.1365820853737433\n",
      "Validation loss decreased (0.136732 --> 0.136582).  Saving model ...\n",
      "epoch 206\n",
      "train loss : 0.13560254727803087, validation loss : 0.13643319543918989\n",
      "Validation loss decreased (0.136582 --> 0.136433).  Saving model ...\n",
      "epoch 207\n",
      "train loss : 0.13544700879533786, validation loss : 0.1362857232307898\n",
      "Validation loss decreased (0.136433 --> 0.136286).  Saving model ...\n",
      "epoch 208\n",
      "train loss : 0.13529289358265467, validation loss : 0.13613964772067327\n",
      "Validation loss decreased (0.136286 --> 0.136140).  Saving model ...\n",
      "epoch 209\n",
      "train loss : 0.13514018232954866, validation loss : 0.13599494871587894\n",
      "Validation loss decreased (0.136140 --> 0.135995).  Saving model ...\n",
      "epoch 210\n",
      "train loss : 0.13498885328615526, validation loss : 0.13585160655909964\n",
      "Validation loss decreased (0.135995 --> 0.135852).  Saving model ...\n",
      "epoch 211\n",
      "train loss : 0.13483888847255998, validation loss : 0.13570960373911603\n",
      "Validation loss decreased (0.135852 --> 0.135710).  Saving model ...\n",
      "epoch 212\n",
      "train loss : 0.13469026790747696, validation loss : 0.13556892254263714\n",
      "Validation loss decreased (0.135710 --> 0.135569).  Saving model ...\n",
      "epoch 213\n",
      "train loss : 0.1345429732647827, validation loss : 0.13542954384054304\n",
      "Validation loss decreased (0.135569 --> 0.135430).  Saving model ...\n",
      "epoch 214\n",
      "train loss : 0.13439698865822436, validation loss : 0.13529145014018037\n",
      "Validation loss decreased (0.135430 --> 0.135291).  Saving model ...\n",
      "epoch 215\n",
      "train loss : 0.13425229350513893, validation loss : 0.13515462332846104\n",
      "Validation loss decreased (0.135291 --> 0.135155).  Saving model ...\n",
      "epoch 216\n",
      "train loss : 0.13410887179341946, validation loss : 0.13501904731266332\n",
      "Validation loss decreased (0.135155 --> 0.135019).  Saving model ...\n",
      "epoch 217\n",
      "train loss : 0.13396670543533837, validation loss : 0.13488470549628503\n",
      "Validation loss decreased (0.135019 --> 0.134885).  Saving model ...\n",
      "epoch 218\n",
      "train loss : 0.13382577786951178, validation loss : 0.13475157947441843\n",
      "Validation loss decreased (0.134885 --> 0.134752).  Saving model ...\n",
      "epoch 219\n",
      "train loss : 0.1336860735357341, validation loss : 0.13461965372789275\n",
      "Validation loss decreased (0.134752 --> 0.134620).  Saving model ...\n",
      "epoch 220\n",
      "train loss : 0.13354757392501915, validation loss : 0.13448891205719096\n",
      "Validation loss decreased (0.134620 --> 0.134489).  Saving model ...\n",
      "epoch 221\n",
      "train loss : 0.13341026539553494, validation loss : 0.13435933934841385\n",
      "Validation loss decreased (0.134489 --> 0.134359).  Saving model ...\n",
      "epoch 222\n",
      "train loss : 0.133274130211863, validation loss : 0.13423091985132857\n",
      "Validation loss decreased (0.134359 --> 0.134231).  Saving model ...\n",
      "epoch 223\n",
      "train loss : 0.13313915302115042, validation loss : 0.1341036384633913\n",
      "Validation loss decreased (0.134231 --> 0.134104).  Saving model ...\n",
      "epoch 224\n",
      "train loss : 0.1330053194101797, validation loss : 0.13397748030184628\n",
      "Validation loss decreased (0.134104 --> 0.133977).  Saving model ...\n",
      "epoch 225\n",
      "train loss : 0.13287261296072167, validation loss : 0.13385242999585334\n",
      "Validation loss decreased (0.133977 --> 0.133852).  Saving model ...\n",
      "epoch 226\n",
      "train loss : 0.13274102119791062, validation loss : 0.13372847410884214\n",
      "Validation loss decreased (0.133852 --> 0.133728).  Saving model ...\n",
      "epoch 227\n",
      "train loss : 0.1326105313535807, validation loss : 0.1336055966948947\n",
      "Validation loss decreased (0.133728 --> 0.133606).  Saving model ...\n",
      "epoch 228\n",
      "train loss : 0.1324811251579022, validation loss : 0.13348378567610553\n",
      "Validation loss decreased (0.133606 --> 0.133484).  Saving model ...\n",
      "epoch 229\n",
      "train loss : 0.13235278991713814, validation loss : 0.13336302697201005\n",
      "Validation loss decreased (0.133484 --> 0.133363).  Saving model ...\n",
      "epoch 230\n",
      "train loss : 0.1322255123040433, validation loss : 0.13324330614139468\n",
      "Validation loss decreased (0.133363 --> 0.133243).  Saving model ...\n",
      "epoch 231\n",
      "train loss : 0.13209928018131745, validation loss : 0.13312461209289717\n",
      "Validation loss decreased (0.133243 --> 0.133125).  Saving model ...\n",
      "epoch 232\n",
      "train loss : 0.13197407862078486, validation loss : 0.13300693077103068\n",
      "Validation loss decreased (0.133125 --> 0.133007).  Saving model ...\n",
      "epoch 233\n",
      "train loss : 0.13184989454865229, validation loss : 0.13289024899796012\n",
      "Validation loss decreased (0.133007 --> 0.132890).  Saving model ...\n",
      "epoch 234\n",
      "train loss : 0.13172671718941933, validation loss : 0.13277455336320892\n",
      "Validation loss decreased (0.132890 --> 0.132775).  Saving model ...\n",
      "epoch 235\n",
      "train loss : 0.13160453069221753, validation loss : 0.13265983222042121\n",
      "Validation loss decreased (0.132775 --> 0.132660).  Saving model ...\n",
      "epoch 236\n",
      "train loss : 0.13148332631698756, validation loss : 0.13254607391738554\n",
      "Validation loss decreased (0.132660 --> 0.132546).  Saving model ...\n",
      "epoch 237\n",
      "train loss : 0.13136308855655265, validation loss : 0.13243326559326324\n",
      "Validation loss decreased (0.132546 --> 0.132433).  Saving model ...\n",
      "epoch 238\n",
      "train loss : 0.13124380794747478, validation loss : 0.1323213966100749\n",
      "Validation loss decreased (0.132433 --> 0.132321).  Saving model ...\n",
      "epoch 239\n",
      "train loss : 0.13112547136306685, validation loss : 0.13221045333840692\n",
      "Validation loss decreased (0.132321 --> 0.132210).  Saving model ...\n",
      "epoch 240\n",
      "train loss : 0.13100806692093497, validation loss : 0.13210042638702887\n",
      "Validation loss decreased (0.132210 --> 0.132100).  Saving model ...\n",
      "epoch 241\n",
      "train loss : 0.13089158493745806, validation loss : 0.131991303669917\n",
      "Validation loss decreased (0.132100 --> 0.131991).  Saving model ...\n",
      "epoch 242\n",
      "train loss : 0.1307760137989359, validation loss : 0.1318830732888909\n",
      "Validation loss decreased (0.131991 --> 0.131883).  Saving model ...\n",
      "epoch 243\n",
      "train loss : 0.1306613416796111, validation loss : 0.1317757269489752\n",
      "Validation loss decreased (0.131883 --> 0.131776).  Saving model ...\n",
      "epoch 244\n",
      "train loss : 0.13054755774393878, validation loss : 0.13166925198563711\n",
      "Validation loss decreased (0.131776 --> 0.131669).  Saving model ...\n",
      "epoch 245\n",
      "train loss : 0.1304346507276377, validation loss : 0.1315636383064745\n",
      "Validation loss decreased (0.131669 --> 0.131564).  Saving model ...\n",
      "epoch 246\n",
      "train loss : 0.13032261148078078, validation loss : 0.13145887580762133\n",
      "Validation loss decreased (0.131564 --> 0.131459).  Saving model ...\n",
      "epoch 247\n",
      "train loss : 0.1302114274589451, validation loss : 0.13135495331937386\n",
      "Validation loss decreased (0.131459 --> 0.131355).  Saving model ...\n",
      "epoch 248\n",
      "train loss : 0.13010108978584387, validation loss : 0.13125186086949542\n",
      "Validation loss decreased (0.131355 --> 0.131252).  Saving model ...\n",
      "epoch 249\n",
      "train loss : 0.12999158831593774, validation loss : 0.13114959020086128\n",
      "Validation loss decreased (0.131252 --> 0.131150).  Saving model ...\n",
      "epoch 250\n",
      "train loss : 0.1298829130287819, validation loss : 0.1310481308618268\n",
      "Validation loss decreased (0.131150 --> 0.131048).  Saving model ...\n",
      "epoch 251\n",
      "train loss : 0.12977505448779664, validation loss : 0.13094747286424718\n",
      "Validation loss decreased (0.131048 --> 0.130947).  Saving model ...\n",
      "epoch 252\n",
      "train loss : 0.12966800273841447, validation loss : 0.13084760649530322\n",
      "Validation loss decreased (0.130947 --> 0.130848).  Saving model ...\n",
      "epoch 253\n",
      "train loss : 0.12956174685662797, validation loss : 0.13074852250692015\n",
      "Validation loss decreased (0.130848 --> 0.130749).  Saving model ...\n",
      "epoch 254\n",
      "train loss : 0.12945628052088465, validation loss : 0.1306502106859322\n",
      "Validation loss decreased (0.130749 --> 0.130650).  Saving model ...\n",
      "epoch 255\n",
      "train loss : 0.12935159203052318, validation loss : 0.1305526631441027\n",
      "Validation loss decreased (0.130650 --> 0.130553).  Saving model ...\n",
      "epoch 256\n",
      "train loss : 0.129247674904999, validation loss : 0.13045587142774018\n",
      "Validation loss decreased (0.130553 --> 0.130456).  Saving model ...\n",
      "epoch 257\n",
      "train loss : 0.1291445176248542, validation loss : 0.13035982682749828\n",
      "Validation loss decreased (0.130456 --> 0.130360).  Saving model ...\n",
      "epoch 258\n",
      "train loss : 0.12904211284623848, validation loss : 0.13026452021490362\n",
      "Validation loss decreased (0.130360 --> 0.130265).  Saving model ...\n",
      "epoch 259\n",
      "train loss : 0.12894045228731388, validation loss : 0.1301699433456489\n",
      "Validation loss decreased (0.130265 --> 0.130170).  Saving model ...\n",
      "epoch 260\n",
      "train loss : 0.1288395283122308, validation loss : 0.1300760883682211\n",
      "Validation loss decreased (0.130170 --> 0.130076).  Saving model ...\n",
      "epoch 261\n",
      "train loss : 0.12873933152609537, validation loss : 0.12998294578876202\n",
      "Validation loss decreased (0.130076 --> 0.129983).  Saving model ...\n",
      "epoch 262\n",
      "train loss : 0.12863985113560594, validation loss : 0.12989050844840647\n",
      "Validation loss decreased (0.129983 --> 0.129891).  Saving model ...\n",
      "epoch 263\n",
      "train loss : 0.1285410823636646, validation loss : 0.1297987685384103\n",
      "Validation loss decreased (0.129891 --> 0.129799).  Saving model ...\n",
      "epoch 264\n",
      "train loss : 0.12844301546861248, validation loss : 0.12970771749920088\n",
      "Validation loss decreased (0.129799 --> 0.129708).  Saving model ...\n",
      "epoch 265\n",
      "train loss : 0.12834564416414224, validation loss : 0.129617347983956\n",
      "Validation loss decreased (0.129708 --> 0.129617).  Saving model ...\n",
      "epoch 266\n",
      "train loss : 0.12824896037285916, validation loss : 0.12952765227626375\n",
      "Validation loss decreased (0.129617 --> 0.129528).  Saving model ...\n",
      "epoch 267\n",
      "train loss : 0.12815295545441863, validation loss : 0.12943862338425693\n",
      "Validation loss decreased (0.129528 --> 0.129439).  Saving model ...\n",
      "epoch 268\n",
      "train loss : 0.12805762271316645, validation loss : 0.12935025420288707\n",
      "Validation loss decreased (0.129439 --> 0.129350).  Saving model ...\n",
      "epoch 269\n",
      "train loss : 0.1279629545528202, validation loss : 0.12926253688754905\n",
      "Validation loss decreased (0.129350 --> 0.129263).  Saving model ...\n",
      "epoch 270\n",
      "train loss : 0.12786894340839797, validation loss : 0.12917546366504387\n",
      "Validation loss decreased (0.129263 --> 0.129175).  Saving model ...\n",
      "epoch 271\n",
      "train loss : 0.12777558278521967, validation loss : 0.12908902831361854\n",
      "Validation loss decreased (0.129175 --> 0.129089).  Saving model ...\n",
      "epoch 272\n",
      "train loss : 0.12768286421186859, validation loss : 0.12900322368088404\n",
      "Validation loss decreased (0.129089 --> 0.129003).  Saving model ...\n",
      "epoch 273\n",
      "train loss : 0.12759078319070022, validation loss : 0.1289180436762696\n",
      "Validation loss decreased (0.129003 --> 0.128918).  Saving model ...\n",
      "epoch 274\n",
      "train loss : 0.12749933103583794, validation loss : 0.12883348083973292\n",
      "Validation loss decreased (0.128918 --> 0.128833).  Saving model ...\n",
      "epoch 275\n",
      "train loss : 0.12740850218836083, validation loss : 0.12874952735349612\n",
      "Validation loss decreased (0.128833 --> 0.128750).  Saving model ...\n",
      "epoch 276\n",
      "train loss : 0.127318288508585, validation loss : 0.1286661780374028\n",
      "Validation loss decreased (0.128750 --> 0.128666).  Saving model ...\n",
      "epoch 277\n",
      "train loss : 0.127228682851428, validation loss : 0.12858342601586464\n",
      "Validation loss decreased (0.128666 --> 0.128583).  Saving model ...\n",
      "epoch 278\n",
      "train loss : 0.12713968264078726, validation loss : 0.12850126397157174\n",
      "Validation loss decreased (0.128583 --> 0.128501).  Saving model ...\n",
      "epoch 279\n",
      "train loss : 0.12705127708989283, validation loss : 0.12841968601404216\n",
      "Validation loss decreased (0.128501 --> 0.128420).  Saving model ...\n",
      "epoch 280\n",
      "train loss : 0.12696346218805746, validation loss : 0.12833868677544683\n",
      "Validation loss decreased (0.128420 --> 0.128339).  Saving model ...\n",
      "epoch 281\n",
      "train loss : 0.12687623014568322, validation loss : 0.12825825985960398\n",
      "Validation loss decreased (0.128339 --> 0.128258).  Saving model ...\n",
      "epoch 282\n",
      "train loss : 0.12678957687855896, validation loss : 0.12817839920117657\n",
      "Validation loss decreased (0.128258 --> 0.128178).  Saving model ...\n",
      "epoch 283\n",
      "train loss : 0.12670349320949423, validation loss : 0.12809909865461047\n",
      "Validation loss decreased (0.128178 --> 0.128099).  Saving model ...\n",
      "epoch 284\n",
      "train loss : 0.1266179761648252, validation loss : 0.12802035168731893\n",
      "Validation loss decreased (0.128099 --> 0.128020).  Saving model ...\n",
      "epoch 285\n",
      "train loss : 0.12653301777968967, validation loss : 0.12794215387066374\n",
      "Validation loss decreased (0.128020 --> 0.127942).  Saving model ...\n",
      "epoch 286\n",
      "train loss : 0.1264486138669878, validation loss : 0.12786449870385602\n",
      "Validation loss decreased (0.127942 --> 0.127864).  Saving model ...\n",
      "epoch 287\n",
      "train loss : 0.12636475719670787, validation loss : 0.1277873800215311\n",
      "Validation loss decreased (0.127864 --> 0.127787).  Saving model ...\n",
      "epoch 288\n",
      "train loss : 0.1262814428345676, validation loss : 0.12771079336555144\n",
      "Validation loss decreased (0.127787 --> 0.127711).  Saving model ...\n",
      "epoch 289\n",
      "train loss : 0.1261986654044256, validation loss : 0.12763473329265587\n",
      "Validation loss decreased (0.127711 --> 0.127635).  Saving model ...\n",
      "epoch 290\n",
      "train loss : 0.1261164191892345, validation loss : 0.12755919359074122\n",
      "Validation loss decreased (0.127635 --> 0.127559).  Saving model ...\n",
      "epoch 291\n",
      "train loss : 0.12603469912102166, validation loss : 0.12748416935483062\n",
      "Validation loss decreased (0.127559 --> 0.127484).  Saving model ...\n",
      "epoch 292\n",
      "train loss : 0.12595349790242472, validation loss : 0.12740965525103076\n",
      "Validation loss decreased (0.127484 --> 0.127410).  Saving model ...\n",
      "epoch 293\n",
      "train loss : 0.1258728141390936, validation loss : 0.12733564654591428\n",
      "Validation loss decreased (0.127410 --> 0.127336).  Saving model ...\n",
      "epoch 294\n",
      "train loss : 0.1257926391613039, validation loss : 0.1272621372441697\n",
      "Validation loss decreased (0.127336 --> 0.127262).  Saving model ...\n",
      "epoch 295\n",
      "train loss : 0.1257129685276593, validation loss : 0.12718912316653774\n",
      "Validation loss decreased (0.127262 --> 0.127189).  Saving model ...\n",
      "epoch 296\n",
      "train loss : 0.12563379859515203, validation loss : 0.12711659827208138\n",
      "Validation loss decreased (0.127189 --> 0.127117).  Saving model ...\n",
      "epoch 297\n",
      "train loss : 0.12555512581981998, validation loss : 0.12704455962174718\n",
      "Validation loss decreased (0.127117 --> 0.127045).  Saving model ...\n",
      "epoch 298\n",
      "train loss : 0.12547693967593035, validation loss : 0.12697300109041815\n",
      "Validation loss decreased (0.127045 --> 0.126973).  Saving model ...\n",
      "epoch 299\n",
      "train loss : 0.12539923809313128, validation loss : 0.12690191756652722\n",
      "Validation loss decreased (0.126973 --> 0.126902).  Saving model ...\n",
      "epoch 300\n",
      "train loss : 0.12532201720639746, validation loss : 0.12683130493662684\n",
      "Validation loss decreased (0.126902 --> 0.126831).  Saving model ...\n",
      "epoch 301\n",
      "train loss : 0.12524527168467592, validation loss : 0.12676115732531415\n",
      "Validation loss decreased (0.126831 --> 0.126761).  Saving model ...\n",
      "epoch 302\n",
      "train loss : 0.12516899763573797, validation loss : 0.12669147215216964\n",
      "Validation loss decreased (0.126761 --> 0.126691).  Saving model ...\n",
      "epoch 303\n",
      "train loss : 0.12509318967178207, validation loss : 0.1266222443541161\n",
      "Validation loss decreased (0.126691 --> 0.126622).  Saving model ...\n",
      "epoch 304\n",
      "train loss : 0.12501784266402916, validation loss : 0.12655346906282938\n",
      "Validation loss decreased (0.126622 --> 0.126553).  Saving model ...\n",
      "epoch 305\n",
      "train loss : 0.12494295277988209, validation loss : 0.1264851418886251\n",
      "Validation loss decreased (0.126553 --> 0.126485).  Saving model ...\n",
      "epoch 306\n",
      "train loss : 0.12486851750597773, validation loss : 0.12641725887616873\n",
      "Validation loss decreased (0.126485 --> 0.126417).  Saving model ...\n",
      "epoch 307\n",
      "train loss : 0.12479453041028425, validation loss : 0.1263498150948048\n",
      "Validation loss decreased (0.126417 --> 0.126350).  Saving model ...\n",
      "epoch 308\n",
      "train loss : 0.1247209877488442, validation loss : 0.1262828067894221\n",
      "Validation loss decreased (0.126350 --> 0.126283).  Saving model ...\n",
      "epoch 309\n",
      "train loss : 0.12464788399732282, validation loss : 0.12621622971706226\n",
      "Validation loss decreased (0.126283 --> 0.126216).  Saving model ...\n",
      "epoch 310\n",
      "train loss : 0.12457521637560812, validation loss : 0.12615007971203787\n",
      "Validation loss decreased (0.126216 --> 0.126150).  Saving model ...\n",
      "epoch 311\n",
      "train loss : 0.12450297989268702, validation loss : 0.12608435279969465\n",
      "Validation loss decreased (0.126150 --> 0.126084).  Saving model ...\n",
      "epoch 312\n",
      "train loss : 0.1244311703073567, validation loss : 0.12601904491173463\n",
      "Validation loss decreased (0.126084 --> 0.126019).  Saving model ...\n",
      "epoch 313\n",
      "train loss : 0.1243597852578818, validation loss : 0.12595415133483054\n",
      "Validation loss decreased (0.126019 --> 0.125954).  Saving model ...\n",
      "epoch 314\n",
      "train loss : 0.12428881917735617, validation loss : 0.1258896688801792\n",
      "Validation loss decreased (0.125954 --> 0.125890).  Saving model ...\n",
      "epoch 315\n",
      "train loss : 0.12421826916241854, validation loss : 0.12582559402808013\n",
      "Validation loss decreased (0.125890 --> 0.125826).  Saving model ...\n",
      "epoch 316\n",
      "train loss : 0.12414813104382699, validation loss : 0.12576192251853854\n",
      "Validation loss decreased (0.125826 --> 0.125762).  Saving model ...\n",
      "epoch 317\n",
      "train loss : 0.12407840044363827, validation loss : 0.1256986506629985\n",
      "Validation loss decreased (0.125762 --> 0.125699).  Saving model ...\n",
      "epoch 318\n",
      "train loss : 0.12400907448842961, validation loss : 0.1256357746493433\n",
      "Validation loss decreased (0.125699 --> 0.125636).  Saving model ...\n",
      "epoch 319\n",
      "train loss : 0.12394014787450773, validation loss : 0.1255732901095303\n",
      "Validation loss decreased (0.125636 --> 0.125573).  Saving model ...\n",
      "epoch 320\n",
      "train loss : 0.12387161831207183, validation loss : 0.12551119409496975\n",
      "Validation loss decreased (0.125573 --> 0.125511).  Saving model ...\n",
      "epoch 321\n",
      "train loss : 0.12380348126994738, validation loss : 0.12544948287426677\n",
      "Validation loss decreased (0.125511 --> 0.125449).  Saving model ...\n",
      "epoch 322\n",
      "train loss : 0.12373573343889589, validation loss : 0.12538815316567062\n",
      "Validation loss decreased (0.125449 --> 0.125388).  Saving model ...\n",
      "epoch 323\n",
      "train loss : 0.12366837151660025, validation loss : 0.12532720114734863\n",
      "Validation loss decreased (0.125388 --> 0.125327).  Saving model ...\n",
      "epoch 324\n",
      "train loss : 0.1236013913413127, validation loss : 0.1252666241629372\n",
      "Validation loss decreased (0.125327 --> 0.125267).  Saving model ...\n",
      "epoch 325\n",
      "train loss : 0.12353478964472515, validation loss : 0.12520641754133036\n",
      "Validation loss decreased (0.125267 --> 0.125206).  Saving model ...\n",
      "epoch 326\n",
      "train loss : 0.1234685641894834, validation loss : 0.12514657866454704\n",
      "Validation loss decreased (0.125206 --> 0.125147).  Saving model ...\n",
      "epoch 327\n",
      "train loss : 0.12340270969863522, validation loss : 0.12508710455083247\n",
      "Validation loss decreased (0.125147 --> 0.125087).  Saving model ...\n",
      "epoch 328\n",
      "train loss : 0.12333722419668547, validation loss : 0.12502799101876794\n",
      "Validation loss decreased (0.125087 --> 0.125028).  Saving model ...\n",
      "epoch 329\n",
      "train loss : 0.12327210261478885, validation loss : 0.12496923523764177\n",
      "Validation loss decreased (0.125028 --> 0.124969).  Saving model ...\n",
      "epoch 330\n",
      "train loss : 0.12320734379402291, validation loss : 0.12491083412880331\n",
      "Validation loss decreased (0.124969 --> 0.124911).  Saving model ...\n",
      "epoch 331\n",
      "train loss : 0.12314294309095718, validation loss : 0.12485278409181351\n",
      "Validation loss decreased (0.124911 --> 0.124853).  Saving model ...\n",
      "epoch 332\n",
      "train loss : 0.12307889763828234, validation loss : 0.12479508239350523\n",
      "Validation loss decreased (0.124853 --> 0.124795).  Saving model ...\n",
      "epoch 333\n",
      "train loss : 0.12301520613833988, validation loss : 0.12473772523854175\n",
      "Validation loss decreased (0.124795 --> 0.124738).  Saving model ...\n",
      "epoch 334\n",
      "train loss : 0.12295186401479871, validation loss : 0.1246807100069102\n",
      "Validation loss decreased (0.124738 --> 0.124681).  Saving model ...\n",
      "epoch 335\n",
      "train loss : 0.12288886708737491, validation loss : 0.12462403342445734\n",
      "Validation loss decreased (0.124681 --> 0.124624).  Saving model ...\n",
      "epoch 336\n",
      "train loss : 0.12282621327674575, validation loss : 0.12456769243502978\n",
      "Validation loss decreased (0.124624 --> 0.124568).  Saving model ...\n",
      "epoch 337\n",
      "train loss : 0.12276390084862737, validation loss : 0.12451168426575006\n",
      "Validation loss decreased (0.124568 --> 0.124512).  Saving model ...\n",
      "epoch 338\n",
      "train loss : 0.12270192403972512, validation loss : 0.12445600489595249\n",
      "Validation loss decreased (0.124512 --> 0.124456).  Saving model ...\n",
      "epoch 339\n",
      "train loss : 0.12264028126482643, validation loss : 0.12440065307774\n",
      "Validation loss decreased (0.124456 --> 0.124401).  Saving model ...\n",
      "epoch 340\n",
      "train loss : 0.12257896991551655, validation loss : 0.12434562576713351\n",
      "Validation loss decreased (0.124401 --> 0.124346).  Saving model ...\n",
      "epoch 341\n",
      "train loss : 0.12251798705880708, validation loss : 0.1242909199015819\n",
      "Validation loss decreased (0.124346 --> 0.124291).  Saving model ...\n",
      "epoch 342\n",
      "train loss : 0.12245733036632717, validation loss : 0.12423653147768433\n",
      "Validation loss decreased (0.124291 --> 0.124237).  Saving model ...\n",
      "epoch 343\n",
      "train loss : 0.12239699782134132, validation loss : 0.12418245910149334\n",
      "Validation loss decreased (0.124237 --> 0.124182).  Saving model ...\n",
      "epoch 344\n",
      "train loss : 0.12233698356732314, validation loss : 0.12412869927402734\n",
      "Validation loss decreased (0.124182 --> 0.124129).  Saving model ...\n",
      "epoch 345\n",
      "train loss : 0.12227728733081987, validation loss : 0.12407525036060657\n",
      "Validation loss decreased (0.124129 --> 0.124075).  Saving model ...\n",
      "epoch 346\n",
      "train loss : 0.12221790494563403, validation loss : 0.12402210870999056\n",
      "Validation loss decreased (0.124075 --> 0.124022).  Saving model ...\n",
      "epoch 347\n",
      "train loss : 0.12215883387663336, validation loss : 0.12396927207886324\n",
      "Validation loss decreased (0.124022 --> 0.123969).  Saving model ...\n",
      "epoch 348\n",
      "train loss : 0.12210007288600307, validation loss : 0.12391673777250252\n",
      "Validation loss decreased (0.123969 --> 0.123917).  Saving model ...\n",
      "epoch 349\n",
      "train loss : 0.1220416184171681, validation loss : 0.12386450306471174\n",
      "Validation loss decreased (0.123917 --> 0.123865).  Saving model ...\n",
      "epoch 350\n",
      "train loss : 0.12198346780736857, validation loss : 0.1238125653944188\n",
      "Validation loss decreased (0.123865 --> 0.123813).  Saving model ...\n",
      "epoch 351\n",
      "train loss : 0.12192561918301965, validation loss : 0.12376092289617313\n",
      "Validation loss decreased (0.123813 --> 0.123761).  Saving model ...\n",
      "epoch 352\n",
      "train loss : 0.12186806894575618, validation loss : 0.1237095723080464\n",
      "Validation loss decreased (0.123761 --> 0.123710).  Saving model ...\n",
      "epoch 353\n",
      "train loss : 0.12181081603771682, validation loss : 0.12365851148486945\n",
      "Validation loss decreased (0.123710 --> 0.123659).  Saving model ...\n",
      "epoch 354\n",
      "train loss : 0.12175385705264602, validation loss : 0.12360773747587757\n",
      "Validation loss decreased (0.123659 --> 0.123608).  Saving model ...\n",
      "epoch 355\n",
      "train loss : 0.12169718952362316, validation loss : 0.12355724792396466\n",
      "Validation loss decreased (0.123608 --> 0.123557).  Saving model ...\n",
      "epoch 356\n",
      "train loss : 0.12164081028648725, validation loss : 0.12350704027099754\n",
      "Validation loss decreased (0.123557 --> 0.123507).  Saving model ...\n",
      "epoch 357\n",
      "train loss : 0.12158471835244537, validation loss : 0.12345711329131089\n",
      "Validation loss decreased (0.123507 --> 0.123457).  Saving model ...\n",
      "epoch 358\n",
      "train loss : 0.1215289111951748, validation loss : 0.12340746349788735\n",
      "Validation loss decreased (0.123457 --> 0.123407).  Saving model ...\n",
      "epoch 359\n",
      "train loss : 0.1214733854795672, validation loss : 0.12335808897099859\n",
      "Validation loss decreased (0.123407 --> 0.123358).  Saving model ...\n",
      "epoch 360\n",
      "train loss : 0.12141813919381608, validation loss : 0.12330898745376193\n",
      "Validation loss decreased (0.123358 --> 0.123309).  Saving model ...\n",
      "epoch 361\n",
      "train loss : 0.12136317054910467, validation loss : 0.12326015592122588\n",
      "Validation loss decreased (0.123309 --> 0.123260).  Saving model ...\n",
      "epoch 362\n",
      "train loss : 0.12130847777888837, validation loss : 0.12321159348606174\n",
      "Validation loss decreased (0.123260 --> 0.123212).  Saving model ...\n",
      "epoch 363\n",
      "train loss : 0.12125405792911345, validation loss : 0.12316329808161575\n",
      "Validation loss decreased (0.123212 --> 0.123163).  Saving model ...\n",
      "epoch 364\n",
      "train loss : 0.12119990906796795, validation loss : 0.12311526620927092\n",
      "Validation loss decreased (0.123163 --> 0.123115).  Saving model ...\n",
      "epoch 365\n",
      "train loss : 0.1211460283009886, validation loss : 0.12306749714965327\n",
      "Validation loss decreased (0.123115 --> 0.123067).  Saving model ...\n",
      "epoch 366\n",
      "train loss : 0.12109241420622366, validation loss : 0.12301998819602436\n",
      "Validation loss decreased (0.123067 --> 0.123020).  Saving model ...\n",
      "epoch 367\n",
      "train loss : 0.1210390652407951, validation loss : 0.12297273609988527\n",
      "Validation loss decreased (0.123020 --> 0.122973).  Saving model ...\n",
      "epoch 368\n",
      "train loss : 0.12098598029820501, validation loss : 0.12292574037549356\n",
      "Validation loss decreased (0.122973 --> 0.122926).  Saving model ...\n",
      "epoch 369\n",
      "train loss : 0.1209331547270599, validation loss : 0.12287899843785803\n",
      "Validation loss decreased (0.122926 --> 0.122879).  Saving model ...\n",
      "epoch 370\n",
      "train loss : 0.120880586728782, validation loss : 0.12283250816104614\n",
      "Validation loss decreased (0.122879 --> 0.122833).  Saving model ...\n",
      "epoch 371\n",
      "train loss : 0.12082827628209451, validation loss : 0.12278626796290226\n",
      "Validation loss decreased (0.122833 --> 0.122786).  Saving model ...\n",
      "epoch 372\n",
      "train loss : 0.12077621882897464, validation loss : 0.12274027545368135\n",
      "Validation loss decreased (0.122786 --> 0.122740).  Saving model ...\n",
      "epoch 373\n",
      "train loss : 0.1207244149670497, validation loss : 0.12269452847921088\n",
      "Validation loss decreased (0.122740 --> 0.122695).  Saving model ...\n",
      "epoch 374\n",
      "train loss : 0.12067286060856532, validation loss : 0.12264902493478964\n",
      "Validation loss decreased (0.122695 --> 0.122649).  Saving model ...\n",
      "epoch 375\n",
      "train loss : 0.12062155443817195, validation loss : 0.1226037637209505\n",
      "Validation loss decreased (0.122649 --> 0.122604).  Saving model ...\n",
      "epoch 376\n",
      "train loss : 0.12057049393578786, validation loss : 0.12255874288578111\n",
      "Validation loss decreased (0.122604 --> 0.122559).  Saving model ...\n",
      "epoch 377\n",
      "train loss : 0.12051967819754325, validation loss : 0.12251395890840128\n",
      "Validation loss decreased (0.122559 --> 0.122514).  Saving model ...\n",
      "epoch 378\n",
      "train loss : 0.12046910564147012, validation loss : 0.12246941285162376\n",
      "Validation loss decreased (0.122514 --> 0.122469).  Saving model ...\n",
      "epoch 379\n",
      "train loss : 0.12041877405752316, validation loss : 0.12242510033040192\n",
      "Validation loss decreased (0.122469 --> 0.122425).  Saving model ...\n",
      "epoch 380\n",
      "train loss : 0.12036868042586123, validation loss : 0.12238102070407476\n",
      "Validation loss decreased (0.122425 --> 0.122381).  Saving model ...\n",
      "epoch 381\n",
      "train loss : 0.12031882328733941, validation loss : 0.12233717230527334\n",
      "Validation loss decreased (0.122381 --> 0.122337).  Saving model ...\n",
      "epoch 382\n",
      "train loss : 0.1202692028936752, validation loss : 0.12229355271401668\n",
      "Validation loss decreased (0.122337 --> 0.122294).  Saving model ...\n",
      "epoch 383\n",
      "train loss : 0.12021981514677246, validation loss : 0.12225015984991072\n",
      "Validation loss decreased (0.122294 --> 0.122250).  Saving model ...\n",
      "epoch 384\n",
      "train loss : 0.12017065842387992, validation loss : 0.12220699193303682\n",
      "Validation loss decreased (0.122250 --> 0.122207).  Saving model ...\n",
      "epoch 385\n",
      "train loss : 0.12012173108284478, validation loss : 0.12216404777898122\n",
      "Validation loss decreased (0.122207 --> 0.122164).  Saving model ...\n",
      "epoch 386\n",
      "train loss : 0.12007303267961272, validation loss : 0.12212132565568722\n",
      "Validation loss decreased (0.122164 --> 0.122121).  Saving model ...\n",
      "epoch 387\n",
      "train loss : 0.12002456062925508, validation loss : 0.1220788227410411\n",
      "Validation loss decreased (0.122121 --> 0.122079).  Saving model ...\n",
      "epoch 388\n",
      "train loss : 0.1199763115782046, validation loss : 0.12203653893149895\n",
      "Validation loss decreased (0.122079 --> 0.122037).  Saving model ...\n",
      "epoch 389\n",
      "train loss : 0.11992828715070873, validation loss : 0.121994471310752\n",
      "Validation loss decreased (0.122037 --> 0.121994).  Saving model ...\n",
      "epoch 390\n",
      "train loss : 0.11988048347151806, validation loss : 0.12195261847194878\n",
      "Validation loss decreased (0.121994 --> 0.121953).  Saving model ...\n",
      "epoch 391\n",
      "train loss : 0.11983289933428745, validation loss : 0.12191097947982121\n",
      "Validation loss decreased (0.121953 --> 0.121911).  Saving model ...\n",
      "epoch 392\n",
      "train loss : 0.1197855323607303, validation loss : 0.12186955183683815\n",
      "Validation loss decreased (0.121911 --> 0.121870).  Saving model ...\n",
      "epoch 393\n",
      "train loss : 0.11973838103766038, validation loss : 0.1218283349269954\n",
      "Validation loss decreased (0.121870 --> 0.121828).  Saving model ...\n",
      "epoch 394\n",
      "train loss : 0.11969144454089457, validation loss : 0.12178732619964183\n",
      "Validation loss decreased (0.121828 --> 0.121787).  Saving model ...\n",
      "epoch 395\n",
      "train loss : 0.11964472054833236, validation loss : 0.12174652423920194\n",
      "Validation loss decreased (0.121787 --> 0.121747).  Saving model ...\n",
      "epoch 396\n",
      "train loss : 0.11959820743658867, validation loss : 0.12170592762464043\n",
      "Validation loss decreased (0.121747 --> 0.121706).  Saving model ...\n",
      "epoch 397\n",
      "train loss : 0.11955190471002795, validation loss : 0.12166553506046968\n",
      "Validation loss decreased (0.121706 --> 0.121666).  Saving model ...\n",
      "epoch 398\n",
      "train loss : 0.11950580963722686, validation loss : 0.12162534453614328\n",
      "Validation loss decreased (0.121666 --> 0.121625).  Saving model ...\n",
      "epoch 399\n",
      "train loss : 0.11945992058451593, validation loss : 0.12158535479512977\n",
      "Validation loss decreased (0.121625 --> 0.121585).  Saving model ...\n",
      "epoch 400\n",
      "train loss : 0.11941423726422638, validation loss : 0.12154556449286663\n",
      "Validation loss decreased (0.121585 --> 0.121546).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def training(epochs) :\n",
    "    # epoch 별 훈련 및 검증 루프 (trainloader를 반복하여 모델 훈련시키고, validloader를 사용해 모델의 성능 검증)\n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0.0\n",
    "        running_vall_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        # 훈련 과정\n",
    "        for data in trainloader:\n",
    "            model.train()\n",
    "            inputs, outputs = data\n",
    "            optimizer.zero_grad()                                                     # Optimizer Gradient를 0으로 초기화. 이는 각 미니배치마다 Gradient가 누적되는 것을 방지\n",
    "            predicted_outputs = model(inputs)                                         # 모델을 사용하여 입력 데이터에 대한 예측값을 계산\n",
    "            train_loss = criterion(predicted_outputs, outputs)                        # 계산된 예측값과 실제 레이블 간의 손실을 계산\n",
    "            train_loss.backward()                                                     # 손실에 대한 역전파를 수행하여 Gradient를 계산\n",
    "            optimizer.step()                                                          # 계산된 Gradient를 사용하여 모델 매개변수를 업데이트\n",
    "            running_train_loss += train_loss.item()                                   # track the loss value\n",
    "        loss_.append(running_train_loss / n)\n",
    "        \n",
    "        # 검증 과정\n",
    "        with torch.no_grad():                                                         # Gradient 계산을 비활성화하여 메모리 사용량을 줄이고 계산 속도를 향상 \n",
    "            model.eval()\n",
    "            for data in validloader:\n",
    "                inputs, outputs = data\n",
    "                predicted_outputs = model(inputs)\n",
    "                val_loss = criterion(predicted_outputs, outputs)\n",
    "\n",
    "                # The label with the highest value will be our prediction\n",
    "                _, predicted = torch.max(predicted_outputs,1)\n",
    "                running_vall_loss += val_loss.item()\n",
    "                total += outputs.size(0)\n",
    "                val_loss_value = running_vall_loss/len(validloader)\n",
    "        valoss_.append(val_loss_value)\n",
    "\n",
    "        avgtrainloss = np.mean(loss_)\n",
    "        avgvalidloss = np.mean(valoss_)\n",
    "        print('epoch', epoch + 1)\n",
    "        print(f'train loss : {avgtrainloss}, validation loss : {avgvalidloss}')\n",
    "        \n",
    "        # EarlyStopping\n",
    "        early_stopping(avgvalidloss, model)                                           # 검증 손실을 기준으로 조기 종료 조건 확인\n",
    "        if early_stopping.early_stop:                                                 # 조건 만족 시 조기 종료\n",
    "            break\n",
    "\n",
    "    # 모델 저장    \n",
    "    saveModel()\n",
    "training(epochs = epoch)\n",
    "\n",
    "\n",
    "# 손실 시각화\n",
    "plt.plot(loss_)\n",
    "plt.plot(valoss_)\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 (Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [11:38<00:00, 14.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluation(dataloader):\n",
    "  # 초기화\n",
    "  predictions = torch.tensor([], dtype=torch.float64,device = device)  # 예측값을 저장하는 텐서.\n",
    "  actual = torch.tensor([], dtype=torch.float64, device = device)      # 실제값을 저장하는 텐서.\n",
    "\n",
    "  # 평가 모드 설정\n",
    "  with torch.no_grad():\n",
    "    model.eval()                                                       # 평가를 할 땐 반드시 eval()을 사용해야 한다.\n",
    "\n",
    "    # 데이터 로더를 통한 반복\n",
    "    for data in dataloader:\n",
    "        inputs, values = data\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 예측값 및 실제값 저장\n",
    "        predictions = torch.cat((predictions, outputs), 0)             # cat함수를 통해 예측값을 누적.\n",
    "        actual = torch.cat((actual, values), 0)                        # cat함수를 통해 실제값을 누적.\n",
    "  \n",
    "  # CPU로 이동 및 NumPy 배열 변환\n",
    "  predictions =predictions.to(device= \"cpu\")\n",
    "  predictions = predictions.numpy()                                    # 넘파이 배열로 변경.\n",
    "  actual = actual.to(device= \"cpu\")\n",
    "  actual = actual.numpy()                                              # 넘파이 배열로 변경.\n",
    "  \n",
    "  # RMSE 계산\n",
    "  rmse = np.sqrt(mean_squared_error(predictions, actual))              # sklearn을 이용해 RMSE를 계산.\n",
    "\n",
    "  return rmse,actual,predictions\n",
    "\n",
    "preds = []\n",
    "for i in tqdm(range(10000)):\n",
    "  test_rmse, actual, pred = evaluation(testloader)\n",
    "  preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32525758359538454"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82925373],\n",
       "       [0.83910036],\n",
       "       [0.7884391 ],\n",
       "       [0.70573872],\n",
       "       [0.82391906],\n",
       "       [0.74098319],\n",
       "       [0.79373807],\n",
       "       [0.74696046],\n",
       "       [0.81288618],\n",
       "       [0.7619018 ],\n",
       "       [0.81172562],\n",
       "       [0.76455021],\n",
       "       [0.71813208],\n",
       "       [0.74045193],\n",
       "       [0.77286613],\n",
       "       [0.79626834],\n",
       "       [0.82769257],\n",
       "       [0.88206357],\n",
       "       [0.83242273],\n",
       "       [0.82615221],\n",
       "       [0.83104068],\n",
       "       [0.76846093],\n",
       "       [0.87365264],\n",
       "       [0.81655878],\n",
       "       [0.77760267],\n",
       "       [0.82767165],\n",
       "       [0.85653996],\n",
       "       [0.82179374],\n",
       "       [0.88621706],\n",
       "       [0.82844895],\n",
       "       [0.78240329],\n",
       "       [0.84869975],\n",
       "       [0.82157689],\n",
       "       [0.91277552],\n",
       "       [0.79256135],\n",
       "       [0.73622233],\n",
       "       [0.91947788],\n",
       "       [0.83733875],\n",
       "       [0.86554122],\n",
       "       [0.88660473],\n",
       "       [0.86042863],\n",
       "       [0.78751326],\n",
       "       [0.85895479],\n",
       "       [0.86732823],\n",
       "       [0.86074179],\n",
       "       [0.88200617],\n",
       "       [0.77711987],\n",
       "       [0.93004119],\n",
       "       [0.89863563],\n",
       "       [0.88549614],\n",
       "       [0.83249593],\n",
       "       [0.94934887],\n",
       "       [0.86072934],\n",
       "       [0.83505803],\n",
       "       [0.93610603],\n",
       "       [0.888466  ],\n",
       "       [0.86260444],\n",
       "       [0.76137245],\n",
       "       [0.89553225],\n",
       "       [0.90963966],\n",
       "       [0.87875634],\n",
       "       [0.84389395],\n",
       "       [0.87978572],\n",
       "       [0.86612618],\n",
       "       [0.88498789],\n",
       "       [0.8676573 ],\n",
       "       [0.84908396],\n",
       "       [0.90042084],\n",
       "       [0.87922412],\n",
       "       [0.91467333],\n",
       "       [0.90307951],\n",
       "       [0.8762486 ],\n",
       "       [0.75285393],\n",
       "       [0.83078492],\n",
       "       [0.87406546],\n",
       "       [0.9036746 ],\n",
       "       [0.91058838],\n",
       "       [0.90805453],\n",
       "       [0.91339296],\n",
       "       [0.94578052],\n",
       "       [0.92354   ],\n",
       "       [0.95598906],\n",
       "       [0.95344108],\n",
       "       [0.87448907],\n",
       "       [0.95734441],\n",
       "       [0.94136131],\n",
       "       [0.89316922],\n",
       "       [0.92453915],\n",
       "       [0.94012761],\n",
       "       [0.92241436],\n",
       "       [0.89181507],\n",
       "       [0.924142  ],\n",
       "       [0.93044329],\n",
       "       [0.90388083],\n",
       "       [0.93029916],\n",
       "       [0.93006927],\n",
       "       [0.95764226],\n",
       "       [0.88849485],\n",
       "       [0.94796056],\n",
       "       [0.8705315 ],\n",
       "       [0.92923146],\n",
       "       [0.9301008 ],\n",
       "       [0.89612174],\n",
       "       [0.79543275],\n",
       "       [0.8851403 ],\n",
       "       [0.8243286 ],\n",
       "       [0.78105235],\n",
       "       [0.8412466 ],\n",
       "       [0.8419643 ],\n",
       "       [0.8460595 ],\n",
       "       [0.86920142],\n",
       "       [0.92200887],\n",
       "       [0.89224946],\n",
       "       [0.91172415],\n",
       "       [0.85260296],\n",
       "       [0.8425169 ],\n",
       "       [0.88582492],\n",
       "       [0.92705905],\n",
       "       [0.85776877],\n",
       "       [0.84509462],\n",
       "       [0.86657792],\n",
       "       [0.89148223],\n",
       "       [0.88653576],\n",
       "       [0.84985554],\n",
       "       [0.86183745],\n",
       "       [0.90014189],\n",
       "       [0.90297335],\n",
       "       [0.88965821],\n",
       "       [0.78090501],\n",
       "       [0.90404266],\n",
       "       [0.87183529],\n",
       "       [0.81362665],\n",
       "       [0.7692011 ],\n",
       "       [0.81920832],\n",
       "       [0.8786034 ],\n",
       "       [0.90006042],\n",
       "       [0.8412739 ],\n",
       "       [0.89094043],\n",
       "       [0.79403651],\n",
       "       [0.88045359],\n",
       "       [0.765396  ],\n",
       "       [0.86658138],\n",
       "       [0.82339603],\n",
       "       [0.84949762],\n",
       "       [0.95577949],\n",
       "       [0.80058038],\n",
       "       [0.77473152],\n",
       "       [0.87766576],\n",
       "       [0.81803167],\n",
       "       [0.67715704],\n",
       "       [0.7638399 ],\n",
       "       [0.93929344],\n",
       "       [0.80715185],\n",
       "       [0.9101457 ],\n",
       "       [0.84518361],\n",
       "       [0.7370649 ],\n",
       "       [0.87070364],\n",
       "       [0.92691034],\n",
       "       [0.85867077],\n",
       "       [0.89890206],\n",
       "       [0.84266704],\n",
       "       [0.83384788],\n",
       "       [0.84237403],\n",
       "       [0.89426184],\n",
       "       [0.78610599],\n",
       "       [0.89449453],\n",
       "       [0.95241904],\n",
       "       [0.85764879],\n",
       "       [0.90437263],\n",
       "       [0.91267586],\n",
       "       [0.80529833],\n",
       "       [0.7707386 ],\n",
       "       [0.73323071],\n",
       "       [0.80625564],\n",
       "       [0.9112944 ],\n",
       "       [0.93767118],\n",
       "       [0.89482027],\n",
       "       [0.81904227],\n",
       "       [0.77361691],\n",
       "       [0.89194459],\n",
       "       [0.74818748],\n",
       "       [0.93141693],\n",
       "       [0.8055945 ],\n",
       "       [0.84407616],\n",
       "       [0.87498617],\n",
       "       [0.80441451],\n",
       "       [0.7643277 ],\n",
       "       [0.76744813],\n",
       "       [0.82158238],\n",
       "       [0.82071167],\n",
       "       [0.74021262],\n",
       "       [0.75664127],\n",
       "       [0.72313857],\n",
       "       [0.75608075],\n",
       "       [0.73969638],\n",
       "       [0.75008088],\n",
       "       [0.87084693],\n",
       "       [0.80627906],\n",
       "       [0.66526771],\n",
       "       [0.78589201],\n",
       "       [0.77416736],\n",
       "       [0.92884588],\n",
       "       [0.84142089],\n",
       "       [0.80061662],\n",
       "       [0.73454702],\n",
       "       [0.84881604],\n",
       "       [0.78892595],\n",
       "       [0.87333173],\n",
       "       [0.8313235 ],\n",
       "       [0.81614578],\n",
       "       [0.86969495],\n",
       "       [0.80929017],\n",
       "       [0.77561629],\n",
       "       [0.86287212],\n",
       "       [0.74521947],\n",
       "       [0.8712582 ],\n",
       "       [0.76420248],\n",
       "       [0.73565978],\n",
       "       [0.72573441],\n",
       "       [0.7614522 ],\n",
       "       [0.76110089],\n",
       "       [0.79708987],\n",
       "       [0.83791971],\n",
       "       [0.77470922],\n",
       "       [0.82716519],\n",
       "       [0.80050462],\n",
       "       [0.93757111],\n",
       "       [0.94762731],\n",
       "       [0.94697934],\n",
       "       [0.75444049],\n",
       "       [0.89705902],\n",
       "       [0.91125232],\n",
       "       [0.72052956],\n",
       "       [0.85805225],\n",
       "       [0.86140841],\n",
       "       [0.88287169],\n",
       "       [0.84067357],\n",
       "       [0.94861603],\n",
       "       [0.88341868],\n",
       "       [0.78435427],\n",
       "       [0.71905291],\n",
       "       [0.9331736 ],\n",
       "       [0.89068854],\n",
       "       [0.8700735 ],\n",
       "       [0.81694412],\n",
       "       [0.92304093],\n",
       "       [0.87891877],\n",
       "       [0.93862605],\n",
       "       [0.95131087],\n",
       "       [0.93256533],\n",
       "       [0.8888973 ],\n",
       "       [0.93983096],\n",
       "       [0.88349068],\n",
       "       [0.95423532],\n",
       "       [0.95722729],\n",
       "       [0.94747537],\n",
       "       [0.91173834],\n",
       "       [0.87966782],\n",
       "       [0.92550212],\n",
       "       [0.9646399 ],\n",
       "       [0.9157837 ],\n",
       "       [0.90744859],\n",
       "       [0.95494878],\n",
       "       [0.80287868],\n",
       "       [0.85431182],\n",
       "       [0.93768102],\n",
       "       [0.96290666],\n",
       "       [0.83801472],\n",
       "       [0.85450482],\n",
       "       [0.7114532 ],\n",
       "       [0.94299883],\n",
       "       [0.85585546],\n",
       "       [0.85974735],\n",
       "       [0.86581171],\n",
       "       [0.91154951],\n",
       "       [0.9280616 ],\n",
       "       [0.95129836],\n",
       "       [0.95557785],\n",
       "       [0.94840705],\n",
       "       [0.92308724],\n",
       "       [0.94581026],\n",
       "       [0.93909889],\n",
       "       [0.94193101],\n",
       "       [0.94644332],\n",
       "       [0.91983861],\n",
       "       [0.90814471],\n",
       "       [0.86385363],\n",
       "       [0.95135045],\n",
       "       [0.85811967],\n",
       "       [0.93634969],\n",
       "       [0.9340055 ],\n",
       "       [0.91844296],\n",
       "       [0.96601647],\n",
       "       [0.89331108],\n",
       "       [0.93992841],\n",
       "       [0.95444643],\n",
       "       [0.94296414],\n",
       "       [0.96690261],\n",
       "       [0.91926569],\n",
       "       [0.89926749],\n",
       "       [0.92352992],\n",
       "       [0.9659375 ],\n",
       "       [0.93953675],\n",
       "       [0.83214092],\n",
       "       [0.9582659 ],\n",
       "       [0.84714997],\n",
       "       [0.85329843],\n",
       "       [0.92561948],\n",
       "       [0.93074757],\n",
       "       [0.87239671],\n",
       "       [0.94549525],\n",
       "       [0.95055276],\n",
       "       [0.93659568],\n",
       "       [0.88326705],\n",
       "       [0.93725598],\n",
       "       [0.93762159],\n",
       "       [0.92214781],\n",
       "       [0.90123701],\n",
       "       [0.93961573],\n",
       "       [0.80865568],\n",
       "       [0.94724762],\n",
       "       [0.91345727],\n",
       "       [0.96237862],\n",
       "       [0.88689315],\n",
       "       [0.91790402],\n",
       "       [0.88596725],\n",
       "       [0.81582904],\n",
       "       [0.85221642],\n",
       "       [0.90134734],\n",
       "       [0.77557015],\n",
       "       [0.91561955],\n",
       "       [0.83541954],\n",
       "       [0.91710138],\n",
       "       [0.85300994],\n",
       "       [0.86944079],\n",
       "       [0.87220097],\n",
       "       [0.88814211],\n",
       "       [0.84585518],\n",
       "       [0.96184772],\n",
       "       [0.81876093],\n",
       "       [0.94669545],\n",
       "       [0.78124028],\n",
       "       [0.85237688],\n",
       "       [0.95888603],\n",
       "       [0.97160667],\n",
       "       [0.87113756],\n",
       "       [0.86756426],\n",
       "       [0.91335922],\n",
       "       [0.8144716 ],\n",
       "       [0.83493578],\n",
       "       [0.86688948],\n",
       "       [0.9218049 ],\n",
       "       [0.77326131],\n",
       "       [0.86236435],\n",
       "       [0.89740729],\n",
       "       [0.82563221],\n",
       "       [0.76409078],\n",
       "       [0.85964334],\n",
       "       [0.93285209],\n",
       "       [0.95053613],\n",
       "       [0.89499068],\n",
       "       [0.82297456],\n",
       "       [0.92125225],\n",
       "       [0.8653971 ],\n",
       "       [0.82899946],\n",
       "       [0.85171759],\n",
       "       [0.91068357],\n",
       "       [0.87370604],\n",
       "       [0.78204316],\n",
       "       [0.85659611],\n",
       "       [0.92626858],\n",
       "       [0.85584819],\n",
       "       [0.97770894],\n",
       "       [0.96843731],\n",
       "       [0.72529411],\n",
       "       [0.91662586],\n",
       "       [0.71110213],\n",
       "       [0.69090956],\n",
       "       [0.94299054],\n",
       "       [0.95010048],\n",
       "       [0.71975023],\n",
       "       [0.74228632],\n",
       "       [0.801     ],\n",
       "       [0.84068853],\n",
       "       [0.75865132],\n",
       "       [0.87791437],\n",
       "       [0.74841374],\n",
       "       [0.75573462],\n",
       "       [0.73270679],\n",
       "       [0.93619913],\n",
       "       [0.72384787],\n",
       "       [0.79185581],\n",
       "       [0.8985433 ],\n",
       "       [0.84606349],\n",
       "       [0.90589511],\n",
       "       [0.93763041],\n",
       "       [0.86786562],\n",
       "       [0.79730588],\n",
       "       [0.77089423],\n",
       "       [0.86021072],\n",
       "       [0.79796982],\n",
       "       [0.80609304],\n",
       "       [0.72734851],\n",
       "       [0.89984   ],\n",
       "       [0.79702502],\n",
       "       [0.85450244],\n",
       "       [0.7559672 ],\n",
       "       [0.90919   ],\n",
       "       [0.88575476],\n",
       "       [0.92000043],\n",
       "       [0.81064326],\n",
       "       [0.80394226],\n",
       "       [0.79659581],\n",
       "       [0.89990175],\n",
       "       [0.88758177],\n",
       "       [0.83598858],\n",
       "       [0.87170464],\n",
       "       [0.82693475],\n",
       "       [0.8835029 ],\n",
       "       [0.82689798],\n",
       "       [0.87815142],\n",
       "       [0.90423948],\n",
       "       [0.89348269],\n",
       "       [0.91250181],\n",
       "       [0.86724675],\n",
       "       [0.80343509],\n",
       "       [0.89805895],\n",
       "       [0.86580259],\n",
       "       [0.80986595],\n",
       "       [0.8642025 ],\n",
       "       [0.80759555],\n",
       "       [0.7647056 ],\n",
       "       [0.84398937],\n",
       "       [0.83562845],\n",
       "       [0.86073393],\n",
       "       [0.83997023],\n",
       "       [0.87323838],\n",
       "       [0.88369304],\n",
       "       [0.88717616],\n",
       "       [0.73295724],\n",
       "       [0.9385134 ],\n",
       "       [0.82074416],\n",
       "       [0.88012058],\n",
       "       [0.95051157],\n",
       "       [0.84798664],\n",
       "       [0.80519772],\n",
       "       [0.85662544],\n",
       "       [0.8387959 ],\n",
       "       [0.8567521 ],\n",
       "       [0.81255788],\n",
       "       [0.84924811],\n",
       "       [0.83132339],\n",
       "       [0.94015384],\n",
       "       [0.8893553 ],\n",
       "       [0.91024101],\n",
       "       [0.92420739],\n",
       "       [0.89025563],\n",
       "       [0.84851211],\n",
       "       [0.80701399],\n",
       "       [0.93366152],\n",
       "       [0.8411935 ],\n",
       "       [0.78744   ],\n",
       "       [0.91060668],\n",
       "       [0.92244756],\n",
       "       [0.84414184],\n",
       "       [0.90463853],\n",
       "       [0.84018183],\n",
       "       [0.94044894],\n",
       "       [0.93889701],\n",
       "       [0.92341626],\n",
       "       [0.89086962],\n",
       "       [0.92508382],\n",
       "       [0.87916702],\n",
       "       [0.8891021 ],\n",
       "       [0.96017683],\n",
       "       [0.89672953],\n",
       "       [0.92605764],\n",
       "       [0.93973422],\n",
       "       [0.97963148],\n",
       "       [0.77888787],\n",
       "       [0.95824641],\n",
       "       [0.90411496],\n",
       "       [0.9159193 ],\n",
       "       [0.94839001],\n",
       "       [0.8612625 ],\n",
       "       [0.77271599],\n",
       "       [0.93851882],\n",
       "       [0.97308218],\n",
       "       [0.88085747],\n",
       "       [0.91473234],\n",
       "       [0.93633801],\n",
       "       [0.96508473],\n",
       "       [0.95286566],\n",
       "       [0.97729295],\n",
       "       [0.88261354],\n",
       "       [0.95589417],\n",
       "       [0.76123232],\n",
       "       [0.94538295],\n",
       "       [0.84255725],\n",
       "       [0.92868948],\n",
       "       [0.92255318],\n",
       "       [0.8518635 ],\n",
       "       [0.95237547],\n",
       "       [0.91061032],\n",
       "       [0.8391661 ],\n",
       "       [0.92011333],\n",
       "       [0.78948277],\n",
       "       [0.83498245],\n",
       "       [0.87768596],\n",
       "       [0.87799633],\n",
       "       [0.95260614],\n",
       "       [0.92127848],\n",
       "       [0.88097763],\n",
       "       [0.85833198],\n",
       "       [0.71257734],\n",
       "       [0.92160285],\n",
       "       [0.84472513],\n",
       "       [0.84521973],\n",
       "       [0.81293482],\n",
       "       [0.94212031],\n",
       "       [0.91649365],\n",
       "       [0.91707951],\n",
       "       [0.89445138],\n",
       "       [0.90578157],\n",
       "       [0.83126205],\n",
       "       [0.93747616],\n",
       "       [0.79976028],\n",
       "       [0.80297464],\n",
       "       [0.8276422 ],\n",
       "       [0.86428475],\n",
       "       [0.85743719],\n",
       "       [0.79541534],\n",
       "       [0.9761681 ],\n",
       "       [0.79120934],\n",
       "       [0.83851635],\n",
       "       [0.87269258],\n",
       "       [0.78855622],\n",
       "       [0.87017304],\n",
       "       [0.85878164],\n",
       "       [0.76331061],\n",
       "       [0.7265833 ],\n",
       "       [0.76480234],\n",
       "       [0.75497484],\n",
       "       [0.83158088],\n",
       "       [0.89530343],\n",
       "       [0.75331831],\n",
       "       [0.9139148 ],\n",
       "       [0.88633174],\n",
       "       [0.73633391],\n",
       "       [0.77950871],\n",
       "       [0.84675729],\n",
       "       [0.83304358],\n",
       "       [0.9342171 ],\n",
       "       [0.97167701],\n",
       "       [0.86252403],\n",
       "       [0.81489789],\n",
       "       [0.90257776],\n",
       "       [0.81589168],\n",
       "       [0.8119294 ],\n",
       "       [0.92247128],\n",
       "       [0.75576043],\n",
       "       [0.90716338],\n",
       "       [0.83801472],\n",
       "       [0.97997528],\n",
       "       [0.88146305],\n",
       "       [0.82007092],\n",
       "       [0.83206028],\n",
       "       [0.90798199],\n",
       "       [0.78160262],\n",
       "       [0.79467773],\n",
       "       [0.81631351],\n",
       "       [0.85735971],\n",
       "       [0.90588331],\n",
       "       [0.91744602],\n",
       "       [0.82703578],\n",
       "       [0.95247042],\n",
       "       [0.83217657],\n",
       "       [0.87045157],\n",
       "       [0.74985182],\n",
       "       [0.73035371],\n",
       "       [0.89444518],\n",
       "       [0.68478632],\n",
       "       [0.8016355 ],\n",
       "       [0.78372604],\n",
       "       [0.70884287],\n",
       "       [0.86982179],\n",
       "       [0.87249959],\n",
       "       [0.96828806],\n",
       "       [0.82136059],\n",
       "       [0.83263618],\n",
       "       [0.73615265],\n",
       "       [0.88406366],\n",
       "       [0.90054393],\n",
       "       [0.73057067],\n",
       "       [0.79843259],\n",
       "       [0.80063093],\n",
       "       [0.78554147],\n",
       "       [0.85214359],\n",
       "       [0.84440935],\n",
       "       [0.7197575 ],\n",
       "       [0.79276937],\n",
       "       [0.73434842],\n",
       "       [0.90432841],\n",
       "       [0.78240311],\n",
       "       [0.84198701],\n",
       "       [0.7618342 ],\n",
       "       [0.77573055],\n",
       "       [0.93515962],\n",
       "       [0.70383251],\n",
       "       [0.93236697],\n",
       "       [0.82384545],\n",
       "       [0.68493271],\n",
       "       [0.96020067],\n",
       "       [0.94793171],\n",
       "       [0.79393542],\n",
       "       [0.76177621],\n",
       "       [0.93536031],\n",
       "       [0.84558719],\n",
       "       [0.97822511],\n",
       "       [0.90955949],\n",
       "       [0.87957531],\n",
       "       [0.7413224 ],\n",
       "       [0.93557954],\n",
       "       [0.93972582],\n",
       "       [0.74210352],\n",
       "       [0.89238793],\n",
       "       [0.94503307],\n",
       "       [0.86525559],\n",
       "       [0.86912173],\n",
       "       [0.98846197],\n",
       "       [0.68959224],\n",
       "       [0.78196323],\n",
       "       [0.95300573],\n",
       "       [0.68228745],\n",
       "       [0.93723118],\n",
       "       [0.79836035],\n",
       "       [0.89685887],\n",
       "       [0.72236884],\n",
       "       [0.75542337],\n",
       "       [0.77904671],\n",
       "       [0.87761533],\n",
       "       [0.85676271],\n",
       "       [0.97077101],\n",
       "       [0.86035806],\n",
       "       [0.73972487],\n",
       "       [0.81922603],\n",
       "       [0.96702778],\n",
       "       [0.82865793],\n",
       "       [0.76778483],\n",
       "       [0.92769194],\n",
       "       [0.83845413],\n",
       "       [0.81236565],\n",
       "       [0.76861519],\n",
       "       [0.78570724],\n",
       "       [0.89496917],\n",
       "       [0.80231124],\n",
       "       [0.91858137],\n",
       "       [0.85510164],\n",
       "       [0.97346127],\n",
       "       [0.90494829],\n",
       "       [0.75335336],\n",
       "       [0.97479206],\n",
       "       [0.90850705],\n",
       "       [0.78006506],\n",
       "       [0.72605133],\n",
       "       [0.91476047],\n",
       "       [0.87349188],\n",
       "       [0.97240353],\n",
       "       [0.78551739],\n",
       "       [0.86945438],\n",
       "       [0.92367792],\n",
       "       [0.72160745],\n",
       "       [0.87551779],\n",
       "       [0.96352458],\n",
       "       [0.99304283],\n",
       "       [0.87699896],\n",
       "       [0.9575184 ],\n",
       "       [0.96815354],\n",
       "       [0.96944261],\n",
       "       [0.95288807],\n",
       "       [0.85527891],\n",
       "       [0.94810069],\n",
       "       [0.98283583],\n",
       "       [0.98308945],\n",
       "       [0.95822728],\n",
       "       [0.92813855],\n",
       "       [0.96211183],\n",
       "       [0.96872723],\n",
       "       [0.98173875],\n",
       "       [0.93429822],\n",
       "       [0.97355145],\n",
       "       [0.87137777],\n",
       "       [0.96716648],\n",
       "       [0.97211254],\n",
       "       [0.94600666],\n",
       "       [0.98191041],\n",
       "       [0.96679103],\n",
       "       [0.98896372],\n",
       "       [0.76181233],\n",
       "       [0.82017058],\n",
       "       [0.91007674],\n",
       "       [0.80242264],\n",
       "       [0.96899009],\n",
       "       [0.79648662],\n",
       "       [0.88855052],\n",
       "       [0.85472631],\n",
       "       [0.83858591],\n",
       "       [0.93170005],\n",
       "       [0.94118357],\n",
       "       [0.9925372 ],\n",
       "       [0.86000842],\n",
       "       [0.89073884],\n",
       "       [0.82730234],\n",
       "       [0.8447693 ],\n",
       "       [0.93270576],\n",
       "       [0.95096576],\n",
       "       [0.82471478],\n",
       "       [0.97809464],\n",
       "       [0.9437288 ],\n",
       "       [0.96947473],\n",
       "       [0.78734577],\n",
       "       [0.94128186],\n",
       "       [0.96267968],\n",
       "       [0.98866946],\n",
       "       [0.87952375],\n",
       "       [0.83552074],\n",
       "       [0.83081365],\n",
       "       [0.95132256],\n",
       "       [0.84686929],\n",
       "       [0.81872582],\n",
       "       [0.99439383],\n",
       "       [0.96260494],\n",
       "       [0.92919087],\n",
       "       [0.93613893],\n",
       "       [0.80373406],\n",
       "       [0.79686987],\n",
       "       [0.95685291],\n",
       "       [0.83074033],\n",
       "       [0.73340541],\n",
       "       [0.89679128],\n",
       "       [0.79781908],\n",
       "       [0.91571611],\n",
       "       [0.79965353],\n",
       "       [0.79269248],\n",
       "       [0.83846003],\n",
       "       [0.84056401],\n",
       "       [0.89460641],\n",
       "       [0.85071808],\n",
       "       [0.91620308],\n",
       "       [0.76531339],\n",
       "       [0.92887658],\n",
       "       [0.98717552],\n",
       "       [0.82729065],\n",
       "       [0.89030021],\n",
       "       [0.95539761],\n",
       "       [0.81478095],\n",
       "       [0.97828931],\n",
       "       [0.95656258],\n",
       "       [0.82836407],\n",
       "       [0.90254921],\n",
       "       [0.87240404],\n",
       "       [0.82077247],\n",
       "       [0.7411738 ],\n",
       "       [0.91400439],\n",
       "       [0.80771881],\n",
       "       [0.97956604],\n",
       "       [0.93799484],\n",
       "       [0.9336316 ],\n",
       "       [0.82425165],\n",
       "       [0.97543794],\n",
       "       [0.81787771],\n",
       "       [0.94463956],\n",
       "       [0.83391118],\n",
       "       [0.95003957],\n",
       "       [0.93787855],\n",
       "       [0.78218138],\n",
       "       [0.86094594],\n",
       "       [0.90217012],\n",
       "       [0.94661558],\n",
       "       [0.83083022],\n",
       "       [0.93959528],\n",
       "       [0.87465686],\n",
       "       [0.94901901],\n",
       "       [0.89531142],\n",
       "       [0.96024638],\n",
       "       [0.96460664],\n",
       "       [0.81898028],\n",
       "       [0.93556166],\n",
       "       [0.75970733],\n",
       "       [0.95003319],\n",
       "       [0.83130753],\n",
       "       [0.80543971],\n",
       "       [0.78173655],\n",
       "       [0.94855857],\n",
       "       [0.91163093],\n",
       "       [0.98817408],\n",
       "       [0.85017085],\n",
       "       [0.79764032],\n",
       "       [0.89990282],\n",
       "       [0.75251281],\n",
       "       [0.80755079],\n",
       "       [0.83176351],\n",
       "       [0.9589808 ],\n",
       "       [0.95816672],\n",
       "       [0.86525345],\n",
       "       [0.81812388],\n",
       "       [0.85823679],\n",
       "       [0.88424474],\n",
       "       [0.83568436],\n",
       "       [0.98855758],\n",
       "       [0.87402076],\n",
       "       [0.82347178],\n",
       "       [0.90312892],\n",
       "       [0.96977782],\n",
       "       [0.86107713],\n",
       "       [0.9255957 ],\n",
       "       [0.98048222],\n",
       "       [0.88473988],\n",
       "       [0.84438658],\n",
       "       [0.95936501],\n",
       "       [0.91088521],\n",
       "       [0.82030934],\n",
       "       [0.97783613],\n",
       "       [0.97274011],\n",
       "       [0.88305759],\n",
       "       [0.8818478 ],\n",
       "       [0.99516076],\n",
       "       [0.90029144],\n",
       "       [0.82095873],\n",
       "       [0.93787318],\n",
       "       [0.84016925],\n",
       "       [0.82644331],\n",
       "       [0.98811227],\n",
       "       [0.87976587],\n",
       "       [0.99320883],\n",
       "       [0.8439033 ],\n",
       "       [0.78456068],\n",
       "       [0.85700393],\n",
       "       [0.86557353],\n",
       "       [0.90830272],\n",
       "       [0.91673774],\n",
       "       [0.87875926],\n",
       "       [0.96436787],\n",
       "       [0.98253906],\n",
       "       [0.81222385],\n",
       "       [0.92033494],\n",
       "       [0.99465162],\n",
       "       [0.94015771],\n",
       "       [0.95651984],\n",
       "       [0.72202504],\n",
       "       [0.80170375],\n",
       "       [0.95939857],\n",
       "       [0.93560266],\n",
       "       [0.97375387],\n",
       "       [0.84189981],\n",
       "       [0.9843387 ],\n",
       "       [0.78630883],\n",
       "       [0.74440712],\n",
       "       [0.77243906],\n",
       "       [0.88930577],\n",
       "       [0.94977742],\n",
       "       [0.98602676],\n",
       "       [0.99037266],\n",
       "       [0.92768651],\n",
       "       [0.96544838],\n",
       "       [0.95348853],\n",
       "       [0.93388021],\n",
       "       [0.81891161],\n",
       "       [0.75984699],\n",
       "       [0.99073863],\n",
       "       [0.88892734],\n",
       "       [0.92860019],\n",
       "       [0.8598398 ],\n",
       "       [0.87774324],\n",
       "       [0.89283526],\n",
       "       [0.92343205],\n",
       "       [0.99785894],\n",
       "       [0.90935647],\n",
       "       [0.86296105],\n",
       "       [0.74006206],\n",
       "       [0.98768789],\n",
       "       [0.97162962],\n",
       "       [0.99591666],\n",
       "       [0.98047423],\n",
       "       [0.9897849 ],\n",
       "       [0.95294058],\n",
       "       [0.86249304],\n",
       "       [0.77415848],\n",
       "       [0.83565378],\n",
       "       [0.97422904],\n",
       "       [0.91158652],\n",
       "       [0.95981318],\n",
       "       [0.86177433],\n",
       "       [0.85831201],\n",
       "       [0.86797655],\n",
       "       [0.91629785],\n",
       "       [0.97698486],\n",
       "       [0.8102144 ],\n",
       "       [0.97300297],\n",
       "       [0.96525675],\n",
       "       [0.8592754 ],\n",
       "       [0.83447546],\n",
       "       [0.82267869],\n",
       "       [0.8266896 ],\n",
       "       [0.96598846],\n",
       "       [0.89329994],\n",
       "       [0.98598284],\n",
       "       [0.97603661],\n",
       "       [0.9642846 ],\n",
       "       [0.92610061],\n",
       "       [0.97173589],\n",
       "       [0.79399836],\n",
       "       [0.8249892 ],\n",
       "       [0.97945172],\n",
       "       [0.83197016],\n",
       "       [0.9072662 ],\n",
       "       [0.77281892],\n",
       "       [0.95001781],\n",
       "       [0.92366672],\n",
       "       [0.95546931],\n",
       "       [0.9939751 ],\n",
       "       [0.90539765],\n",
       "       [0.90315378],\n",
       "       [0.87149572],\n",
       "       [0.87956154],\n",
       "       [0.9668628 ],\n",
       "       [0.6821959 ],\n",
       "       [0.9867453 ],\n",
       "       [0.9582842 ],\n",
       "       [0.9731679 ],\n",
       "       [0.91050965],\n",
       "       [0.87670314],\n",
       "       [0.92064542],\n",
       "       [0.92597502],\n",
       "       [0.88168979],\n",
       "       [0.96708608],\n",
       "       [0.97790819],\n",
       "       [0.83685434],\n",
       "       [0.83927035],\n",
       "       [0.84070647],\n",
       "       [0.83536446],\n",
       "       [0.94744545],\n",
       "       [0.82456744],\n",
       "       [0.92712754],\n",
       "       [0.99912804],\n",
       "       [0.98093629],\n",
       "       [0.99065125],\n",
       "       [0.81623244],\n",
       "       [0.97545946],\n",
       "       [0.86889619],\n",
       "       [0.99994886],\n",
       "       [0.82716316],\n",
       "       [0.93696761],\n",
       "       [0.91974187],\n",
       "       [0.93084061],\n",
       "       [0.98915488],\n",
       "       [0.96019566],\n",
       "       [0.91750109],\n",
       "       [0.98546672],\n",
       "       [0.90059614],\n",
       "       [0.89453965],\n",
       "       [0.75623149],\n",
       "       [0.90014291],\n",
       "       [0.9123165 ],\n",
       "       [0.94183546],\n",
       "       [0.92680979],\n",
       "       [0.90923947],\n",
       "       [0.9696638 ],\n",
       "       [0.95398235],\n",
       "       [0.79778624],\n",
       "       [0.93989229],\n",
       "       [0.88605976],\n",
       "       [0.9929322 ],\n",
       "       [0.96928096],\n",
       "       [0.99704486],\n",
       "       [0.88796943],\n",
       "       [0.88344872],\n",
       "       [0.89779073],\n",
       "       [0.99318236],\n",
       "       [0.89562237],\n",
       "       [0.99419135],\n",
       "       [0.71993607],\n",
       "       [0.79555613],\n",
       "       [0.89909852],\n",
       "       [0.8674131 ],\n",
       "       [0.89694154],\n",
       "       [0.91750163],\n",
       "       [0.89154476],\n",
       "       [0.79361761],\n",
       "       [0.84492362],\n",
       "       [0.87617022],\n",
       "       [0.91915131],\n",
       "       [0.9040733 ],\n",
       "       [0.92287731],\n",
       "       [0.92495781],\n",
       "       [0.88167781],\n",
       "       [0.78685033],\n",
       "       [0.92121798],\n",
       "       [0.95925933],\n",
       "       [0.88616651]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제값과 예측값 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 실제 값과 예측값을 시각화하는 코드\n",
    "plt.figure(figsize=(10, 6))  # 그래프 크기 설정\n",
    "plt.plot(actual, label='Actual Values')  # 실제 값 그래프\n",
    "plt.plot(pred, label='Predicted Values', alpha=0.7)  # 예측 값 그래프, 투명도를 주어 구분하기 쉽게 함\n",
    "plt.title('Actual vs Predicted Values')  # 그래프 제목\n",
    "plt.xlabel('Sample Index')  # x축 라벨\n",
    "plt.ylabel('Value')  # y축 라벨\n",
    "plt.legend()  # 범례 표시\n",
    "plt.show()  # 그래프 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "l = 24\n",
    "\n",
    "# 관리상태 / 이상상태 데이터 생성 함수\n",
    "def armagen(phi1, theta, psi, delta, gamma, length) :\n",
    "\n",
    "    e = np.random.normal(loc=0, scale = 1,size = length)\n",
    "    sigma = math.sqrt((1 + 2 * phi1 * theta + pow(theta, 2)) / (1 - pow(phi1, 2))) \n",
    "    x = np.array(np.repeat(0, length), dtype= np.float64)\n",
    "    x[0] = e[0]\n",
    "    z = np.array(np.repeat(0, length), dtype=np.float64)\n",
    "\n",
    "    for i in range(1, psi):\n",
    "        x[i] = phi1 * x[i-1] + e[i] - theta * e[i-1]\n",
    "        z[i] = x[i]\n",
    "    for i in range(psi,len(x)):\n",
    "        x[i] = phi1 * x[i-1] + gamma * e[i] - theta * e[i-1]\n",
    "        z[i] = x[i]\n",
    "    for i in range(psi,len(z)):\n",
    "        z[i] = z[i] + delta * sigma\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arl(phi1, theta, delta,gamma, run, length,cl) :\n",
    "    rl = np.array([], dtype=np.float64)\n",
    "\n",
    "    for i in tqdm(range(run)) :\n",
    "        y = armagen(phi1=phi1, theta = theta, psi=l-1, delta=delta, gamma = gamma,length=length)\n",
    "        a = np.array([length-l])\n",
    "        x = np.zeros(shape=(length-l, l)) # empty 대신 zeros\n",
    "        for j in range(length-l):\n",
    "            x[j] = y[j: j + l]\n",
    "            \n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j in range(0,len(x)):\n",
    "                input = x[[j]]      # shape 형태\n",
    "\n",
    "                output = model(input)\n",
    "\n",
    "                if output[0] > cl :\n",
    "                    a = np.array([j + 1])\n",
    "                    break\n",
    "                elif j == len(x):\n",
    "                    a = len(x)\n",
    "\n",
    "            rl = np.append(rl,a)\n",
    "\n",
    "    arl = np.mean(rl)\n",
    "    return arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "790.828"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 1000, length = 1000, cl = 0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 18.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "770.526"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 1000, length = 1000, cl = 0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=(10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arl1(phi1, theta, run,length,cl):\n",
    "    a5 = arl(phi1, theta, 0.5, 1, run, length, cl)\n",
    "    a1 = arl(phi1, theta, 1, 1, run, length, cl)\n",
    "    a2 = arl(phi1, theta, 2, 1, run, length, cl)\n",
    "    a3 = arl(phi1, theta, 3, 1, run, length, cl)\n",
    "    b5 = arl(phi1, theta, 0.5, 1.5,run, length, cl)\n",
    "    b1 = arl(phi1, theta, 1, 1.5, run, length, cl)\n",
    "    b2 = arl(phi1, theta, 2, 1.5, run, length, cl)\n",
    "    b3 = arl(phi1, theta, 3, 1, run, length, cl)\n",
    "    c1 = arl(phi1, theta, 0, 1.5, run, length, cl)\n",
    "    c2 = arl(phi1, theta, 0, 2, run, length, cl)\n",
    "    c3 = arl(phi1, theta, 0, 3, run, length, cl)\n",
    "    print(f'0.5: {a5}, 1:{a1},2:{a2},3:{a3}')\n",
    "    print(f'0.5:{b5},1:{b1},2:{b2},3:{b3}')\n",
    "    print(f'1.5:{c1},2:{c2},3:{c3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.2, theta = -0.2 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [07:29<00:00, 22.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "782.7461"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:32<00:00, 65.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "257.3008"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:53<00:00, 34.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "506.347"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:32<00:00, 47.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "364.5542"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:48<00:00, 43.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "394.3494"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.926)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:35<00:00, 46.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "374.2658"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:33<00:00, 46.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "369.8483"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:34<00:00, 46.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "374.2256"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.92535)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:36<00:00, 275.75it/s]\n",
      "100%|██████████| 10000/10000 [00:16<00:00, 606.96it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 924.51it/s]\n",
      "100%|██████████| 10000/10000 [00:09<00:00, 1056.55it/s]\n",
      "100%|██████████| 10000/10000 [00:20<00:00, 493.25it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 695.63it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 932.93it/s]\n",
      "100%|██████████| 10000/10000 [00:09<00:00, 1053.82it/s]\n",
      "100%|██████████| 10000/10000 [00:39<00:00, 251.74it/s]\n",
      "100%|██████████| 10000/10000 [00:21<00:00, 455.26it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 701.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 51.3561, 1:15.6918,2:5.4799,3:3.1322\n",
      "0.5:22.3631,1:11.7541,2:5.2671,3:3.1381\n",
      "1.5:57.0259,2:25.3908,3:11.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(phi1=0.2, theta = -0.2, run=10000, length=1000, cl=0.9253)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.2, theta = -0.8 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:52<00:00, 188.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81.3287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:56<00:00, 56.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "302.3867"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:48<00:00, 28.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "611.2836"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:56<00:00, 42.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "410.0827"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.954)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:23<00:00, 49.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "354.3022"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:43<00:00, 44.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "391.0003"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:33<00:00, 46.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "371.4289"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.2, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:47<00:00, 210.61it/s]\n",
      "100%|██████████| 10000/10000 [00:20<00:00, 483.82it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 831.40it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 993.59it/s]\n",
      "100%|██████████| 10000/10000 [00:27<00:00, 361.71it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 574.88it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 848.96it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 998.99it/s]\n",
      "100%|██████████| 10000/10000 [01:00<00:00, 163.95it/s]\n",
      "100%|██████████| 10000/10000 [00:32<00:00, 310.02it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 543.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 70.8224, 1:22.9899,2:7.5159,3:4.1006\n",
      "0.5:35.5315,1:17.1962,2:7.085,3:4.0877\n",
      "1.5:95.2733,2:43.636,3:18.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(phi1=0.2, theta = -0.8, run=10000, length=1000, cl=0.9525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.4, theta = -0.4 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:29<00:00, 47.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "360.33"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:50<00:00, 34.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "509.5376"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.955)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:02<00:00, 41.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423.701"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:42<00:00, 44.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "387.3558"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.951)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:33<00:00, 46.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "371.8089"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:32<00:00, 47.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "368.6519"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:34<00:00, 46.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "373.423"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9503)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:33<00:00, 46.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "370.7984"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.4, theta = -0.4, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.95025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:57<00:00, 175.36it/s]\n",
      "100%|██████████| 10000/10000 [00:25<00:00, 397.75it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 753.79it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 945.16it/s]\n",
      "100%|██████████| 10000/10000 [00:27<00:00, 360.23it/s]\n",
      "100%|██████████| 10000/10000 [00:19<00:00, 521.91it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 781.28it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 935.20it/s]\n",
      "100%|██████████| 10000/10000 [00:52<00:00, 189.10it/s]\n",
      "100%|██████████| 10000/10000 [00:28<00:00, 347.35it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 571.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 88.3139, 1:31.0203,2:9.7291,3:5.0722\n",
      "0.5:35.8561,1:20.3327,2:8.9624,3:5.1784\n",
      "1.5:79.5719,2:37.1351,3:17.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(phi1=0.4, theta = -0.4, run=10000, length=1000, cl=0.95025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.8, theta = -0.2 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:29<00:00, 30.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "581.7198"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1 = 0.8, theta = -0.2, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.988495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:31<00:00, 65.88it/s]\n",
      "100%|██████████| 10000/10000 [01:05<00:00, 153.58it/s]\n",
      "100%|██████████| 10000/10000 [00:22<00:00, 446.18it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 711.54it/s]\n",
      "100%|██████████| 10000/10000 [00:55<00:00, 180.08it/s]\n",
      "100%|██████████| 10000/10000 [00:36<00:00, 276.65it/s]\n",
      "100%|██████████| 10000/10000 [00:19<00:00, 501.11it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 711.14it/s]\n",
      "100%|██████████| 10000/10000 [01:33<00:00, 107.39it/s]\n",
      "100%|██████████| 10000/10000 [00:50<00:00, 199.39it/s]\n",
      "100%|██████████| 10000/10000 [00:28<00:00, 351.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 259.6065, 1:103.2527,2:26.3821,3:11.374\n",
      "0.5:84.5914,1:50.9058,2:21.801,3:11.3911\n",
      "1.5:154.1462,2:76.2075,3:37.2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(phi1=0.8, theta = -0.2, run=10000, length=1000, cl=0.988495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.8, theta = -0.8 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:36<00:00, 29.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "610.5634"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(phi1=0.8, theta = -0.8, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.997524)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:03<00:00, 41.07it/s]\n",
      "100%|██████████| 10000/10000 [02:43<00:00, 61.29it/s]\n",
      "100%|██████████| 10000/10000 [01:12<00:00, 137.57it/s]\n",
      "100%|██████████| 10000/10000 [00:38<00:00, 259.81it/s]\n",
      "100%|██████████| 10000/10000 [01:39<00:00, 100.43it/s]\n",
      "100%|██████████| 10000/10000 [01:14<00:00, 134.92it/s]\n",
      "100%|██████████| 10000/10000 [00:43<00:00, 227.68it/s]\n",
      "100%|██████████| 10000/10000 [00:38<00:00, 260.40it/s]\n",
      "100%|██████████| 10000/10000 [02:18<00:00, 72.44it/s]\n",
      "100%|██████████| 10000/10000 [01:12<00:00, 138.36it/s]\n",
      "100%|██████████| 10000/10000 [00:38<00:00, 260.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 439.01, 1:288.9004,2:120.926,3:57.3125\n",
      "0.5:171.1143,1:123.6757,2:67.2496,3:57.0942\n",
      "1.5:241.9267,2:119.7791,3:56.9063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(phi1=0.8, theta = -0.8, run=10000, length=1000, cl=0.997524)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
