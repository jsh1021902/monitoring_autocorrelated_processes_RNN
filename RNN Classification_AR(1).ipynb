{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pytorchtools import EarlyStopping\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy & Python Version 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 버전: 1.26.0\n",
      "Python 버전: 3.10.12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "# NumPy 버전 확인\n",
    "numpy_version = np.__version__\n",
    "\n",
    "# Python 버전 확인\n",
    "python_version = platform.python_version()\n",
    "\n",
    "print(f\"NumPy 버전: {numpy_version}\")\n",
    "print(f\"Python 버전: {python_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA 사용 및 EarlyStopping 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "early_stopping = EarlyStopping(patience = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맥북 M2 MAX는 NVIDEA GPU를 사용하지 않아 CUDA 지원 안 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    torch.save(model.state_dict(), 'model_ar1.pt') # 모델의 학습된 매개변수 파일에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "length = 24            # 윈도우 사이즈 (생성할 시계열 데이터의 길이)\n",
    "hidden_size1 = 48      # 은닉층 1 크기\n",
    "hidden_size2 = 24      # 은닉층 2 크기\n",
    "hidden_size3 = 12      # 은닉층 3 크기\n",
    "hidden_size4 = 6       # 은닉층 4 크기\n",
    "learning_rate = 1e-6   # 학습률\n",
    "epoch = 400\n",
    "trainrun = 50          # 생성할 학습 데이터 시퀀스의 수 \n",
    "testrun = 25           # 생성할 평가 데이터 시퀀스의 수 \n",
    "validrun = 25          # 생성할 검증 데이터 시퀀스의 수 \n",
    "\n",
    "# 시계열 데이터 생성을 위한 매개변수\n",
    "# 자기상관계수 (phi값을 0, 0.25, 0.5, 0.75, 0.95별로 생성)\n",
    "phi1 = np.array([np.repeat(0,8),\n",
    "                 np.repeat(0.25,8),\n",
    "                 np.repeat(0.5,8),\n",
    "                 np.repeat(0.75,8),\n",
    "                 np.repeat(0.95,8)])\n",
    "phi1 = np.concatenate(phi1)\n",
    "\n",
    "# 변화율 크기 (= 이상상태 포함 정도, psi) * 논문과 수치가 약간 변동이 있음\n",
    "psi1 = np.array([0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,\n",
    "                 0, 10, 15, 20, 23, 14, 17, 23,])\n",
    "\n",
    "# 공정의 수준 변화율 (delta)\n",
    "de1 = np.array([0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "                0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "                0, 0.5, 1, 2, 3, 0, 0, 0,])\n",
    "\n",
    "# 공정의 분산 변화율 (gamma)\n",
    "ga = np.array([1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,\n",
    "               1, 1, 1, 1, 1, 1.5, 2, 3,])             0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "                0, 0.5, 1, 2, 3, 0, 0, 0,\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시계열 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# AR(1) 시계열 데이터 생성 함수\n",
    "def ar(ar1, delta, gamma, psi, length, run):\n",
    "    # 초기 설정\n",
    "    y = np.zeros(shape=(run, length))                           # 생성될 시계열 데이터를 저장할 빈 배열을 초기화. 배열의 크기는 (생성할 데이터 시퀀스의 수, 각 시퀀스의 길이) \n",
    "    sigma = math.sqrt(1 / (1 - pow(ar1, 2)))                    # AR(1)모델의 표준 편차\n",
    "    \n",
    "    # 데이터 시퀀스 생성\n",
    "    for j in range(0, run):                                     # 각 run 마다 랜덤 노이즈(e)를 정규분포에서 추출하여 시계열의 기본 노이즈 생성 (과적합 방지 차원)\n",
    "        e = np.random.normal(loc=0, scale=1, size=length)       \n",
    "        x = np.array(np.repeat(0, length), dtype=np.float64)\n",
    "        \n",
    "        x[0] = e[0]                                             # x 배열 초기화하고, 첫 번째 시점의 값은 첫 번째 노이즈 값으로 설정 (시계열의 시작점에서 발생할 수 있는 임의성 반영 및 자기상관 구조 구현)\n",
    "\n",
    "        # psi 시점 이전의 데이터 생성\n",
    "        for i in range(1, psi):                                 # psi 시점 이전까지는 관리상태 데이터\n",
    "            x[i] = ar1 * x[i - 1] + e[i]                        # 각 시점에서의 값은 이전 시점의 값에 자기상관 계수 ar1을 곱한 것과 현재 시점의 노이즈를 더한 값으로 설정\n",
    "            \n",
    "        # psi 시점 이후의 데이터 생성 및 변동성 적용\n",
    "        for i in range(psi,len(x)):                             # psi 시점 이후에는 각 에러 항에 gamma 값을 곱하여 에러 항의 변동성을 조절 \n",
    "            e[i] = gamma * e[i]\n",
    "            x[i] = ar1 * x[i-1] + e[i]\n",
    "        for i in range(psi,len(x)):                             # delta(변동성 크기 조절하는 매개변수)를 통한 추가 변동성 적용\n",
    "            x[i] = x[i] + delta*sigma\n",
    "        \n",
    "        # 최종 데이터 반환 (각 run에 대해 생성된 시계열 데이터를 저장)  \n",
    "        y[j] = x\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# 다양한 매개변수 조합에 대한 시계열 데이터 세트 생성\n",
    "def totaldat(run,length):\n",
    "    # 빈 데이터 배열 초기화\n",
    "    y = np.zeros(shape=(len(phi1), run, length))\n",
    "    # 매개변수 조합별 데이터 생성\n",
    "    for i in range(len(phi1)):\n",
    "        y[i]= ar(phi1[i], de1[i], ga[i], psi1[i], length, run)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# 훈련용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "train_x = totaldat(trainrun,length)                             # 훈련용 시계열 데이터 생성\n",
    "train_x = train_x.reshape(trainrun*len(phi1),length)            # 생성된 훈련용 데이터를 적절한 형태로 재배열\n",
    "\n",
    "# 레이블 생성 및 변형\n",
    "train_y =  [np.repeat(0,trainrun),np.repeat(1,trainrun*7),      # 관리상태(1가지)는 0, 이상상태(7가지)는 1로 가정\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),\n",
    "            np.repeat(0,trainrun),np.repeat(1,trainrun*7),]\n",
    "train_y =  np.concatenate(train_y)\n",
    "train_y = train_y.reshape(2000,1)                               # 최종 레이블 배열의 형태를 조정 (학습용 데이터 세트 2000개)\n",
    "\n",
    "# 평가용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "test_x = totaldat(run = testrun, length = length)\n",
    "test_x = test_x.reshape(testrun*len(phi1),length)\n",
    "\n",
    "# 레이블 생성 및 변형\n",
    "test_y = [np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),\n",
    "          np.repeat(0,testrun),np.repeat(1,testrun*7),]\n",
    "test_y = np.concatenate(test_y)\n",
    "test_y = test_y.reshape(1000,1)                                 # 최종 레이블 배열의 형태를 조정 (평가용 데이터 세트 1000개)\n",
    "\n",
    "# 검증용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "valid_x = totaldat(run = validrun, length = length)\n",
    "valid_x = valid_x.reshape(validrun*len(phi1),length)\n",
    "\n",
    "# 레이블 생성 및 변형\n",
    "valid_y = [np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),\n",
    "            np.repeat(0,validrun),np.repeat(1,validrun*7),]\n",
    "valid_y = np.concatenate(valid_y)\n",
    "valid_y = valid_y.reshape(1000,1)                                 # 최종 레이블 배열의 형태를 조정 (검증용 데이터 세트 1000개)\n",
    "\n",
    "\n",
    "# PyTorch 텐서로 변환 및 장치 할당\n",
    "train_x = torch.FloatTensor(train_x).to(device)\n",
    "train_y = torch.FloatTensor(train_y).to(device)\n",
    "test_x = torch.FloatTensor(test_x).to(device)\n",
    "test_y = torch.FloatTensor(test_y).to(device)\n",
    "valid_x = torch.FloatTensor(valid_x).to(device)\n",
    "valid_y = torch.FloatTensor(valid_y).to(device)\n",
    "\n",
    "# DataLoader 설정\n",
    "trainset = TensorDataset(train_x, train_y)                        # 데이터와 레이블 쌍을 포함하는 데이터셋 생성\n",
    "trainloader = DataLoader(trainset, shuffle=True)                  # 데이터셋에서 미니배치 자동으로 생성 후 모델 학습 및 평가 시 배치 처리를 용이하게 함 (훈련에서는 데이터를 섞어 학습 과정에서의 일반화 능력 향상)\n",
    "testset = TensorDataset(test_x, test_y)\n",
    "testloader = DataLoader(testset,shuffle = False)                  # 학습 및 검증에서는 데이터 순서 유지\n",
    "validset = TensorDataset(valid_x, valid_y)\n",
    "validloader = DataLoader(validset,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    # 클래스 초기화\n",
    "    def __init__(self, input_size, hidden_size, num_layers, device):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.device = device                                          \n",
    "        self.hidden_size = hidden_size                                # RNN 모델의 은닉층 크기 \n",
    "        self.num_layers = num_layers                                  # RNN 모델의 층 개수\n",
    "        \n",
    "        # 기본 RNN 레이어 생성\n",
    "        self.rnn = nn.RNN(input_size= input_size,\n",
    "                          hidden_size = hidden_size1,\n",
    "                          num_layers = num_layers,\n",
    "                          nonlinearity= \"relu\",                       # 활성화 함수로 relu를 사용\n",
    "                          batch_first= True)                          # 입력 텐서의 첫 번째 차원이 배치 크기임을 나타냄\n",
    "        \n",
    "        # 완전 연결 레이어 (여러 레이어를 연속적으로 적용할 수 있게 하기 위해 nn.Sequential을 사용)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size,hidden_size2),  # 선형 레이어\n",
    "                                nn.GELU(),                            # GELU 활성화 함수\n",
    "                                nn.Linear(hidden_size2,hidden_size3),\n",
    "                                nn.GELU(),\n",
    "                                nn.Linear(hidden_size3,hidden_size4),\n",
    "                                nn.GELU(),\n",
    "                                nn.Linear(hidden_size4,1),\n",
    "                                nn.Sigmoid()                          # 시그모이드 활성화함수를 사용해 출력을 [0, 1] 범위로 조정\n",
    "                                )\n",
    "\n",
    "    # 순전파 (forward)\n",
    "    def forward(self, x):\n",
    "        # 초기 hidden state 설정\n",
    "        h0 = torch.zeros(x.size()[0], self.hidden_size).to(device)    # 각 배치에 대한 초기 은닉층을 0으로 설정 (= RNN의 첫 번째 시점에서 이전 상태가 없음을 의미)\n",
    "        \n",
    "        # RNN 레이어 실행\n",
    "        out, _ = self.rnn(x, h0)                                      # 입력 데이터 x와 초기 은닉층 h0를 RNN 층에 전달함. out: RNN의 마지막 레이어로부터 나온 output feature 를 반환 (hn: hidden state를 반환)\n",
    "        \n",
    "        # 데이터 재구성 및 완전 연결 레이어 실행\n",
    "        out = out.reshape(out.shape[0], -1)                           # many to many 전략 : 시퀀스의 각 타임 스텝에 대응하는 출력 생성 \n",
    "        out = self.fc(out)                                            # out을 재구성하여 모든 시간 단계의 출력을 하나의 벡터로 평탄화하고 완전 연결 레이어를 통해 최종 예측값을 계산\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "model = NeuralNetwork(input_size = length, hidden_size = hidden_size1, num_layers = 1, device = device).to(device)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 손실 기록을 위한 리스트 초기화\n",
    "loss_ = []                                                            # 훈련 과정에서의 손실값 기록\n",
    "n = len(trainloader)\n",
    "valoss_ = []                                                          # 검증 과정에서의 손실값 기록\n",
    "logger = {\"train_loss\": list(),\n",
    "          \"validation_loss\": list()\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "train loss : 0.18339458154141902, validation loss : 0.18287986285984517\n",
      "Validation loss decreased (inf --> 0.182880).  Saving model ...\n",
      "epoch 2\n",
      "train loss : 0.18297171778976917, validation loss : 0.18244530859589578\n",
      "Validation loss decreased (0.182880 --> 0.182445).  Saving model ...\n",
      "epoch 3\n",
      "train loss : 0.18253963969647882, validation loss : 0.18200153190394244\n",
      "Validation loss decreased (0.182445 --> 0.182002).  Saving model ...\n",
      "epoch 4\n",
      "train loss : 0.18209899440594016, validation loss : 0.18154845437034967\n",
      "Validation loss decreased (0.182002 --> 0.181548).  Saving model ...\n",
      "epoch 5\n",
      "train loss : 0.1816493692368269, validation loss : 0.1810861652120948\n",
      "Validation loss decreased (0.181548 --> 0.181086).  Saving model ...\n",
      "epoch 6\n",
      "train loss : 0.1811899957321584, validation loss : 0.18061375958969195\n",
      "Validation loss decreased (0.181086 --> 0.180614).  Saving model ...\n",
      "epoch 7\n",
      "train loss : 0.18072123059949702, validation loss : 0.1801316195407084\n",
      "Validation loss decreased (0.180614 --> 0.180132).  Saving model ...\n",
      "epoch 8\n",
      "train loss : 0.1802423708527349, validation loss : 0.17963835771847517\n",
      "Validation loss decreased (0.180132 --> 0.179638).  Saving model ...\n",
      "epoch 9\n",
      "train loss : 0.1797531144337522, validation loss : 0.17913422007858754\n",
      "Validation loss decreased (0.179638 --> 0.179134).  Saving model ...\n",
      "epoch 10\n",
      "train loss : 0.17925288042165338, validation loss : 0.1786188238352537\n",
      "Validation loss decreased (0.179134 --> 0.178619).  Saving model ...\n",
      "epoch 11\n",
      "train loss : 0.1787421670688147, validation loss : 0.1780928209749135\n",
      "Validation loss decreased (0.178619 --> 0.178093).  Saving model ...\n",
      "epoch 12\n",
      "train loss : 0.17822006236109886, validation loss : 0.17755453344496586\n",
      "Validation loss decreased (0.178093 --> 0.177555).  Saving model ...\n",
      "epoch 13\n",
      "train loss : 0.17768623681681664, validation loss : 0.17700469129647198\n",
      "Validation loss decreased (0.177555 --> 0.177005).  Saving model ...\n",
      "epoch 14\n",
      "train loss : 0.17714068944885267, validation loss : 0.17644284109930905\n",
      "Validation loss decreased (0.177005 --> 0.176443).  Saving model ...\n",
      "epoch 15\n",
      "train loss : 0.17658339549700422, validation loss : 0.1758687551230192\n",
      "Validation loss decreased (0.176443 --> 0.175869).  Saving model ...\n",
      "epoch 16\n",
      "train loss : 0.1760146051091142, validation loss : 0.17528325129440053\n",
      "Validation loss decreased (0.175869 --> 0.175283).  Saving model ...\n",
      "epoch 17\n",
      "train loss : 0.1754342955206247, validation loss : 0.1746861074943753\n",
      "Validation loss decreased (0.175283 --> 0.174686).  Saving model ...\n",
      "epoch 18\n",
      "train loss : 0.17484261075096827, validation loss : 0.17407735823508766\n",
      "Validation loss decreased (0.174686 --> 0.174077).  Saving model ...\n",
      "epoch 19\n",
      "train loss : 0.17423990266397596, validation loss : 0.17345794909682713\n",
      "Validation loss decreased (0.174077 --> 0.173458).  Saving model ...\n",
      "epoch 20\n",
      "train loss : 0.1736261431685649, validation loss : 0.17282746737375856\n",
      "Validation loss decreased (0.173458 --> 0.172827).  Saving model ...\n",
      "epoch 21\n",
      "train loss : 0.17300178743384423, validation loss : 0.17218714464136534\n",
      "Validation loss decreased (0.172827 --> 0.172187).  Saving model ...\n",
      "epoch 22\n",
      "train loss : 0.17236737985121595, validation loss : 0.1715371965420517\n",
      "Validation loss decreased (0.172187 --> 0.171537).  Saving model ...\n",
      "epoch 23\n",
      "train loss : 0.17172322568839984, validation loss : 0.17087783080760552\n",
      "Validation loss decreased (0.171537 --> 0.170878).  Saving model ...\n",
      "epoch 24\n",
      "train loss : 0.1710693056024611, validation loss : 0.17020914881117644\n",
      "Validation loss decreased (0.170878 --> 0.170209).  Saving model ...\n",
      "epoch 25\n",
      "train loss : 0.1704064693517983, validation loss : 0.16953249362468722\n",
      "Validation loss decreased (0.170209 --> 0.169532).  Saving model ...\n",
      "epoch 26\n",
      "train loss : 0.16973523305163074, validation loss : 0.16884755361925527\n",
      "Validation loss decreased (0.169532 --> 0.168848).  Saving model ...\n",
      "epoch 27\n",
      "train loss : 0.16905604173863928, validation loss : 0.16815605742422246\n",
      "Validation loss decreased (0.168848 --> 0.168156).  Saving model ...\n",
      "epoch 28\n",
      "train loss : 0.16836973525883095, validation loss : 0.16745836236048492\n",
      "Validation loss decreased (0.168156 --> 0.167458).  Saving model ...\n",
      "epoch 29\n",
      "train loss : 0.16767678165840433, validation loss : 0.1667548289864228\n",
      "Validation loss decreased (0.167458 --> 0.166755).  Saving model ...\n",
      "epoch 30\n",
      "train loss : 0.1669776688335774, validation loss : 0.16604611856819446\n",
      "Validation loss decreased (0.166755 --> 0.166046).  Saving model ...\n",
      "epoch 31\n",
      "train loss : 0.16627311997554237, validation loss : 0.1653333021039444\n",
      "Validation loss decreased (0.166046 --> 0.165333).  Saving model ...\n",
      "epoch 32\n",
      "train loss : 0.16556388621075893, validation loss : 0.1646167303310067\n",
      "Validation loss decreased (0.165333 --> 0.164617).  Saving model ...\n",
      "epoch 33\n",
      "train loss : 0.16485065620827177, validation loss : 0.16389737572117402\n",
      "Validation loss decreased (0.164617 --> 0.163897).  Saving model ...\n",
      "epoch 34\n",
      "train loss : 0.1641340683008106, validation loss : 0.16317575484084187\n",
      "Validation loss decreased (0.163897 --> 0.163176).  Saving model ...\n",
      "epoch 35\n",
      "train loss : 0.16341497538052502, validation loss : 0.1624529303892782\n",
      "Validation loss decreased (0.163176 --> 0.162453).  Saving model ...\n",
      "epoch 36\n",
      "train loss : 0.16269401415612084, validation loss : 0.161729444064588\n",
      "Validation loss decreased (0.162453 --> 0.161729).  Saving model ...\n",
      "epoch 37\n",
      "train loss : 0.16197218353216966, validation loss : 0.1610063169626016\n",
      "Validation loss decreased (0.161729 --> 0.161006).  Saving model ...\n",
      "epoch 38\n",
      "train loss : 0.16125015032092896, validation loss : 0.1602841712859778\n",
      "Validation loss decreased (0.161006 --> 0.160284).  Saving model ...\n",
      "epoch 39\n",
      "train loss : 0.160528558224768, validation loss : 0.15956382237233893\n",
      "Validation loss decreased (0.160284 --> 0.159564).  Saving model ...\n",
      "epoch 40\n",
      "train loss : 0.15980809701512916, validation loss : 0.15884588988895992\n",
      "Validation loss decreased (0.159564 --> 0.158846).  Saving model ...\n",
      "epoch 41\n",
      "train loss : 0.1590894394467085, validation loss : 0.1581309884877523\n",
      "Validation loss decreased (0.158846 --> 0.158131).  Saving model ...\n",
      "epoch 42\n",
      "train loss : 0.15837339524026695, validation loss : 0.15742000808935458\n",
      "Validation loss decreased (0.158131 --> 0.157420).  Saving model ...\n",
      "epoch 43\n",
      "train loss : 0.15766031432965077, validation loss : 0.15671297323869454\n",
      "Validation loss decreased (0.157420 --> 0.156713).  Saving model ...\n",
      "epoch 44\n",
      "train loss : 0.15695097271302208, validation loss : 0.15601111372272664\n",
      "Validation loss decreased (0.156713 --> 0.156011).  Saving model ...\n",
      "epoch 45\n",
      "train loss : 0.15624605939345074, validation loss : 0.15531490446786758\n",
      "Validation loss decreased (0.156011 --> 0.155315).  Saving model ...\n",
      "epoch 46\n",
      "train loss : 0.15554607418885977, validation loss : 0.15462471976775027\n",
      "Validation loss decreased (0.155315 --> 0.154625).  Saving model ...\n",
      "epoch 47\n",
      "train loss : 0.15485155296383768, validation loss : 0.15394105701374156\n",
      "Validation loss decreased (0.154625 --> 0.153941).  Saving model ...\n",
      "epoch 48\n",
      "train loss : 0.15416320928252025, validation loss : 0.153264680281521\n",
      "Validation loss decreased (0.153941 --> 0.153265).  Saving model ...\n",
      "epoch 49\n",
      "train loss : 0.15348118899939034, validation loss : 0.15259569350782037\n",
      "Validation loss decreased (0.153265 --> 0.152596).  Saving model ...\n",
      "epoch 50\n",
      "train loss : 0.15280617779080405, validation loss : 0.1519347021498758\n",
      "Validation loss decreased (0.152596 --> 0.151935).  Saving model ...\n",
      "epoch 51\n",
      "train loss : 0.15213850154933706, validation loss : 0.15128200078084061\n",
      "Validation loss decreased (0.151935 --> 0.151282).  Saving model ...\n",
      "epoch 52\n",
      "train loss : 0.15147844954650364, validation loss : 0.15063777103185652\n",
      "Validation loss decreased (0.151282 --> 0.150638).  Saving model ...\n",
      "epoch 53\n",
      "train loss : 0.15082637003142707, validation loss : 0.15000238061003662\n",
      "Validation loss decreased (0.150638 --> 0.150002).  Saving model ...\n",
      "epoch 54\n",
      "train loss : 0.15018258351025185, validation loss : 0.14937606742880505\n",
      "Validation loss decreased (0.150002 --> 0.149376).  Saving model ...\n",
      "epoch 55\n",
      "train loss : 0.14954725542982686, validation loss : 0.14875895854882282\n",
      "Validation loss decreased (0.149376 --> 0.148759).  Saving model ...\n",
      "epoch 56\n",
      "train loss : 0.1489206730323957, validation loss : 0.1481512883520308\n",
      "Validation loss decreased (0.148759 --> 0.148151).  Saving model ...\n",
      "epoch 57\n",
      "train loss : 0.1483029033587662, validation loss : 0.1475529404761229\n",
      "Validation loss decreased (0.148151 --> 0.147553).  Saving model ...\n",
      "epoch 58\n",
      "train loss : 0.14769419755502028, validation loss : 0.14696445882732315\n",
      "Validation loss decreased (0.147553 --> 0.146964).  Saving model ...\n",
      "epoch 59\n",
      "train loss : 0.14709471072859734, validation loss : 0.1463857375802949\n",
      "Validation loss decreased (0.146964 --> 0.146386).  Saving model ...\n",
      "epoch 60\n",
      "train loss : 0.14650452410401635, validation loss : 0.14581679294467056\n",
      "Validation loss decreased (0.146386 --> 0.145817).  Saving model ...\n",
      "epoch 61\n",
      "train loss : 0.1459236617760177, validation loss : 0.14525767176079787\n",
      "Validation loss decreased (0.145817 --> 0.145258).  Saving model ...\n",
      "epoch 62\n",
      "train loss : 0.1453522447324158, validation loss : 0.14470846243887828\n",
      "Validation loss decreased (0.145258 --> 0.144708).  Saving model ...\n",
      "epoch 63\n",
      "train loss : 0.14479025226007783, validation loss : 0.14416896779618776\n",
      "Validation loss decreased (0.144708 --> 0.144169).  Saving model ...\n",
      "epoch 64\n",
      "train loss : 0.14423766269034766, validation loss : 0.1436392871922274\n",
      "Validation loss decreased (0.144169 --> 0.143639).  Saving model ...\n",
      "epoch 65\n",
      "train loss : 0.1436945029709485, validation loss : 0.1431192932442382\n",
      "Validation loss decreased (0.143639 --> 0.143119).  Saving model ...\n",
      "epoch 66\n",
      "train loss : 0.14316077175430184, validation loss : 0.14260906443936106\n",
      "Validation loss decreased (0.143119 --> 0.142609).  Saving model ...\n",
      "epoch 67\n",
      "train loss : 0.14263642237512575, validation loss : 0.14210843086059008\n",
      "Validation loss decreased (0.142609 --> 0.142108).  Saving model ...\n",
      "epoch 68\n",
      "train loss : 0.1421213876844959, validation loss : 0.14161735546171011\n",
      "Validation loss decreased (0.142108 --> 0.141617).  Saving model ...\n",
      "epoch 69\n",
      "train loss : 0.14161552810376055, validation loss : 0.14113560004414064\n",
      "Validation loss decreased (0.141617 --> 0.141136).  Saving model ...\n",
      "epoch 70\n",
      "train loss : 0.1411187790633811, validation loss : 0.14066314196898774\n",
      "Validation loss decreased (0.141136 --> 0.140663).  Saving model ...\n",
      "epoch 71\n",
      "train loss : 0.1406311059898421, validation loss : 0.14019987978140158\n",
      "Validation loss decreased (0.140663 --> 0.140200).  Saving model ...\n",
      "epoch 72\n",
      "train loss : 0.1401523997314812, validation loss : 0.13974570187499857\n",
      "Validation loss decreased (0.140200 --> 0.139746).  Saving model ...\n",
      "epoch 73\n",
      "train loss : 0.13968251943060236, validation loss : 0.139300420120243\n",
      "Validation loss decreased (0.139746 --> 0.139300).  Saving model ...\n",
      "epoch 74\n",
      "train loss : 0.1392213588268985, validation loss : 0.13886389993432277\n",
      "Validation loss decreased (0.139300 --> 0.138864).  Saving model ...\n",
      "epoch 75\n",
      "train loss : 0.13876876462946106, validation loss : 0.1384359713165203\n",
      "Validation loss decreased (0.138864 --> 0.138436).  Saving model ...\n",
      "epoch 76\n",
      "train loss : 0.13832458768582537, validation loss : 0.13801648892922389\n",
      "Validation loss decreased (0.138436 --> 0.138016).  Saving model ...\n",
      "epoch 77\n",
      "train loss : 0.13788869020525413, validation loss : 0.13760528737432087\n",
      "Validation loss decreased (0.138016 --> 0.137605).  Saving model ...\n",
      "epoch 78\n",
      "train loss : 0.13746095915640374, validation loss : 0.1372022618181391\n",
      "Validation loss decreased (0.137605 --> 0.137202).  Saving model ...\n",
      "epoch 79\n",
      "train loss : 0.13704125843772158, validation loss : 0.13680719346715564\n",
      "Validation loss decreased (0.137202 --> 0.136807).  Saving model ...\n",
      "epoch 80\n",
      "train loss : 0.13662942603944375, validation loss : 0.1364199706195472\n",
      "Validation loss decreased (0.136807 --> 0.136420).  Saving model ...\n",
      "epoch 81\n",
      "train loss : 0.13622530048339182, validation loss : 0.13604038001980473\n",
      "Validation loss decreased (0.136420 --> 0.136040).  Saving model ...\n",
      "epoch 82\n",
      "train loss : 0.13582872632040058, validation loss : 0.13566825392613854\n",
      "Validation loss decreased (0.136040 --> 0.135668).  Saving model ...\n",
      "epoch 83\n",
      "train loss : 0.13543954575549738, validation loss : 0.1353034612826895\n",
      "Validation loss decreased (0.135668 --> 0.135303).  Saving model ...\n",
      "epoch 84\n",
      "train loss : 0.13505764166563425, validation loss : 0.13494587002582367\n",
      "Validation loss decreased (0.135303 --> 0.134946).  Saving model ...\n",
      "epoch 85\n",
      "train loss : 0.13468286334262866, validation loss : 0.134595275326975\n",
      "Validation loss decreased (0.134946 --> 0.134595).  Saving model ...\n",
      "epoch 86\n",
      "train loss : 0.13431504940209396, validation loss : 0.13425155007556225\n",
      "Validation loss decreased (0.134595 --> 0.134252).  Saving model ...\n",
      "epoch 87\n",
      "train loss : 0.13395405639951258, validation loss : 0.133914521879281\n",
      "Validation loss decreased (0.134252 --> 0.133915).  Saving model ...\n",
      "epoch 88\n",
      "train loss : 0.13359973877604203, validation loss : 0.1335840283249755\n",
      "Validation loss decreased (0.133915 --> 0.133584).  Saving model ...\n",
      "epoch 89\n",
      "train loss : 0.13325194862189105, validation loss : 0.13325994141881786\n",
      "Validation loss decreased (0.133584 --> 0.133260).  Saving model ...\n",
      "epoch 90\n",
      "train loss : 0.13291055037688856, validation loss : 0.13294211165055395\n",
      "Validation loss decreased (0.133260 --> 0.132942).  Saving model ...\n",
      "epoch 91\n",
      "train loss : 0.13257539656886264, validation loss : 0.13263039707045124\n",
      "Validation loss decreased (0.132942 --> 0.132630).  Saving model ...\n",
      "epoch 92\n",
      "train loss : 0.13224635183668135, validation loss : 0.13232464135963565\n",
      "Validation loss decreased (0.132630 --> 0.132325).  Saving model ...\n",
      "epoch 93\n",
      "train loss : 0.13192327524362985, validation loss : 0.13202469703148867\n",
      "Validation loss decreased (0.132325 --> 0.132025).  Saving model ...\n",
      "epoch 94\n",
      "train loss : 0.13160603487716652, validation loss : 0.13173043319074507\n",
      "Validation loss decreased (0.132025 --> 0.131730).  Saving model ...\n",
      "epoch 95\n",
      "train loss : 0.13129448446089037, validation loss : 0.13144169842781941\n",
      "Validation loss decreased (0.131730 --> 0.131442).  Saving model ...\n",
      "epoch 96\n",
      "train loss : 0.130988501630611, validation loss : 0.13115838485919604\n",
      "Validation loss decreased (0.131442 --> 0.131158).  Saving model ...\n",
      "epoch 97\n",
      "train loss : 0.13068795622969687, validation loss : 0.13088034242097046\n",
      "Validation loss decreased (0.131158 --> 0.130880).  Saving model ...\n",
      "epoch 98\n",
      "train loss : 0.13039272837265847, validation loss : 0.13060745060849155\n",
      "Validation loss decreased (0.130880 --> 0.130607).  Saving model ...\n",
      "epoch 99\n",
      "train loss : 0.13010268972117447, validation loss : 0.13033958606523133\n",
      "Validation loss decreased (0.130607 --> 0.130340).  Saving model ...\n",
      "epoch 100\n",
      "train loss : 0.12981771691977845, validation loss : 0.13007662213869353\n",
      "Validation loss decreased (0.130340 --> 0.130077).  Saving model ...\n",
      "epoch 101\n",
      "train loss : 0.1295376935091819, validation loss : 0.12981843284721042\n",
      "Validation loss decreased (0.130077 --> 0.129818).  Saving model ...\n",
      "epoch 102\n",
      "train loss : 0.12926250545400236, validation loss : 0.12956492727550786\n",
      "Validation loss decreased (0.129818 --> 0.129565).  Saving model ...\n",
      "epoch 103\n",
      "train loss : 0.12899203907154705, validation loss : 0.12931596624372474\n",
      "Validation loss decreased (0.129565 --> 0.129316).  Saving model ...\n",
      "epoch 104\n",
      "train loss : 0.12872617984399956, validation loss : 0.1290714592670258\n",
      "Validation loss decreased (0.129316 --> 0.129071).  Saving model ...\n",
      "epoch 105\n",
      "train loss : 0.1284648319228414, validation loss : 0.1288312929772482\n",
      "Validation loss decreased (0.129071 --> 0.128831).  Saving model ...\n",
      "epoch 106\n",
      "train loss : 0.12820788113038586, validation loss : 0.1285953624717448\n",
      "Validation loss decreased (0.128831 --> 0.128595).  Saving model ...\n",
      "epoch 107\n",
      "train loss : 0.1279552271500648, validation loss : 0.1283635649211727\n",
      "Validation loss decreased (0.128595 --> 0.128364).  Saving model ...\n",
      "epoch 108\n",
      "train loss : 0.12770677496043403, validation loss : 0.12813579299889907\n",
      "Validation loss decreased (0.128364 --> 0.128136).  Saving model ...\n",
      "epoch 109\n",
      "train loss : 0.1274624159334345, validation loss : 0.12791196739321928\n",
      "Validation loss decreased (0.128136 --> 0.127912).  Saving model ...\n",
      "epoch 110\n",
      "train loss : 0.12722206735037855, validation loss : 0.12769199247886887\n",
      "Validation loss decreased (0.127912 --> 0.127692).  Saving model ...\n",
      "epoch 111\n",
      "train loss : 0.12698563409873875, validation loss : 0.127475764782827\n",
      "Validation loss decreased (0.127692 --> 0.127476).  Saving model ...\n",
      "epoch 112\n",
      "train loss : 0.12675302012418518, validation loss : 0.127263198218702\n",
      "Validation loss decreased (0.127476 --> 0.127263).  Saving model ...\n",
      "epoch 113\n",
      "train loss : 0.12652413821825864, validation loss : 0.1270542084156644\n",
      "Validation loss decreased (0.127263 --> 0.127054).  Saving model ...\n",
      "epoch 114\n",
      "train loss : 0.12629890623172282, validation loss : 0.12684871270486467\n",
      "Validation loss decreased (0.127054 --> 0.126849).  Saving model ...\n",
      "epoch 115\n",
      "train loss : 0.126077242138225, validation loss : 0.12664663251325411\n",
      "Validation loss decreased (0.126849 --> 0.126647).  Saving model ...\n",
      "epoch 116\n",
      "train loss : 0.1258590639692169, validation loss : 0.12644788169392726\n",
      "Validation loss decreased (0.126647 --> 0.126448).  Saving model ...\n",
      "epoch 117\n",
      "train loss : 0.12564429244562694, validation loss : 0.1262523810483199\n",
      "Validation loss decreased (0.126448 --> 0.126252).  Saving model ...\n",
      "epoch 118\n",
      "train loss : 0.1254328491648752, validation loss : 0.12606006119024632\n",
      "Validation loss decreased (0.126252 --> 0.126060).  Saving model ...\n",
      "epoch 119\n",
      "train loss : 0.12522466138779895, validation loss : 0.125870848440273\n",
      "Validation loss decreased (0.126060 --> 0.125871).  Saving model ...\n",
      "epoch 120\n",
      "train loss : 0.1250196530639746, validation loss : 0.12568466776672943\n",
      "Validation loss decreased (0.125871 --> 0.125685).  Saving model ...\n",
      "epoch 121\n",
      "train loss : 0.1248177552019887, validation loss : 0.12550145279743652\n",
      "Validation loss decreased (0.125685 --> 0.125501).  Saving model ...\n",
      "epoch 122\n",
      "train loss : 0.12461889956520153, validation loss : 0.1253211322618492\n",
      "Validation loss decreased (0.125501 --> 0.125321).  Saving model ...\n",
      "epoch 123\n",
      "train loss : 0.12442301723612244, validation loss : 0.12514364265586791\n",
      "Validation loss decreased (0.125321 --> 0.125144).  Saving model ...\n",
      "epoch 124\n",
      "train loss : 0.12423004269055825, validation loss : 0.12496891912397032\n",
      "Validation loss decreased (0.125144 --> 0.124969).  Saving model ...\n",
      "epoch 125\n",
      "train loss : 0.12403991437395277, validation loss : 0.12479690211521013\n",
      "Validation loss decreased (0.124969 --> 0.124797).  Saving model ...\n",
      "epoch 126\n",
      "train loss : 0.12385257123879348, validation loss : 0.12462752944780857\n",
      "Validation loss decreased (0.124797 --> 0.124628).  Saving model ...\n",
      "epoch 127\n",
      "train loss : 0.12366794889314546, validation loss : 0.12446074061474426\n",
      "Validation loss decreased (0.124628 --> 0.124461).  Saving model ...\n",
      "epoch 128\n",
      "train loss : 0.12348598683476472, validation loss : 0.12429648357553784\n",
      "Validation loss decreased (0.124461 --> 0.124296).  Saving model ...\n",
      "epoch 129\n",
      "train loss : 0.12330663236207552, validation loss : 0.12413470164624325\n",
      "Validation loss decreased (0.124296 --> 0.124135).  Saving model ...\n",
      "epoch 130\n",
      "train loss : 0.12312982811098491, validation loss : 0.12397533920270831\n",
      "Validation loss decreased (0.124135 --> 0.123975).  Saving model ...\n",
      "epoch 131\n",
      "train loss : 0.12295551837728207, validation loss : 0.12381834281400358\n",
      "Validation loss decreased (0.123975 --> 0.123818).  Saving model ...\n",
      "epoch 132\n",
      "train loss : 0.12278365016564413, validation loss : 0.12366366455728162\n",
      "Validation loss decreased (0.123818 --> 0.123664).  Saving model ...\n",
      "epoch 133\n",
      "train loss : 0.12261417269551692, validation loss : 0.12351125476685793\n",
      "Validation loss decreased (0.123664 --> 0.123511).  Saving model ...\n",
      "epoch 134\n",
      "train loss : 0.12244703828549147, validation loss : 0.12336106352588638\n",
      "Validation loss decreased (0.123511 --> 0.123361).  Saving model ...\n",
      "epoch 135\n",
      "train loss : 0.12228219538942119, validation loss : 0.12321304436291047\n",
      "Validation loss decreased (0.123361 --> 0.123213).  Saving model ...\n",
      "epoch 136\n",
      "train loss : 0.1221195960186936, validation loss : 0.1230671514306963\n",
      "Validation loss decreased (0.123213 --> 0.123067).  Saving model ...\n",
      "epoch 137\n",
      "train loss : 0.12195919493225246, validation loss : 0.12292333759185078\n",
      "Validation loss decreased (0.123067 --> 0.122923).  Saving model ...\n",
      "epoch 138\n",
      "train loss : 0.12180094764598966, validation loss : 0.12278156021121674\n",
      "Validation loss decreased (0.122923 --> 0.122782).  Saving model ...\n",
      "epoch 139\n",
      "train loss : 0.12164481038289364, validation loss : 0.12264177979954204\n",
      "Validation loss decreased (0.122782 --> 0.122642).  Saving model ...\n",
      "epoch 140\n",
      "train loss : 0.12149073781376205, validation loss : 0.12250394949798249\n",
      "Validation loss decreased (0.122642 --> 0.122504).  Saving model ...\n",
      "epoch 141\n",
      "train loss : 0.12133869164182115, validation loss : 0.12236803447592845\n",
      "Validation loss decreased (0.122504 --> 0.122368).  Saving model ...\n",
      "epoch 142\n",
      "train loss : 0.12118863090935571, validation loss : 0.12223399288758059\n",
      "Validation loss decreased (0.122368 --> 0.122234).  Saving model ...\n",
      "epoch 143\n",
      "train loss : 0.12104051457721528, validation loss : 0.12210178699078653\n",
      "Validation loss decreased (0.122234 --> 0.122102).  Saving model ...\n",
      "epoch 144\n",
      "train loss : 0.12089430305896347, validation loss : 0.1219713780545364\n",
      "Validation loss decreased (0.122102 --> 0.121971).  Saving model ...\n",
      "epoch 145\n",
      "train loss : 0.12074995747245926, validation loss : 0.12184273161795907\n",
      "Validation loss decreased (0.121971 --> 0.121843).  Saving model ...\n",
      "epoch 146\n",
      "train loss : 0.12060744543031938, validation loss : 0.12171581076657215\n",
      "Validation loss decreased (0.121843 --> 0.121716).  Saving model ...\n",
      "epoch 147\n",
      "train loss : 0.12046672759100487, validation loss : 0.1215905834352144\n",
      "Validation loss decreased (0.121716 --> 0.121591).  Saving model ...\n",
      "epoch 148\n",
      "train loss : 0.12032777169169362, validation loss : 0.1214670153443684\n",
      "Validation loss decreased (0.121591 --> 0.121467).  Saving model ...\n",
      "epoch 149\n",
      "train loss : 0.12019054302959296, validation loss : 0.1213450738482051\n",
      "Validation loss decreased (0.121467 --> 0.121345).  Saving model ...\n",
      "epoch 150\n",
      "train loss : 0.12005500915543221, validation loss : 0.12122472676281129\n",
      "Validation loss decreased (0.121345 --> 0.121225).  Saving model ...\n",
      "epoch 151\n",
      "train loss : 0.11992113541313998, validation loss : 0.12110594342377427\n",
      "Validation loss decreased (0.121225 --> 0.121106).  Saving model ...\n",
      "epoch 152\n",
      "train loss : 0.1197888927380597, validation loss : 0.12098869455226947\n",
      "Validation loss decreased (0.121106 --> 0.120989).  Saving model ...\n",
      "epoch 153\n",
      "train loss : 0.11965825115796282, validation loss : 0.12087294943832874\n",
      "Validation loss decreased (0.120989 --> 0.120873).  Saving model ...\n",
      "epoch 154\n",
      "train loss : 0.11952917827629653, validation loss : 0.12075867865693672\n",
      "Validation loss decreased (0.120873 --> 0.120759).  Saving model ...\n",
      "epoch 155\n",
      "train loss : 0.11940164503389111, validation loss : 0.12064585681701971\n",
      "Validation loss decreased (0.120759 --> 0.120646).  Saving model ...\n",
      "epoch 156\n",
      "train loss : 0.1192756223412864, validation loss : 0.12053445458350347\n",
      "Validation loss decreased (0.120646 --> 0.120534).  Saving model ...\n",
      "epoch 157\n",
      "train loss : 0.11915108695216915, validation loss : 0.12042444611984011\n",
      "Validation loss decreased (0.120534 --> 0.120424).  Saving model ...\n",
      "epoch 158\n",
      "train loss : 0.11902800652830822, validation loss : 0.12031580482620968\n",
      "Validation loss decreased (0.120424 --> 0.120316).  Saving model ...\n",
      "epoch 159\n",
      "train loss : 0.11890635566969182, validation loss : 0.12020850634075857\n",
      "Validation loss decreased (0.120316 --> 0.120209).  Saving model ...\n",
      "epoch 160\n",
      "train loss : 0.1187861071725417, validation loss : 0.12010252482619661\n",
      "Validation loss decreased (0.120209 --> 0.120103).  Saving model ...\n",
      "epoch 161\n",
      "train loss : 0.11866723837165506, validation loss : 0.1199978374389896\n",
      "Validation loss decreased (0.120103 --> 0.119998).  Saving model ...\n",
      "epoch 162\n",
      "train loss : 0.11854972312126501, validation loss : 0.11989441920834464\n",
      "Validation loss decreased (0.119998 --> 0.119894).  Saving model ...\n",
      "epoch 163\n",
      "train loss : 0.11843353549315538, validation loss : 0.1197922475320322\n",
      "Validation loss decreased (0.119894 --> 0.119792).  Saving model ...\n",
      "epoch 164\n",
      "train loss : 0.11831865326823134, validation loss : 0.1196912992648343\n",
      "Validation loss decreased (0.119792 --> 0.119691).  Saving model ...\n",
      "epoch 165\n",
      "train loss : 0.11820505569280249, validation loss : 0.11959155261970189\n",
      "Validation loss decreased (0.119691 --> 0.119592).  Saving model ...\n",
      "epoch 166\n",
      "train loss : 0.11809271711967335, validation loss : 0.1194929866788159\n",
      "Validation loss decreased (0.119592 --> 0.119493).  Saving model ...\n",
      "epoch 167\n",
      "train loss : 0.11798161714829783, validation loss : 0.11939558078081117\n",
      "Validation loss decreased (0.119493 --> 0.119396).  Saving model ...\n",
      "epoch 168\n",
      "train loss : 0.11787173334257278, validation loss : 0.11929931372650684\n",
      "Validation loss decreased (0.119396 --> 0.119299).  Saving model ...\n",
      "epoch 169\n",
      "train loss : 0.11776304503395947, validation loss : 0.11920416506632092\n",
      "Validation loss decreased (0.119299 --> 0.119204).  Saving model ...\n",
      "epoch 170\n",
      "train loss : 0.11765553161786495, validation loss : 0.11911011613506632\n",
      "Validation loss decreased (0.119204 --> 0.119110).  Saving model ...\n",
      "epoch 171\n",
      "train loss : 0.1175491729285759, validation loss : 0.11901714691234454\n",
      "Validation loss decreased (0.119110 --> 0.119017).  Saving model ...\n",
      "epoch 172\n",
      "train loss : 0.1174439496875719, validation loss : 0.11892523927593394\n",
      "Validation loss decreased (0.119017 --> 0.118925).  Saving model ...\n",
      "epoch 173\n",
      "train loss : 0.11733984025978712, validation loss : 0.11883437460120344\n",
      "Validation loss decreased (0.118925 --> 0.118834).  Saving model ...\n",
      "epoch 174\n",
      "train loss : 0.11723682446251725, validation loss : 0.11874453593852528\n",
      "Validation loss decreased (0.118834 --> 0.118745).  Saving model ...\n",
      "epoch 175\n",
      "train loss : 0.11713488658072922, validation loss : 0.11865570558961318\n",
      "Validation loss decreased (0.118745 --> 0.118656).  Saving model ...\n",
      "epoch 176\n",
      "train loss : 0.11703401013653571, validation loss : 0.11856786512133445\n",
      "Validation loss decreased (0.118656 --> 0.118568).  Saving model ...\n",
      "epoch 177\n",
      "train loss : 0.11693417477109723, validation loss : 0.11848099999953544\n",
      "Validation loss decreased (0.118568 --> 0.118481).  Saving model ...\n",
      "epoch 178\n",
      "train loss : 0.11683536628054456, validation loss : 0.1183950937817387\n",
      "Validation loss decreased (0.118481 --> 0.118395).  Saving model ...\n",
      "epoch 179\n",
      "train loss : 0.11673756618468074, validation loss : 0.1183101295493864\n",
      "Validation loss decreased (0.118395 --> 0.118310).  Saving model ...\n",
      "epoch 180\n",
      "train loss : 0.11664075822635774, validation loss : 0.11822609250868076\n",
      "Validation loss decreased (0.118310 --> 0.118226).  Saving model ...\n",
      "epoch 181\n",
      "train loss : 0.11654492630845724, validation loss : 0.11814296792540996\n",
      "Validation loss decreased (0.118226 --> 0.118143).  Saving model ...\n",
      "epoch 182\n",
      "train loss : 0.11645005393871492, validation loss : 0.11806074004761193\n",
      "Validation loss decreased (0.118143 --> 0.118061).  Saving model ...\n",
      "epoch 183\n",
      "train loss : 0.1163561262621595, validation loss : 0.11797939354876352\n",
      "Validation loss decreased (0.118061 --> 0.117979).  Saving model ...\n",
      "epoch 184\n",
      "train loss : 0.11626312859327687, validation loss : 0.11789891695433616\n",
      "Validation loss decreased (0.117979 --> 0.117899).  Saving model ...\n",
      "epoch 185\n",
      "train loss : 0.11617104594951841, validation loss : 0.11781929378218117\n",
      "Validation loss decreased (0.117899 --> 0.117819).  Saving model ...\n",
      "epoch 186\n",
      "train loss : 0.11607986351942097, validation loss : 0.1177405111942914\n",
      "Validation loss decreased (0.117819 --> 0.117741).  Saving model ...\n",
      "epoch 187\n",
      "train loss : 0.11598956823553062, validation loss : 0.1176625555706836\n",
      "Validation loss decreased (0.117741 --> 0.117663).  Saving model ...\n",
      "epoch 188\n",
      "train loss : 0.1159001457658595, validation loss : 0.11758541397925056\n",
      "Validation loss decreased (0.117663 --> 0.117585).  Saving model ...\n",
      "epoch 189\n",
      "train loss : 0.11581158189938098, validation loss : 0.11750907384555806\n",
      "Validation loss decreased (0.117585 --> 0.117509).  Saving model ...\n",
      "epoch 190\n",
      "train loss : 0.11572386368522014, validation loss : 0.11743352166427326\n",
      "Validation loss decreased (0.117509 --> 0.117434).  Saving model ...\n",
      "epoch 191\n",
      "train loss : 0.11563697791029164, validation loss : 0.11735874629724137\n",
      "Validation loss decreased (0.117434 --> 0.117359).  Saving model ...\n",
      "epoch 192\n",
      "train loss : 0.11555091228281665, validation loss : 0.1172847333339877\n",
      "Validation loss decreased (0.117359 --> 0.117285).  Saving model ...\n",
      "epoch 193\n",
      "train loss : 0.11546565491255294, validation loss : 0.11721147285053667\n",
      "Validation loss decreased (0.117285 --> 0.117211).  Saving model ...\n",
      "epoch 194\n",
      "train loss : 0.11538119355046961, validation loss : 0.11713895421068175\n",
      "Validation loss decreased (0.117211 --> 0.117139).  Saving model ...\n",
      "epoch 195\n",
      "train loss : 0.11529751519840542, validation loss : 0.11706716443878255\n",
      "Validation loss decreased (0.117139 --> 0.117067).  Saving model ...\n",
      "epoch 196\n",
      "train loss : 0.11521460828189625, validation loss : 0.11699609229804447\n",
      "Validation loss decreased (0.117067 --> 0.116996).  Saving model ...\n",
      "epoch 197\n",
      "train loss : 0.11513246152914215, validation loss : 0.11692572723379237\n",
      "Validation loss decreased (0.116996 --> 0.116926).  Saving model ...\n",
      "epoch 198\n",
      "train loss : 0.11505106498333773, validation loss : 0.11685605947654464\n",
      "Validation loss decreased (0.116926 --> 0.116856).  Saving model ...\n",
      "epoch 199\n",
      "train loss : 0.11497040720333662, validation loss : 0.11678707887222707\n",
      "Validation loss decreased (0.116856 --> 0.116787).  Saving model ...\n",
      "epoch 200\n",
      "train loss : 0.11489047730139428, validation loss : 0.11671877377380532\n",
      "Validation loss decreased (0.116787 --> 0.116719).  Saving model ...\n",
      "epoch 201\n",
      "train loss : 0.11481126213987085, validation loss : 0.11665113514220107\n",
      "Validation loss decreased (0.116719 --> 0.116651).  Saving model ...\n",
      "epoch 202\n",
      "train loss : 0.11473275194996797, validation loss : 0.11658415328991856\n",
      "Validation loss decreased (0.116651 --> 0.116584).  Saving model ...\n",
      "epoch 203\n",
      "train loss : 0.11465493833233308, validation loss : 0.11651781871747864\n",
      "Validation loss decreased (0.116584 --> 0.116518).  Saving model ...\n",
      "epoch 204\n",
      "train loss : 0.11457781185231096, validation loss : 0.1164521219694587\n",
      "Validation loss decreased (0.116518 --> 0.116452).  Saving model ...\n",
      "epoch 205\n",
      "train loss : 0.11450136005810399, validation loss : 0.1163870532089088\n",
      "Validation loss decreased (0.116452 --> 0.116387).  Saving model ...\n",
      "epoch 206\n",
      "train loss : 0.11442557211318061, validation loss : 0.11632260201183386\n",
      "Validation loss decreased (0.116387 --> 0.116323).  Saving model ...\n",
      "epoch 207\n",
      "train loss : 0.11435044335758979, validation loss : 0.11625876157777469\n",
      "Validation loss decreased (0.116323 --> 0.116259).  Saving model ...\n",
      "epoch 208\n",
      "train loss : 0.1142759622901255, validation loss : 0.11619552218183596\n",
      "Validation loss decreased (0.116259 --> 0.116196).  Saving model ...\n",
      "epoch 209\n",
      "train loss : 0.11420211852154595, validation loss : 0.11613287530502338\n",
      "Validation loss decreased (0.116196 --> 0.116133).  Saving model ...\n",
      "epoch 210\n",
      "train loss : 0.11412890313071947, validation loss : 0.11607081226458667\n",
      "Validation loss decreased (0.116133 --> 0.116071).  Saving model ...\n",
      "epoch 211\n",
      "train loss : 0.1140563079234749, validation loss : 0.11600932562499026\n",
      "Validation loss decreased (0.116071 --> 0.116009).  Saving model ...\n",
      "epoch 212\n",
      "train loss : 0.11398432372668028, validation loss : 0.11594840691664421\n",
      "Validation loss decreased (0.116009 --> 0.115948).  Saving model ...\n",
      "epoch 213\n",
      "train loss : 0.11391294191603298, validation loss : 0.1158880472452108\n",
      "Validation loss decreased (0.115948 --> 0.115888).  Saving model ...\n",
      "epoch 214\n",
      "train loss : 0.11384215534120647, validation loss : 0.11582823996631801\n",
      "Validation loss decreased (0.115888 --> 0.115828).  Saving model ...\n",
      "epoch 215\n",
      "train loss : 0.11377195327841952, validation loss : 0.11576897683686563\n",
      "Validation loss decreased (0.115828 --> 0.115769).  Saving model ...\n",
      "epoch 216\n",
      "train loss : 0.1137023301507149, validation loss : 0.11571025025204928\n",
      "Validation loss decreased (0.115769 --> 0.115710).  Saving model ...\n",
      "epoch 217\n",
      "train loss : 0.11363327601767334, validation loss : 0.11565205283035285\n",
      "Validation loss decreased (0.115710 --> 0.115652).  Saving model ...\n",
      "epoch 218\n",
      "train loss : 0.11356478640957025, validation loss : 0.11559437866784614\n",
      "Validation loss decreased (0.115652 --> 0.115594).  Saving model ...\n",
      "epoch 219\n",
      "train loss : 0.11349685388737134, validation loss : 0.11553721832558216\n",
      "Validation loss decreased (0.115594 --> 0.115537).  Saving model ...\n",
      "epoch 220\n",
      "train loss : 0.11342946745090099, validation loss : 0.11548056657214344\n",
      "Validation loss decreased (0.115537 --> 0.115481).  Saving model ...\n",
      "epoch 221\n",
      "train loss : 0.11336262148061164, validation loss : 0.11542441575464017\n",
      "Validation loss decreased (0.115481 --> 0.115424).  Saving model ...\n",
      "epoch 222\n",
      "train loss : 0.11329630807798709, validation loss : 0.11536875893706823\n",
      "Validation loss decreased (0.115424 --> 0.115369).  Saving model ...\n",
      "epoch 223\n",
      "train loss : 0.11323052059670204, validation loss : 0.11531358908772252\n",
      "Validation loss decreased (0.115369 --> 0.115314).  Saving model ...\n",
      "epoch 224\n",
      "train loss : 0.11316525367570114, validation loss : 0.11525890030043918\n",
      "Validation loss decreased (0.115314 --> 0.115259).  Saving model ...\n",
      "epoch 225\n",
      "train loss : 0.11310049691183828, validation loss : 0.11520468620091286\n",
      "Validation loss decreased (0.115259 --> 0.115205).  Saving model ...\n",
      "epoch 226\n",
      "train loss : 0.11303624678623954, validation loss : 0.11515094109163576\n",
      "Validation loss decreased (0.115205 --> 0.115151).  Saving model ...\n",
      "epoch 227\n",
      "train loss : 0.1129724960003903, validation loss : 0.11509765848274604\n",
      "Validation loss decreased (0.115151 --> 0.115098).  Saving model ...\n",
      "epoch 228\n",
      "train loss : 0.11290923597036742, validation loss : 0.11504483159556571\n",
      "Validation loss decreased (0.115098 --> 0.115045).  Saving model ...\n",
      "epoch 229\n",
      "train loss : 0.11284646373918662, validation loss : 0.11499245513504042\n",
      "Validation loss decreased (0.115045 --> 0.114992).  Saving model ...\n",
      "epoch 230\n",
      "train loss : 0.11278416929874485, validation loss : 0.11494052247940599\n",
      "Validation loss decreased (0.114992 --> 0.114941).  Saving model ...\n",
      "epoch 231\n",
      "train loss : 0.11272234967560618, validation loss : 0.11488902830613872\n",
      "Validation loss decreased (0.114941 --> 0.114889).  Saving model ...\n",
      "epoch 232\n",
      "train loss : 0.11266099762847571, validation loss : 0.11483796728476818\n",
      "Validation loss decreased (0.114889 --> 0.114838).  Saving model ...\n",
      "epoch 233\n",
      "train loss : 0.11260010559049631, validation loss : 0.11478733403390401\n",
      "Validation loss decreased (0.114838 --> 0.114787).  Saving model ...\n",
      "epoch 234\n",
      "train loss : 0.11253966955167398, validation loss : 0.11473712229041563\n",
      "Validation loss decreased (0.114787 --> 0.114737).  Saving model ...\n",
      "epoch 235\n",
      "train loss : 0.11247968320465547, validation loss : 0.11468732776916146\n",
      "Validation loss decreased (0.114737 --> 0.114687).  Saving model ...\n",
      "epoch 236\n",
      "train loss : 0.11242014249693048, validation loss : 0.1146379444138706\n",
      "Validation loss decreased (0.114687 --> 0.114638).  Saving model ...\n",
      "epoch 237\n",
      "train loss : 0.11236104018773956, validation loss : 0.11458896708516131\n",
      "Validation loss decreased (0.114638 --> 0.114589).  Saving model ...\n",
      "epoch 238\n",
      "train loss : 0.11230237179424898, validation loss : 0.11454039108815375\n",
      "Validation loss decreased (0.114589 --> 0.114540).  Saving model ...\n",
      "epoch 239\n",
      "train loss : 0.11224413089756091, validation loss : 0.11449220990620282\n",
      "Validation loss decreased (0.114540 --> 0.114492).  Saving model ...\n",
      "epoch 240\n",
      "train loss : 0.1121863117995634, validation loss : 0.11444441976260301\n",
      "Validation loss decreased (0.114492 --> 0.114444).  Saving model ...\n",
      "epoch 241\n",
      "train loss : 0.1121289102266961, validation loss : 0.11439701553620761\n",
      "Validation loss decreased (0.114444 --> 0.114397).  Saving model ...\n",
      "epoch 242\n",
      "train loss : 0.11207192276234879, validation loss : 0.11434999195420763\n",
      "Validation loss decreased (0.114397 --> 0.114350).  Saving model ...\n",
      "epoch 243\n",
      "train loss : 0.11201534077852428, validation loss : 0.11430334513179723\n",
      "Validation loss decreased (0.114350 --> 0.114303).  Saving model ...\n",
      "epoch 244\n",
      "train loss : 0.1119591625024099, validation loss : 0.114257070179014\n",
      "Validation loss decreased (0.114303 --> 0.114257).  Saving model ...\n",
      "epoch 245\n",
      "train loss : 0.11190338108606236, validation loss : 0.11421116250451824\n",
      "Validation loss decreased (0.114257 --> 0.114211).  Saving model ...\n",
      "epoch 246\n",
      "train loss : 0.11184799203140779, validation loss : 0.11416561656974944\n",
      "Validation loss decreased (0.114211 --> 0.114166).  Saving model ...\n",
      "epoch 247\n",
      "train loss : 0.11179299190081078, validation loss : 0.11412043012505682\n",
      "Validation loss decreased (0.114166 --> 0.114120).  Saving model ...\n",
      "epoch 248\n",
      "train loss : 0.11173837439582007, validation loss : 0.11407559740216607\n",
      "Validation loss decreased (0.114120 --> 0.114076).  Saving model ...\n",
      "epoch 249\n",
      "train loss : 0.11168413551011792, validation loss : 0.11403111497428739\n",
      "Validation loss decreased (0.114076 --> 0.114031).  Saving model ...\n",
      "epoch 250\n",
      "train loss : 0.11163027095722394, validation loss : 0.11398697870778202\n",
      "Validation loss decreased (0.114031 --> 0.113987).  Saving model ...\n",
      "epoch 251\n",
      "train loss : 0.11157677591068228, validation loss : 0.11394318337412239\n",
      "Validation loss decreased (0.113987 --> 0.113943).  Saving model ...\n",
      "epoch 252\n",
      "train loss : 0.11152364747850992, validation loss : 0.11389972622592891\n",
      "Validation loss decreased (0.113943 --> 0.113900).  Saving model ...\n",
      "epoch 253\n",
      "train loss : 0.11147087982218883, validation loss : 0.11385660158060222\n",
      "Validation loss decreased (0.113900 --> 0.113857).  Saving model ...\n",
      "epoch 254\n",
      "train loss : 0.111418471167283, validation loss : 0.11381380650730862\n",
      "Validation loss decreased (0.113857 --> 0.113814).  Saving model ...\n",
      "epoch 255\n",
      "train loss : 0.11136641587953194, validation loss : 0.11377133764787604\n",
      "Validation loss decreased (0.113814 --> 0.113771).  Saving model ...\n",
      "epoch 256\n",
      "train loss : 0.11131470778629765, validation loss : 0.11372919073756418\n",
      "Validation loss decreased (0.113771 --> 0.113729).  Saving model ...\n",
      "epoch 257\n",
      "train loss : 0.11126334578451376, validation loss : 0.11368736259902586\n",
      "Validation loss decreased (0.113729 --> 0.113687).  Saving model ...\n",
      "epoch 258\n",
      "train loss : 0.11121232497600615, validation loss : 0.11364584841528855\n",
      "Validation loss decreased (0.113687 --> 0.113646).  Saving model ...\n",
      "epoch 259\n",
      "train loss : 0.11116163960686364, validation loss : 0.11360464561849716\n",
      "Validation loss decreased (0.113646 --> 0.113605).  Saving model ...\n",
      "epoch 260\n",
      "train loss : 0.11111128979803382, validation loss : 0.11356375052054533\n",
      "Validation loss decreased (0.113605 --> 0.113564).  Saving model ...\n",
      "epoch 261\n",
      "train loss : 0.11106126706259613, validation loss : 0.11352315946480171\n",
      "Validation loss decreased (0.113564 --> 0.113523).  Saving model ...\n",
      "epoch 262\n",
      "train loss : 0.1110115722566821, validation loss : 0.1134828687009917\n",
      "Validation loss decreased (0.113523 --> 0.113483).  Saving model ...\n",
      "epoch 263\n",
      "train loss : 0.11096219705007156, validation loss : 0.11344287490782391\n",
      "Validation loss decreased (0.113483 --> 0.113443).  Saving model ...\n",
      "epoch 264\n",
      "train loss : 0.1109131396002849, validation loss : 0.11340317584376468\n",
      "Validation loss decreased (0.113443 --> 0.113403).  Saving model ...\n",
      "epoch 265\n",
      "train loss : 0.11086439879096002, validation loss : 0.11336376729990799\n",
      "Validation loss decreased (0.113403 --> 0.113364).  Saving model ...\n",
      "epoch 266\n",
      "train loss : 0.11081596799146352, validation loss : 0.11332464558658971\n",
      "Validation loss decreased (0.113364 --> 0.113325).  Saving model ...\n",
      "epoch 267\n",
      "train loss : 0.11076784272142491, validation loss : 0.11328580843356749\n",
      "Validation loss decreased (0.113325 --> 0.113286).  Saving model ...\n",
      "epoch 268\n",
      "train loss : 0.11072002349426358, validation loss : 0.11324725204783949\n",
      "Validation loss decreased (0.113286 --> 0.113247).  Saving model ...\n",
      "epoch 269\n",
      "train loss : 0.11067250447824377, validation loss : 0.11320897344601277\n",
      "Validation loss decreased (0.113247 --> 0.113209).  Saving model ...\n",
      "epoch 270\n",
      "train loss : 0.11062528444403816, validation loss : 0.1131709692329019\n",
      "Validation loss decreased (0.113209 --> 0.113171).  Saving model ...\n",
      "epoch 271\n",
      "train loss : 0.1105783586967345, validation loss : 0.11313323683445345\n",
      "Validation loss decreased (0.113171 --> 0.113133).  Saving model ...\n",
      "epoch 272\n",
      "train loss : 0.11053172400788229, validation loss : 0.11309577245697344\n",
      "Validation loss decreased (0.113133 --> 0.113096).  Saving model ...\n",
      "epoch 273\n",
      "train loss : 0.11048537673015979, validation loss : 0.11305857330919303\n",
      "Validation loss decreased (0.113096 --> 0.113059).  Saving model ...\n",
      "epoch 274\n",
      "train loss : 0.1104393160144811, validation loss : 0.1130216362773332\n",
      "Validation loss decreased (0.113059 --> 0.113022).  Saving model ...\n",
      "epoch 275\n",
      "train loss : 0.11039353573812463, validation loss : 0.11298495911931414\n",
      "Validation loss decreased (0.113022 --> 0.112985).  Saving model ...\n",
      "epoch 276\n",
      "train loss : 0.11034803431318443, validation loss : 0.11294853875628232\n",
      "Validation loss decreased (0.112985 --> 0.112949).  Saving model ...\n",
      "epoch 277\n",
      "train loss : 0.11030281075734086, validation loss : 0.11291237193669393\n",
      "Validation loss decreased (0.112949 --> 0.112912).  Saving model ...\n",
      "epoch 278\n",
      "train loss : 0.11025785904532294, validation loss : 0.11287645589917077\n",
      "Validation loss decreased (0.112912 --> 0.112876).  Saving model ...\n",
      "epoch 279\n",
      "train loss : 0.11021317834640683, validation loss : 0.11284078902889923\n",
      "Validation loss decreased (0.112876 --> 0.112841).  Saving model ...\n",
      "epoch 280\n",
      "train loss : 0.11016876596612021, validation loss : 0.11280536777398557\n",
      "Validation loss decreased (0.112841 --> 0.112805).  Saving model ...\n",
      "epoch 281\n",
      "train loss : 0.11012461787061598, validation loss : 0.1127701896260161\n",
      "Validation loss decreased (0.112805 --> 0.112770).  Saving model ...\n",
      "epoch 282\n",
      "train loss : 0.11008073229261994, validation loss : 0.11273525277320004\n",
      "Validation loss decreased (0.112770 --> 0.112735).  Saving model ...\n",
      "epoch 283\n",
      "train loss : 0.11003710554901812, validation loss : 0.11270055312854851\n",
      "Validation loss decreased (0.112735 --> 0.112701).  Saving model ...\n",
      "epoch 284\n",
      "train loss : 0.10999373531096256, validation loss : 0.11266608888521525\n",
      "Validation loss decreased (0.112701 --> 0.112666).  Saving model ...\n",
      "epoch 285\n",
      "train loss : 0.10995061924569775, validation loss : 0.11263185742284033\n",
      "Validation loss decreased (0.112666 --> 0.112632).  Saving model ...\n",
      "epoch 286\n",
      "train loss : 0.1099077536985961, validation loss : 0.11259785694574552\n",
      "Validation loss decreased (0.112632 --> 0.112598).  Saving model ...\n",
      "epoch 287\n",
      "train loss : 0.10986513841576516, validation loss : 0.11256408471898861\n",
      "Validation loss decreased (0.112598 --> 0.112564).  Saving model ...\n",
      "epoch 288\n",
      "train loss : 0.10982276732992893, validation loss : 0.11253053782890914\n",
      "Validation loss decreased (0.112564 --> 0.112531).  Saving model ...\n",
      "epoch 289\n",
      "train loss : 0.10978064287848938, validation loss : 0.11249721460700224\n",
      "Validation loss decreased (0.112531 --> 0.112497).  Saving model ...\n",
      "epoch 290\n",
      "train loss : 0.10973875914972168, validation loss : 0.11246411236600139\n",
      "Validation loss decreased (0.112497 --> 0.112464).  Saving model ...\n",
      "epoch 291\n",
      "train loss : 0.10969711208447636, validation loss : 0.11243122851240754\n",
      "Validation loss decreased (0.112464 --> 0.112431).  Saving model ...\n",
      "epoch 292\n",
      "train loss : 0.10965570269789209, validation loss : 0.11239856097922032\n",
      "Validation loss decreased (0.112431 --> 0.112399).  Saving model ...\n",
      "epoch 293\n",
      "train loss : 0.1096145262999942, validation loss : 0.11236610713813984\n",
      "Validation loss decreased (0.112399 --> 0.112366).  Saving model ...\n",
      "epoch 294\n",
      "train loss : 0.1095735830741102, validation loss : 0.11233386532115223\n",
      "Validation loss decreased (0.112366 --> 0.112334).  Saving model ...\n",
      "epoch 295\n",
      "train loss : 0.10953286693070825, validation loss : 0.11230183394922841\n",
      "Validation loss decreased (0.112334 --> 0.112302).  Saving model ...\n",
      "epoch 296\n",
      "train loss : 0.10949237799275724, validation loss : 0.11227000952941323\n",
      "Validation loss decreased (0.112302 --> 0.112270).  Saving model ...\n",
      "epoch 297\n",
      "train loss : 0.10945211402484316, validation loss : 0.11223839084373163\n",
      "Validation loss decreased (0.112270 --> 0.112238).  Saving model ...\n",
      "epoch 298\n",
      "train loss : 0.10941207176571878, validation loss : 0.11220697591189766\n",
      "Validation loss decreased (0.112238 --> 0.112207).  Saving model ...\n",
      "epoch 299\n",
      "train loss : 0.1093722511980165, validation loss : 0.11217576232455077\n",
      "Validation loss decreased (0.112207 --> 0.112176).  Saving model ...\n",
      "epoch 300\n",
      "train loss : 0.10933264971423604, validation loss : 0.1121447480584702\n",
      "Validation loss decreased (0.112176 --> 0.112145).  Saving model ...\n",
      "epoch 301\n",
      "train loss : 0.10929326311428236, validation loss : 0.11211393097024502\n",
      "Validation loss decreased (0.112145 --> 0.112114).  Saving model ...\n",
      "epoch 302\n",
      "train loss : 0.10925409013085653, validation loss : 0.11208330970653811\n",
      "Validation loss decreased (0.112114 --> 0.112083).  Saving model ...\n",
      "epoch 303\n",
      "train loss : 0.10921512853252001, validation loss : 0.11205288183982831\n",
      "Validation loss decreased (0.112083 --> 0.112053).  Saving model ...\n",
      "epoch 304\n",
      "train loss : 0.10917637561697287, validation loss : 0.11202264532411021\n",
      "Validation loss decreased (0.112053 --> 0.112023).  Saving model ...\n",
      "epoch 305\n",
      "train loss : 0.1091378298213168, validation loss : 0.11199259907546086\n",
      "Validation loss decreased (0.112023 --> 0.111993).  Saving model ...\n",
      "epoch 306\n",
      "train loss : 0.10909948909320574, validation loss : 0.11196274040274701\n",
      "Validation loss decreased (0.111993 --> 0.111963).  Saving model ...\n",
      "epoch 307\n",
      "train loss : 0.1090613520848913, validation loss : 0.11193306727488053\n",
      "Validation loss decreased (0.111963 --> 0.111933).  Saving model ...\n",
      "epoch 308\n",
      "train loss : 0.10902341676517514, validation loss : 0.1119035782760275\n",
      "Validation loss decreased (0.111933 --> 0.111904).  Saving model ...\n",
      "epoch 309\n",
      "train loss : 0.10898568133139487, validation loss : 0.1118742709558241\n",
      "Validation loss decreased (0.111904 --> 0.111874).  Saving model ...\n",
      "epoch 310\n",
      "train loss : 0.10894814330834078, validation loss : 0.11184514480079345\n",
      "Validation loss decreased (0.111874 --> 0.111845).  Saving model ...\n",
      "epoch 311\n",
      "train loss : 0.10891080257454086, validation loss : 0.11181619751716079\n",
      "Validation loss decreased (0.111845 --> 0.111816).  Saving model ...\n",
      "epoch 312\n",
      "train loss : 0.10887365355383735, validation loss : 0.11178742727174089\n",
      "Validation loss decreased (0.111816 --> 0.111787).  Saving model ...\n",
      "epoch 313\n",
      "train loss : 0.10883669584544019, validation loss : 0.11175883235420922\n",
      "Validation loss decreased (0.111787 --> 0.111759).  Saving model ...\n",
      "epoch 314\n",
      "train loss : 0.1087999285657202, validation loss : 0.11173041050215023\n",
      "Validation loss decreased (0.111759 --> 0.111730).  Saving model ...\n",
      "epoch 315\n",
      "train loss : 0.10876334983468267, validation loss : 0.11170216046920373\n",
      "Validation loss decreased (0.111730 --> 0.111702).  Saving model ...\n",
      "epoch 316\n",
      "train loss : 0.10872695681333021, validation loss : 0.1116740805551381\n",
      "Validation loss decreased (0.111702 --> 0.111674).  Saving model ...\n",
      "epoch 317\n",
      "train loss : 0.10869074820338591, validation loss : 0.11164616954421988\n",
      "Validation loss decreased (0.111674 --> 0.111646).  Saving model ...\n",
      "epoch 318\n",
      "train loss : 0.10865472315807541, validation loss : 0.11161842571952521\n",
      "Validation loss decreased (0.111646 --> 0.111618).  Saving model ...\n",
      "epoch 319\n",
      "train loss : 0.10861887887549906, validation loss : 0.11159084772608935\n",
      "Validation loss decreased (0.111618 --> 0.111591).  Saving model ...\n",
      "epoch 320\n",
      "train loss : 0.10858321292860298, validation loss : 0.11156343311563677\n",
      "Validation loss decreased (0.111591 --> 0.111563).  Saving model ...\n",
      "epoch 321\n",
      "train loss : 0.10854772684071542, validation loss : 0.11153618061116818\n",
      "Validation loss decreased (0.111563 --> 0.111536).  Saving model ...\n",
      "epoch 322\n",
      "train loss : 0.10851241842470671, validation loss : 0.11150908917047699\n",
      "Validation loss decreased (0.111536 --> 0.111509).  Saving model ...\n",
      "epoch 323\n",
      "train loss : 0.10847728339643149, validation loss : 0.1114821569673162\n",
      "Validation loss decreased (0.111509 --> 0.111482).  Saving model ...\n",
      "epoch 324\n",
      "train loss : 0.10844232083936699, validation loss : 0.1114553824485677\n",
      "Validation loss decreased (0.111482 --> 0.111455).  Saving model ...\n",
      "epoch 325\n",
      "train loss : 0.10840753083338406, validation loss : 0.11142876476478211\n",
      "Validation loss decreased (0.111455 --> 0.111429).  Saving model ...\n",
      "epoch 326\n",
      "train loss : 0.10837291042453148, validation loss : 0.11140230212143905\n",
      "Validation loss decreased (0.111429 --> 0.111402).  Saving model ...\n",
      "epoch 327\n",
      "train loss : 0.10833845962110543, validation loss : 0.11137599282006586\n",
      "Validation loss decreased (0.111402 --> 0.111376).  Saving model ...\n",
      "epoch 328\n",
      "train loss : 0.10830417410668598, validation loss : 0.1113498361691378\n",
      "Validation loss decreased (0.111376 --> 0.111350).  Saving model ...\n",
      "epoch 329\n",
      "train loss : 0.10827005425380477, validation loss : 0.1113238299346199\n",
      "Validation loss decreased (0.111350 --> 0.111324).  Saving model ...\n",
      "epoch 330\n",
      "train loss : 0.10823609851294415, validation loss : 0.11129797376769761\n",
      "Validation loss decreased (0.111324 --> 0.111298).  Saving model ...\n",
      "epoch 331\n",
      "train loss : 0.10820230644302875, validation loss : 0.11127226511871134\n",
      "Validation loss decreased (0.111298 --> 0.111272).  Saving model ...\n",
      "epoch 332\n",
      "train loss : 0.10816867499075017, validation loss : 0.11124670383843684\n",
      "Validation loss decreased (0.111272 --> 0.111247).  Saving model ...\n",
      "epoch 333\n",
      "train loss : 0.1081352008075868, validation loss : 0.11122128736146299\n",
      "Validation loss decreased (0.111247 --> 0.111221).  Saving model ...\n",
      "epoch 334\n",
      "train loss : 0.10810188612535994, validation loss : 0.11119601489376942\n",
      "Validation loss decreased (0.111221 --> 0.111196).  Saving model ...\n",
      "epoch 335\n",
      "train loss : 0.10806872863613746, validation loss : 0.11117088433895418\n",
      "Validation loss decreased (0.111196 --> 0.111171).  Saving model ...\n",
      "epoch 336\n",
      "train loss : 0.10803572568956221, validation loss : 0.11114589507414736\n",
      "Validation loss decreased (0.111171 --> 0.111146).  Saving model ...\n",
      "epoch 337\n",
      "train loss : 0.10800287839903096, validation loss : 0.11112104455425018\n",
      "Validation loss decreased (0.111146 --> 0.111121).  Saving model ...\n",
      "epoch 338\n",
      "train loss : 0.10797018365305144, validation loss : 0.11109633319560792\n",
      "Validation loss decreased (0.111121 --> 0.111096).  Saving model ...\n",
      "epoch 339\n",
      "train loss : 0.10793763858056425, validation loss : 0.11107175885387466\n",
      "Validation loss decreased (0.111096 --> 0.111072).  Saving model ...\n",
      "epoch 340\n",
      "train loss : 0.10790524439691128, validation loss : 0.11104732012366354\n",
      "Validation loss decreased (0.111072 --> 0.111047).  Saving model ...\n",
      "epoch 341\n",
      "train loss : 0.10787299865383806, validation loss : 0.11102301638601599\n",
      "Validation loss decreased (0.111047 --> 0.111023).  Saving model ...\n",
      "epoch 342\n",
      "train loss : 0.10784090185622684, validation loss : 0.1109988459205807\n",
      "Validation loss decreased (0.111023 --> 0.110999).  Saving model ...\n",
      "epoch 343\n",
      "train loss : 0.10780895090412605, validation loss : 0.1109748081991458\n",
      "Validation loss decreased (0.110999 --> 0.110975).  Saving model ...\n",
      "epoch 344\n",
      "train loss : 0.10777714388468286, validation loss : 0.11095090107741433\n",
      "Validation loss decreased (0.110975 --> 0.110951).  Saving model ...\n",
      "epoch 345\n",
      "train loss : 0.10774548101321207, validation loss : 0.11092712425006997\n",
      "Validation loss decreased (0.110951 --> 0.110927).  Saving model ...\n",
      "epoch 346\n",
      "train loss : 0.10771396113376466, validation loss : 0.11090347561455702\n",
      "Validation loss decreased (0.110927 --> 0.110903).  Saving model ...\n",
      "epoch 347\n",
      "train loss : 0.10768258139851974, validation loss : 0.11087995466005035\n",
      "Validation loss decreased (0.110903 --> 0.110880).  Saving model ...\n",
      "epoch 348\n",
      "train loss : 0.10765134141751125, validation loss : 0.11085656072350467\n",
      "Validation loss decreased (0.110880 --> 0.110857).  Saving model ...\n",
      "epoch 349\n",
      "train loss : 0.10762024053240521, validation loss : 0.11083329207585023\n",
      "Validation loss decreased (0.110857 --> 0.110833).  Saving model ...\n",
      "epoch 350\n",
      "train loss : 0.10758927815446112, validation loss : 0.1108101480282807\n",
      "Validation loss decreased (0.110833 --> 0.110810).  Saving model ...\n",
      "epoch 351\n",
      "train loss : 0.1075584509202123, validation loss : 0.11078712784901701\n",
      "Validation loss decreased (0.110810 --> 0.110787).  Saving model ...\n",
      "epoch 352\n",
      "train loss : 0.10752775878260147, validation loss : 0.1107642288713697\n",
      "Validation loss decreased (0.110787 --> 0.110764).  Saving model ...\n",
      "epoch 353\n",
      "train loss : 0.1074972009758766, validation loss : 0.1107414512412286\n",
      "Validation loss decreased (0.110764 --> 0.110741).  Saving model ...\n",
      "epoch 354\n",
      "train loss : 0.10746677634446944, validation loss : 0.11071879370512558\n",
      "Validation loss decreased (0.110741 --> 0.110719).  Saving model ...\n",
      "epoch 355\n",
      "train loss : 0.10743648316425625, validation loss : 0.11069625525205969\n",
      "Validation loss decreased (0.110719 --> 0.110696).  Saving model ...\n",
      "epoch 356\n",
      "train loss : 0.10740632026868656, validation loss : 0.11067383504744599\n",
      "Validation loss decreased (0.110696 --> 0.110674).  Saving model ...\n",
      "epoch 357\n",
      "train loss : 0.10737628739568007, validation loss : 0.11065153198304331\n",
      "Validation loss decreased (0.110674 --> 0.110652).  Saving model ...\n",
      "epoch 358\n",
      "train loss : 0.10734638486016852, validation loss : 0.11062934480385722\n",
      "Validation loss decreased (0.110652 --> 0.110629).  Saving model ...\n",
      "epoch 359\n",
      "train loss : 0.10731660788741056, validation loss : 0.11060727298652538\n",
      "Validation loss decreased (0.110629 --> 0.110607).  Saving model ...\n",
      "epoch 360\n",
      "train loss : 0.10728695801590704, validation loss : 0.1105853154645919\n",
      "Validation loss decreased (0.110607 --> 0.110585).  Saving model ...\n",
      "epoch 361\n",
      "train loss : 0.10725743203286273, validation loss : 0.11056347048836325\n",
      "Validation loss decreased (0.110585 --> 0.110563).  Saving model ...\n",
      "epoch 362\n",
      "train loss : 0.10722803088225628, validation loss : 0.1105417383318631\n",
      "Validation loss decreased (0.110563 --> 0.110542).  Saving model ...\n",
      "epoch 363\n",
      "train loss : 0.10719875514747622, validation loss : 0.11052011772049904\n",
      "Validation loss decreased (0.110542 --> 0.110520).  Saving model ...\n",
      "epoch 364\n",
      "train loss : 0.10716960105587305, validation loss : 0.11049860727032595\n",
      "Validation loss decreased (0.110520 --> 0.110499).  Saving model ...\n",
      "epoch 365\n",
      "train loss : 0.10714056837060804, validation loss : 0.11047720647828946\n",
      "Validation loss decreased (0.110499 --> 0.110477).  Saving model ...\n",
      "epoch 366\n",
      "train loss : 0.10711165536573007, validation loss : 0.11045591358093582\n",
      "Validation loss decreased (0.110477 --> 0.110456).  Saving model ...\n",
      "epoch 367\n",
      "train loss : 0.1070828627851304, validation loss : 0.11043472870126113\n",
      "Validation loss decreased (0.110456 --> 0.110435).  Saving model ...\n",
      "epoch 368\n",
      "train loss : 0.10705418838560553, validation loss : 0.11041365047126947\n",
      "Validation loss decreased (0.110435 --> 0.110414).  Saving model ...\n",
      "epoch 369\n",
      "train loss : 0.10702563183835859, validation loss : 0.11039267808308269\n",
      "Validation loss decreased (0.110414 --> 0.110393).  Saving model ...\n",
      "epoch 370\n",
      "train loss : 0.10699719281699788, validation loss : 0.1103718108096719\n",
      "Validation loss decreased (0.110393 --> 0.110372).  Saving model ...\n",
      "epoch 371\n",
      "train loss : 0.10696886939591402, validation loss : 0.11035104821472672\n",
      "Validation loss decreased (0.110372 --> 0.110351).  Saving model ...\n",
      "epoch 372\n",
      "train loss : 0.10694066248946268, validation loss : 0.1103303889997554\n",
      "Validation loss decreased (0.110351 --> 0.110330).  Saving model ...\n",
      "epoch 373\n",
      "train loss : 0.10691256894663632, validation loss : 0.1103098330934054\n",
      "Validation loss decreased (0.110330 --> 0.110310).  Saving model ...\n",
      "epoch 374\n",
      "train loss : 0.10688458801007873, validation loss : 0.11028937874835988\n",
      "Validation loss decreased (0.110310 --> 0.110289).  Saving model ...\n",
      "epoch 375\n",
      "train loss : 0.10685672012507644, validation loss : 0.11026902598095971\n",
      "Validation loss decreased (0.110289 --> 0.110269).  Saving model ...\n",
      "epoch 376\n",
      "train loss : 0.10682896156025842, validation loss : 0.11024877398144922\n",
      "Validation loss decreased (0.110269 --> 0.110249).  Saving model ...\n",
      "epoch 377\n",
      "train loss : 0.10680131502120574, validation loss : 0.11022862141418127\n",
      "Validation loss decreased (0.110249 --> 0.110229).  Saving model ...\n",
      "epoch 378\n",
      "train loss : 0.10677377796785728, validation loss : 0.11020856738379645\n",
      "Validation loss decreased (0.110229 --> 0.110209).  Saving model ...\n",
      "epoch 379\n",
      "train loss : 0.10674634932846334, validation loss : 0.11018861187761389\n",
      "Validation loss decreased (0.110209 --> 0.110189).  Saving model ...\n",
      "epoch 380\n",
      "train loss : 0.1067190293173619, validation loss : 0.110168754897379\n",
      "Validation loss decreased (0.110189 --> 0.110169).  Saving model ...\n",
      "epoch 381\n",
      "train loss : 0.10669181572614098, validation loss : 0.11014899434640209\n",
      "Validation loss decreased (0.110169 --> 0.110149).  Saving model ...\n",
      "epoch 382\n",
      "train loss : 0.10666470847172872, validation loss : 0.11012932935702067\n",
      "Validation loss decreased (0.110149 --> 0.110129).  Saving model ...\n",
      "epoch 383\n",
      "train loss : 0.10663770601320752, validation loss : 0.11010975942817877\n",
      "Validation loss decreased (0.110129 --> 0.110110).  Saving model ...\n",
      "epoch 384\n",
      "train loss : 0.10661080887009466, validation loss : 0.11009028337618787\n",
      "Validation loss decreased (0.110110 --> 0.110090).  Saving model ...\n",
      "epoch 385\n",
      "train loss : 0.10658401579784432, validation loss : 0.11007090047240428\n",
      "Validation loss decreased (0.110090 --> 0.110071).  Saving model ...\n",
      "epoch 386\n",
      "train loss : 0.10655732461893083, validation loss : 0.11005160994894392\n",
      "Validation loss decreased (0.110071 --> 0.110052).  Saving model ...\n",
      "epoch 387\n",
      "train loss : 0.10653073581631652, validation loss : 0.1100324118850467\n",
      "Validation loss decreased (0.110052 --> 0.110032).  Saving model ...\n",
      "epoch 388\n",
      "train loss : 0.10650424854629326, validation loss : 0.11001330513914037\n",
      "Validation loss decreased (0.110032 --> 0.110013).  Saving model ...\n",
      "epoch 389\n",
      "train loss : 0.10647786235463474, validation loss : 0.10999428863915914\n",
      "Validation loss decreased (0.110013 --> 0.109994).  Saving model ...\n",
      "epoch 390\n",
      "train loss : 0.10645157538902868, validation loss : 0.1099753625848423\n",
      "Validation loss decreased (0.109994 --> 0.109975).  Saving model ...\n",
      "epoch 391\n",
      "train loss : 0.10642538810935706, validation loss : 0.10995652545551082\n",
      "Validation loss decreased (0.109975 --> 0.109957).  Saving model ...\n",
      "epoch 392\n",
      "train loss : 0.10639929841863505, validation loss : 0.10993777730631968\n",
      "Validation loss decreased (0.109957 --> 0.109938).  Saving model ...\n",
      "epoch 393\n",
      "train loss : 0.10637330660655563, validation loss : 0.10991911742606507\n",
      "Validation loss decreased (0.109938 --> 0.109919).  Saving model ...\n",
      "epoch 394\n",
      "train loss : 0.10634741128095467, validation loss : 0.10990054469287998\n",
      "Validation loss decreased (0.109919 --> 0.109901).  Saving model ...\n",
      "epoch 395\n",
      "train loss : 0.10632161157724358, validation loss : 0.10988205843165758\n",
      "Validation loss decreased (0.109901 --> 0.109882).  Saving model ...\n",
      "epoch 396\n",
      "train loss : 0.10629590815884649, validation loss : 0.10986365822546232\n",
      "Validation loss decreased (0.109882 --> 0.109864).  Saving model ...\n",
      "epoch 397\n",
      "train loss : 0.10627029965532618, validation loss : 0.10984534277320944\n",
      "Validation loss decreased (0.109864 --> 0.109845).  Saving model ...\n",
      "epoch 398\n",
      "train loss : 0.106244784462527, validation loss : 0.10982711183502679\n",
      "Validation loss decreased (0.109845 --> 0.109827).  Saving model ...\n",
      "epoch 399\n",
      "train loss : 0.10621936217492486, validation loss : 0.10980896495594777\n",
      "Validation loss decreased (0.109827 --> 0.109809).  Saving model ...\n",
      "epoch 400\n",
      "train loss : 0.10619403410667133, validation loss : 0.1097909010777806\n",
      "Validation loss decreased (0.109809 --> 0.109791).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def training(epochs) :\n",
    "    # epoch 별 훈련 및 검증 루프 (trainloader를 반복하여 모델 훈련시키고, validloader를 사용해 모델의 성능 검증)\n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0.0\n",
    "        running_vall_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        # 훈련 과정\n",
    "        for data in trainloader:\n",
    "            model.train()\n",
    "            inputs, outputs = data\n",
    "            optimizer.zero_grad()                                                     # Optimizer Gradient를 0으로 초기화. 이는 각 미니배치마다 Gradient가 누적되는 것을 방지\n",
    "            predicted_outputs = model(inputs)                                         # 모델을 사용하여 입력 데이터에 대한 예측값을 계산\n",
    "            train_loss = criterion(predicted_outputs, outputs)                        # 계산된 예측값과 실제 레이블 간의 손실을 계산\n",
    "            train_loss.backward()                                                     # 손실에 대한 역전파를 수행하여 Gradient를 계산\n",
    "            optimizer.step()                                                          # 계산된 Gradient를 사용하여 모델 매개변수를 업데이트\n",
    "            running_train_loss += train_loss.item()                                   # track the loss value\n",
    "        loss_.append(running_train_loss / n)\n",
    "        \n",
    "        # 검증 과정\n",
    "        with torch.no_grad():                                                         # Gradient 계산을 비활성화하여 메모리 사용량을 줄이고 계산 속도를 향상 \n",
    "            model.eval()\n",
    "            for data in validloader:\n",
    "                inputs, outputs = data\n",
    "                predicted_outputs = model(inputs)\n",
    "                val_loss = criterion(predicted_outputs, outputs)\n",
    "\n",
    "                # The label with the highest value will be our prediction\n",
    "                _, predicted = torch.max(predicted_outputs,1)\n",
    "                running_vall_loss += val_loss.item()\n",
    "                total += outputs.size(0)\n",
    "                val_loss_value = running_vall_loss/len(validloader)\n",
    "        valoss_.append(val_loss_value)\n",
    "\n",
    "        avgtrainloss = np.mean(loss_)\n",
    "        avgvalidloss = np.mean(valoss_)\n",
    "        print('epoch', epoch + 1)\n",
    "        print(f'train loss : {avgtrainloss}, validation loss : {avgvalidloss}')\n",
    "        \n",
    "        # EarlyStopping\n",
    "        early_stopping(avgvalidloss, model)                                           # 검증 손실을 기준으로 조기 종료 조건 확인\n",
    "        if early_stopping.early_stop:                                                 # 조건 만족 시 조기 종료\n",
    "            break\n",
    "\n",
    "    # 모델 저장    \n",
    "    saveModel()\n",
    "training(epochs = epoch)\n",
    "\n",
    "\n",
    "# 손실 시각화\n",
    "plt.plot(loss_)\n",
    "plt.plot(valoss_)\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 (Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)):\n\u001b[0;32m---> 32\u001b[0m   test_rmse, actual, pred \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m   preds\u001b[38;5;241m.\u001b[39mappend(pred)\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 평가 모드 설정\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m   \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()                                                       \u001b[38;5;66;03m# 평가를 할 땐 반드시 eval()을 사용해야 한다.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# 데이터 로더를 통한 반복\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "def evaluation(dataloader):\n",
    "  # 초기화\n",
    "  predictions = torch.tensor([], dtype=torch.float64,device = device)  # 예측값을 저장하는 텐서.\n",
    "  actual = torch.tensor([], dtype=torch.float64, device = device)      # 실제값을 저장하는 텐서.\n",
    "\n",
    "  # 평가 모드 설정\n",
    "  with torch.no_grad():\n",
    "    model.eval()                                                       # 평가를 할 땐 반드시 eval()을 사용해야 한다.\n",
    "\n",
    "    # 데이터 로더를 통한 반복\n",
    "    for data in dataloader:\n",
    "        inputs, values = data\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 예측값 및 실제값 저장\n",
    "        predictions = torch.cat((predictions, outputs), 0)             # cat함수를 통해 예측값을 누적.\n",
    "        actual = torch.cat((actual, values), 0)                        # cat함수를 통해 실제값을 누적.\n",
    "  \n",
    "  # CPU로 이동 및 NumPy 배열 변환\n",
    "  predictions =predictions.to(device= \"cpu\")\n",
    "  predictions = predictions.numpy()                                    # 넘파이 배열로 변경.\n",
    "  actual = actual.to(device= \"cpu\")\n",
    "  actual = actual.numpy()                                              # 넘파이 배열로 변경.\n",
    "  \n",
    "  # RMSE 계산\n",
    "  rmse = np.sqrt(mean_squared_error(predictions, actual))              # sklearn을 이용해 RMSE를 계산.\n",
    "\n",
    "  return rmse,actual,predictions\n",
    "\n",
    "preds = []\n",
    "for i in tqdm(range(1000)):\n",
    "  test_rmse, actual, pred = evaluation(testloader)\n",
    "  preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32045842141585223"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79414427],\n",
       "       [0.79857379],\n",
       "       [0.75345188],\n",
       "       [0.69617122],\n",
       "       [0.74821973],\n",
       "       [0.69831884],\n",
       "       [0.81406605],\n",
       "       [0.71896827],\n",
       "       [0.81185931],\n",
       "       [0.81028706],\n",
       "       [0.76456857],\n",
       "       [0.72057891],\n",
       "       [0.67779344],\n",
       "       [0.6953336 ],\n",
       "       [0.71612912],\n",
       "       [0.8021335 ],\n",
       "       [0.71617234],\n",
       "       [0.91390908],\n",
       "       [0.81437641],\n",
       "       [0.72651291],\n",
       "       [0.81546825],\n",
       "       [0.73143101],\n",
       "       [0.81274134],\n",
       "       [0.79042393],\n",
       "       [0.70673347],\n",
       "       [0.89656335],\n",
       "       [0.8965947 ],\n",
       "       [0.71331888],\n",
       "       [0.87819546],\n",
       "       [0.85288715],\n",
       "       [0.82518595],\n",
       "       [0.89592975],\n",
       "       [0.85329705],\n",
       "       [0.9303776 ],\n",
       "       [0.81652856],\n",
       "       [0.72765625],\n",
       "       [0.94350713],\n",
       "       [0.81115413],\n",
       "       [0.82701367],\n",
       "       [0.90527886],\n",
       "       [0.90483165],\n",
       "       [0.8478567 ],\n",
       "       [0.85520297],\n",
       "       [0.8760823 ],\n",
       "       [0.86505413],\n",
       "       [0.95100522],\n",
       "       [0.73236126],\n",
       "       [0.93464929],\n",
       "       [0.8926782 ],\n",
       "       [0.90937603],\n",
       "       [0.88465059],\n",
       "       [0.97454315],\n",
       "       [0.87272495],\n",
       "       [0.84150791],\n",
       "       [0.97632658],\n",
       "       [0.92019266],\n",
       "       [0.8749994 ],\n",
       "       [0.8364203 ],\n",
       "       [0.92310017],\n",
       "       [0.95852363],\n",
       "       [0.88681448],\n",
       "       [0.91128016],\n",
       "       [0.91097218],\n",
       "       [0.91885895],\n",
       "       [0.87321126],\n",
       "       [0.88140905],\n",
       "       [0.90844601],\n",
       "       [0.92479342],\n",
       "       [0.9045471 ],\n",
       "       [0.92308307],\n",
       "       [0.91830742],\n",
       "       [0.91901934],\n",
       "       [0.78279698],\n",
       "       [0.83591145],\n",
       "       [0.90410978],\n",
       "       [0.91543889],\n",
       "       [0.93330783],\n",
       "       [0.91943705],\n",
       "       [0.91041774],\n",
       "       [0.96590549],\n",
       "       [0.94926012],\n",
       "       [0.96204787],\n",
       "       [0.96911311],\n",
       "       [0.90172052],\n",
       "       [0.96474719],\n",
       "       [0.96008784],\n",
       "       [0.92574257],\n",
       "       [0.97185385],\n",
       "       [0.93993205],\n",
       "       [0.92750216],\n",
       "       [0.94248873],\n",
       "       [0.92583287],\n",
       "       [0.94419849],\n",
       "       [0.89535379],\n",
       "       [0.95921141],\n",
       "       [0.95230281],\n",
       "       [0.97084069],\n",
       "       [0.89982319],\n",
       "       [0.9682911 ],\n",
       "       [0.85106248],\n",
       "       [0.95633596],\n",
       "       [0.93309242],\n",
       "       [0.91292208],\n",
       "       [0.82299548],\n",
       "       [0.89925706],\n",
       "       [0.90617186],\n",
       "       [0.79584348],\n",
       "       [0.86454308],\n",
       "       [0.88979656],\n",
       "       [0.89395595],\n",
       "       [0.9114849 ],\n",
       "       [0.96877074],\n",
       "       [0.89252341],\n",
       "       [0.92453372],\n",
       "       [0.8222037 ],\n",
       "       [0.87607121],\n",
       "       [0.90684342],\n",
       "       [0.96205682],\n",
       "       [0.85880411],\n",
       "       [0.82749581],\n",
       "       [0.89145213],\n",
       "       [0.94167989],\n",
       "       [0.91928238],\n",
       "       [0.87886661],\n",
       "       [0.71808982],\n",
       "       [0.81929529],\n",
       "       [0.90845728],\n",
       "       [0.91339624],\n",
       "       [0.79753774],\n",
       "       [0.93100303],\n",
       "       [0.85392821],\n",
       "       [0.85484087],\n",
       "       [0.7846837 ],\n",
       "       [0.74866736],\n",
       "       [0.87013537],\n",
       "       [0.75165689],\n",
       "       [0.8782807 ],\n",
       "       [0.85346574],\n",
       "       [0.77384472],\n",
       "       [0.88965285],\n",
       "       [0.73575503],\n",
       "       [0.87821752],\n",
       "       [0.7108804 ],\n",
       "       [0.79742783],\n",
       "       [0.94761884],\n",
       "       [0.74923038],\n",
       "       [0.73518866],\n",
       "       [0.79969776],\n",
       "       [0.87428725],\n",
       "       [0.72756749],\n",
       "       [0.76134664],\n",
       "       [0.95104146],\n",
       "       [0.80351299],\n",
       "       [0.92829096],\n",
       "       [0.90732431],\n",
       "       [0.73689634],\n",
       "       [0.89603031],\n",
       "       [0.89642435],\n",
       "       [0.86856288],\n",
       "       [0.85261434],\n",
       "       [0.85504794],\n",
       "       [0.83346456],\n",
       "       [0.91222543],\n",
       "       [0.91285092],\n",
       "       [0.83197576],\n",
       "       [0.90526807],\n",
       "       [0.93844479],\n",
       "       [0.81495118],\n",
       "       [0.92727834],\n",
       "       [0.87803501],\n",
       "       [0.77667779],\n",
       "       [0.78241181],\n",
       "       [0.7152316 ],\n",
       "       [0.81771648],\n",
       "       [0.89571786],\n",
       "       [0.96842426],\n",
       "       [0.91922343],\n",
       "       [0.87440258],\n",
       "       [0.75102186],\n",
       "       [0.91647774],\n",
       "       [0.7333498 ],\n",
       "       [0.95810854],\n",
       "       [0.78018808],\n",
       "       [0.77262497],\n",
       "       [0.89731562],\n",
       "       [0.85557741],\n",
       "       [0.83924323],\n",
       "       [0.74498832],\n",
       "       [0.78600979],\n",
       "       [0.7753405 ],\n",
       "       [0.75364155],\n",
       "       [0.71768534],\n",
       "       [0.6821962 ],\n",
       "       [0.76444751],\n",
       "       [0.70895535],\n",
       "       [0.76671886],\n",
       "       [0.89398766],\n",
       "       [0.88079602],\n",
       "       [0.70147109],\n",
       "       [0.81311506],\n",
       "       [0.81930727],\n",
       "       [0.93823719],\n",
       "       [0.79694182],\n",
       "       [0.76780379],\n",
       "       [0.83130819],\n",
       "       [0.78236169],\n",
       "       [0.71441519],\n",
       "       [0.8549068 ],\n",
       "       [0.75086093],\n",
       "       [0.80691874],\n",
       "       [0.8833918 ],\n",
       "       [0.72656739],\n",
       "       [0.71244889],\n",
       "       [0.8383106 ],\n",
       "       [0.72084087],\n",
       "       [0.87631941],\n",
       "       [0.79620945],\n",
       "       [0.70867974],\n",
       "       [0.7371583 ],\n",
       "       [0.77837968],\n",
       "       [0.74785721],\n",
       "       [0.69408792],\n",
       "       [0.82146996],\n",
       "       [0.67059177],\n",
       "       [0.79698294],\n",
       "       [0.77221853],\n",
       "       [0.92350775],\n",
       "       [0.92640537],\n",
       "       [0.9377284 ],\n",
       "       [0.76974499],\n",
       "       [0.91880041],\n",
       "       [0.87820929],\n",
       "       [0.68298715],\n",
       "       [0.87464398],\n",
       "       [0.87381381],\n",
       "       [0.87277281],\n",
       "       [0.85830677],\n",
       "       [0.9496367 ],\n",
       "       [0.88555419],\n",
       "       [0.82005841],\n",
       "       [0.74234378],\n",
       "       [0.89889544],\n",
       "       [0.89467949],\n",
       "       [0.8459397 ],\n",
       "       [0.76016849],\n",
       "       [0.88254344],\n",
       "       [0.93259621],\n",
       "       [0.93145365],\n",
       "       [0.9656105 ],\n",
       "       [0.97337407],\n",
       "       [0.87971753],\n",
       "       [0.95613658],\n",
       "       [0.91393268],\n",
       "       [0.95963585],\n",
       "       [0.96757317],\n",
       "       [0.95898324],\n",
       "       [0.9312073 ],\n",
       "       [0.92286819],\n",
       "       [0.93505722],\n",
       "       [0.96645218],\n",
       "       [0.94370979],\n",
       "       [0.90654927],\n",
       "       [0.96176726],\n",
       "       [0.8731848 ],\n",
       "       [0.81030065],\n",
       "       [0.92246908],\n",
       "       [0.9715513 ],\n",
       "       [0.89260167],\n",
       "       [0.87984532],\n",
       "       [0.70976609],\n",
       "       [0.9592635 ],\n",
       "       [0.87192762],\n",
       "       [0.89836645],\n",
       "       [0.87593329],\n",
       "       [0.95424211],\n",
       "       [0.92550123],\n",
       "       [0.96341139],\n",
       "       [0.97299212],\n",
       "       [0.95503354],\n",
       "       [0.85233831],\n",
       "       [0.97725379],\n",
       "       [0.94434392],\n",
       "       [0.94974142],\n",
       "       [0.93687171],\n",
       "       [0.93958759],\n",
       "       [0.92701441],\n",
       "       [0.87215877],\n",
       "       [0.96945202],\n",
       "       [0.87125695],\n",
       "       [0.94083655],\n",
       "       [0.94820261],\n",
       "       [0.93228108],\n",
       "       [0.9809714 ],\n",
       "       [0.85051078],\n",
       "       [0.96649384],\n",
       "       [0.9492166 ],\n",
       "       [0.96155524],\n",
       "       [0.97603923],\n",
       "       [0.95434874],\n",
       "       [0.93265802],\n",
       "       [0.95100749],\n",
       "       [0.98228443],\n",
       "       [0.96871996],\n",
       "       [0.83866203],\n",
       "       [0.97405022],\n",
       "       [0.86743283],\n",
       "       [0.87244231],\n",
       "       [0.85054559],\n",
       "       [0.95143443],\n",
       "       [0.89994317],\n",
       "       [0.96225089],\n",
       "       [0.96647662],\n",
       "       [0.96775144],\n",
       "       [0.95518017],\n",
       "       [0.92530185],\n",
       "       [0.92067915],\n",
       "       [0.90865946],\n",
       "       [0.92444307],\n",
       "       [0.95057517],\n",
       "       [0.77121109],\n",
       "       [0.9642067 ],\n",
       "       [0.91853952],\n",
       "       [0.97010404],\n",
       "       [0.89799261],\n",
       "       [0.92154515],\n",
       "       [0.89379603],\n",
       "       [0.81158119],\n",
       "       [0.80226016],\n",
       "       [0.8681491 ],\n",
       "       [0.76933652],\n",
       "       [0.91599119],\n",
       "       [0.9057526 ],\n",
       "       [0.88387233],\n",
       "       [0.82774079],\n",
       "       [0.82317036],\n",
       "       [0.83949715],\n",
       "       [0.93595654],\n",
       "       [0.77771181],\n",
       "       [0.92766815],\n",
       "       [0.76083201],\n",
       "       [0.95063353],\n",
       "       [0.85322839],\n",
       "       [0.80902064],\n",
       "       [0.89060408],\n",
       "       [0.9766385 ],\n",
       "       [0.74495608],\n",
       "       [0.85292989],\n",
       "       [0.93932122],\n",
       "       [0.75128311],\n",
       "       [0.79713643],\n",
       "       [0.87846011],\n",
       "       [0.80670035],\n",
       "       [0.71936876],\n",
       "       [0.88177806],\n",
       "       [0.91033876],\n",
       "       [0.88881683],\n",
       "       [0.68625754],\n",
       "       [0.87489921],\n",
       "       [0.94721562],\n",
       "       [0.92347348],\n",
       "       [0.87228209],\n",
       "       [0.77581704],\n",
       "       [0.77176327],\n",
       "       [0.88349402],\n",
       "       [0.85277104],\n",
       "       [0.83317274],\n",
       "       [0.818066  ],\n",
       "       [0.86046517],\n",
       "       [0.72997159],\n",
       "       [0.8693133 ],\n",
       "       [0.93943238],\n",
       "       [0.79940045],\n",
       "       [0.97691184],\n",
       "       [0.97425687],\n",
       "       [0.72224927],\n",
       "       [0.9279291 ],\n",
       "       [0.69389981],\n",
       "       [0.69027352],\n",
       "       [0.92585647],\n",
       "       [0.96963322],\n",
       "       [0.74075192],\n",
       "       [0.72008121],\n",
       "       [0.79939753],\n",
       "       [0.73149568],\n",
       "       [0.75630677],\n",
       "       [0.89203417],\n",
       "       [0.74072504],\n",
       "       [0.8368609 ],\n",
       "       [0.70389885],\n",
       "       [0.94013214],\n",
       "       [0.69761765],\n",
       "       [0.7965939 ],\n",
       "       [0.87089014],\n",
       "       [0.75070733],\n",
       "       [0.88364017],\n",
       "       [0.96689045],\n",
       "       [0.8828299 ],\n",
       "       [0.75003576],\n",
       "       [0.70914912],\n",
       "       [0.76378119],\n",
       "       [0.71306849],\n",
       "       [0.7822845 ],\n",
       "       [0.72321671],\n",
       "       [0.91488552],\n",
       "       [0.72345787],\n",
       "       [0.89693791],\n",
       "       [0.75249344],\n",
       "       [0.91582817],\n",
       "       [0.85075039],\n",
       "       [0.93621987],\n",
       "       [0.84355128],\n",
       "       [0.83838552],\n",
       "       [0.70047963],\n",
       "       [0.90712094],\n",
       "       [0.85249007],\n",
       "       [0.87907112],\n",
       "       [0.88926297],\n",
       "       [0.85090172],\n",
       "       [0.93679559],\n",
       "       [0.81356275],\n",
       "       [0.80526292],\n",
       "       [0.89893502],\n",
       "       [0.9100098 ],\n",
       "       [0.93421417],\n",
       "       [0.8832981 ],\n",
       "       [0.88016659],\n",
       "       [0.93355608],\n",
       "       [0.89988488],\n",
       "       [0.84066117],\n",
       "       [0.92124647],\n",
       "       [0.73992735],\n",
       "       [0.75359344],\n",
       "       [0.85494024],\n",
       "       [0.82660669],\n",
       "       [0.83445346],\n",
       "       [0.8688764 ],\n",
       "       [0.89751685],\n",
       "       [0.87786055],\n",
       "       [0.92026162],\n",
       "       [0.72763419],\n",
       "       [0.97289354],\n",
       "       [0.85536104],\n",
       "       [0.94371277],\n",
       "       [0.97037929],\n",
       "       [0.7997759 ],\n",
       "       [0.87212396],\n",
       "       [0.91783994],\n",
       "       [0.84694761],\n",
       "       [0.89162463],\n",
       "       [0.81467599],\n",
       "       [0.90367657],\n",
       "       [0.82714087],\n",
       "       [0.97742635],\n",
       "       [0.92145443],\n",
       "       [0.9615742 ],\n",
       "       [0.97045904],\n",
       "       [0.92605376],\n",
       "       [0.91621459],\n",
       "       [0.80381233],\n",
       "       [0.96597499],\n",
       "       [0.88902569],\n",
       "       [0.79968953],\n",
       "       [0.95238793],\n",
       "       [0.95485377],\n",
       "       [0.84072924],\n",
       "       [0.92236573],\n",
       "       [0.87313783],\n",
       "       [0.9721328 ],\n",
       "       [0.97578681],\n",
       "       [0.96113485],\n",
       "       [0.91644001],\n",
       "       [0.96124035],\n",
       "       [0.93840271],\n",
       "       [0.9356516 ],\n",
       "       [0.98789948],\n",
       "       [0.92279124],\n",
       "       [0.94749606],\n",
       "       [0.96479136],\n",
       "       [0.99456584],\n",
       "       [0.8336578 ],\n",
       "       [0.9799819 ],\n",
       "       [0.91859382],\n",
       "       [0.94207162],\n",
       "       [0.98038131],\n",
       "       [0.9191758 ],\n",
       "       [0.80085868],\n",
       "       [0.96607   ],\n",
       "       [0.99175417],\n",
       "       [0.88946569],\n",
       "       [0.95879745],\n",
       "       [0.97239166],\n",
       "       [0.97573799],\n",
       "       [0.98563254],\n",
       "       [0.99282491],\n",
       "       [0.94718474],\n",
       "       [0.98934096],\n",
       "       [0.82906115],\n",
       "       [0.97077954],\n",
       "       [0.85592186],\n",
       "       [0.96039569],\n",
       "       [0.97920418],\n",
       "       [0.86315101],\n",
       "       [0.97732246],\n",
       "       [0.92144758],\n",
       "       [0.92572492],\n",
       "       [0.96389604],\n",
       "       [0.75708681],\n",
       "       [0.86654705],\n",
       "       [0.93012506],\n",
       "       [0.92980897],\n",
       "       [0.98732054],\n",
       "       [0.94687587],\n",
       "       [0.93230551],\n",
       "       [0.88772452],\n",
       "       [0.77500629],\n",
       "       [0.96872979],\n",
       "       [0.84380007],\n",
       "       [0.90356356],\n",
       "       [0.8052125 ],\n",
       "       [0.95714551],\n",
       "       [0.96166259],\n",
       "       [0.88714629],\n",
       "       [0.90470982],\n",
       "       [0.90416044],\n",
       "       [0.84682906],\n",
       "       [0.93652344],\n",
       "       [0.84450346],\n",
       "       [0.71673006],\n",
       "       [0.78215176],\n",
       "       [0.89130956],\n",
       "       [0.84658229],\n",
       "       [0.73816341],\n",
       "       [0.99090868],\n",
       "       [0.74611777],\n",
       "       [0.83911061],\n",
       "       [0.90232861],\n",
       "       [0.83243722],\n",
       "       [0.92253631],\n",
       "       [0.88714606],\n",
       "       [0.72650003],\n",
       "       [0.76038522],\n",
       "       [0.71074474],\n",
       "       [0.838799  ],\n",
       "       [0.82262868],\n",
       "       [0.93183649],\n",
       "       [0.69497591],\n",
       "       [0.91031361],\n",
       "       [0.91565853],\n",
       "       [0.71093476],\n",
       "       [0.86079645],\n",
       "       [0.81344628],\n",
       "       [0.79897189],\n",
       "       [0.94280112],\n",
       "       [0.98268145],\n",
       "       [0.92621541],\n",
       "       [0.7874977 ],\n",
       "       [0.88872427],\n",
       "       [0.82141966],\n",
       "       [0.73225403],\n",
       "       [0.88538319],\n",
       "       [0.81981468],\n",
       "       [0.93948656],\n",
       "       [0.80114132],\n",
       "       [0.98519343],\n",
       "       [0.87569481],\n",
       "       [0.85562748],\n",
       "       [0.84562135],\n",
       "       [0.91579992],\n",
       "       [0.70227551],\n",
       "       [0.77359122],\n",
       "       [0.72211915],\n",
       "       [0.87620741],\n",
       "       [0.87684304],\n",
       "       [0.93378127],\n",
       "       [0.77973098],\n",
       "       [0.97134227],\n",
       "       [0.85326511],\n",
       "       [0.87954873],\n",
       "       [0.71790719],\n",
       "       [0.82511187],\n",
       "       [0.95801997],\n",
       "       [0.67129993],\n",
       "       [0.84270573],\n",
       "       [0.80375922],\n",
       "       [0.71903217],\n",
       "       [0.88244474],\n",
       "       [0.87354892],\n",
       "       [0.98485005],\n",
       "       [0.81602913],\n",
       "       [0.89041322],\n",
       "       [0.76488954],\n",
       "       [0.90644711],\n",
       "       [0.88893634],\n",
       "       [0.71995711],\n",
       "       [0.81887484],\n",
       "       [0.78653705],\n",
       "       [0.74650979],\n",
       "       [0.9195056 ],\n",
       "       [0.77071369],\n",
       "       [0.70767975],\n",
       "       [0.8882466 ],\n",
       "       [0.6976704 ],\n",
       "       [0.90096891],\n",
       "       [0.72178578],\n",
       "       [0.81326371],\n",
       "       [0.75335181],\n",
       "       [0.73241359],\n",
       "       [0.89058471],\n",
       "       [0.6648891 ],\n",
       "       [0.96444947],\n",
       "       [0.84019184],\n",
       "       [0.69642764],\n",
       "       [0.95744932],\n",
       "       [0.94936335],\n",
       "       [0.71139812],\n",
       "       [0.69968277],\n",
       "       [0.94740456],\n",
       "       [0.81797647],\n",
       "       [0.98232818],\n",
       "       [0.84325296],\n",
       "       [0.80531174],\n",
       "       [0.68667281],\n",
       "       [0.92097318],\n",
       "       [0.95506054],\n",
       "       [0.68763834],\n",
       "       [0.89170164],\n",
       "       [0.98309571],\n",
       "       [0.91052526],\n",
       "       [0.89015687],\n",
       "       [0.99590898],\n",
       "       [0.65297163],\n",
       "       [0.85087705],\n",
       "       [0.96057212],\n",
       "       [0.69432265],\n",
       "       [0.9579885 ],\n",
       "       [0.86898279],\n",
       "       [0.92518878],\n",
       "       [0.69092721],\n",
       "       [0.68595541],\n",
       "       [0.82946807],\n",
       "       [0.92781591],\n",
       "       [0.91609913],\n",
       "       [0.96534449],\n",
       "       [0.75650883],\n",
       "       [0.82353514],\n",
       "       [0.74047166],\n",
       "       [0.97321755],\n",
       "       [0.86816502],\n",
       "       [0.73959661],\n",
       "       [0.95947891],\n",
       "       [0.93234456],\n",
       "       [0.8970145 ],\n",
       "       [0.90223008],\n",
       "       [0.77388299],\n",
       "       [0.908925  ],\n",
       "       [0.882882  ],\n",
       "       [0.92110717],\n",
       "       [0.93240106],\n",
       "       [0.99100763],\n",
       "       [0.93525803],\n",
       "       [0.77101594],\n",
       "       [0.98847252],\n",
       "       [0.9621008 ],\n",
       "       [0.74964142],\n",
       "       [0.74333173],\n",
       "       [0.92337066],\n",
       "       [0.93541288],\n",
       "       [0.98604834],\n",
       "       [0.89592773],\n",
       "       [0.91823906],\n",
       "       [0.95864874],\n",
       "       [0.71962303],\n",
       "       [0.91679788],\n",
       "       [0.98485571],\n",
       "       [0.99767643],\n",
       "       [0.91761148],\n",
       "       [0.98265874],\n",
       "       [0.99059147],\n",
       "       [0.9844141 ],\n",
       "       [0.97686857],\n",
       "       [0.91323143],\n",
       "       [0.97398347],\n",
       "       [0.99143982],\n",
       "       [0.99420655],\n",
       "       [0.9818812 ],\n",
       "       [0.95853901],\n",
       "       [0.98812103],\n",
       "       [0.9861747 ],\n",
       "       [0.98669922],\n",
       "       [0.97173786],\n",
       "       [0.98959702],\n",
       "       [0.94949901],\n",
       "       [0.9888227 ],\n",
       "       [0.98896772],\n",
       "       [0.97682661],\n",
       "       [0.98411775],\n",
       "       [0.98501402],\n",
       "       [0.99528176],\n",
       "       [0.80772442],\n",
       "       [0.72728622],\n",
       "       [0.96890378],\n",
       "       [0.80671924],\n",
       "       [0.99128813],\n",
       "       [0.86229789],\n",
       "       [0.93480593],\n",
       "       [0.92304003],\n",
       "       [0.91434771],\n",
       "       [0.97262508],\n",
       "       [0.95187968],\n",
       "       [0.99644899],\n",
       "       [0.88181305],\n",
       "       [0.95265365],\n",
       "       [0.77293247],\n",
       "       [0.88188338],\n",
       "       [0.97471493],\n",
       "       [0.97189766],\n",
       "       [0.88780099],\n",
       "       [0.9936626 ],\n",
       "       [0.97176826],\n",
       "       [0.99341536],\n",
       "       [0.82777113],\n",
       "       [0.97543657],\n",
       "       [0.97892511],\n",
       "       [0.99568403],\n",
       "       [0.92684162],\n",
       "       [0.86486757],\n",
       "       [0.80690324],\n",
       "       [0.96464026],\n",
       "       [0.81985241],\n",
       "       [0.80080295],\n",
       "       [0.9976095 ],\n",
       "       [0.97674483],\n",
       "       [0.95930344],\n",
       "       [0.96734023],\n",
       "       [0.78399104],\n",
       "       [0.80531645],\n",
       "       [0.93380278],\n",
       "       [0.80094826],\n",
       "       [0.73037338],\n",
       "       [0.8884806 ],\n",
       "       [0.67577714],\n",
       "       [0.9422676 ],\n",
       "       [0.76741689],\n",
       "       [0.71288133],\n",
       "       [0.85756123],\n",
       "       [0.83174711],\n",
       "       [0.87176269],\n",
       "       [0.81298786],\n",
       "       [0.93787962],\n",
       "       [0.81264132],\n",
       "       [0.92912942],\n",
       "       [0.99073911],\n",
       "       [0.83480281],\n",
       "       [0.82477742],\n",
       "       [0.94972605],\n",
       "       [0.83883077],\n",
       "       [0.983751  ],\n",
       "       [0.9770658 ],\n",
       "       [0.78585863],\n",
       "       [0.88936865],\n",
       "       [0.86525488],\n",
       "       [0.77164352],\n",
       "       [0.68859279],\n",
       "       [0.93307352],\n",
       "       [0.7537089 ],\n",
       "       [0.98614162],\n",
       "       [0.96309006],\n",
       "       [0.94933575],\n",
       "       [0.85194582],\n",
       "       [0.98740828],\n",
       "       [0.78063214],\n",
       "       [0.9568525 ],\n",
       "       [0.86457777],\n",
       "       [0.96768403],\n",
       "       [0.94985348],\n",
       "       [0.86345518],\n",
       "       [0.86139637],\n",
       "       [0.84352165],\n",
       "       [0.98857456],\n",
       "       [0.85730243],\n",
       "       [0.95845085],\n",
       "       [0.8413204 ],\n",
       "       [0.94307619],\n",
       "       [0.89023077],\n",
       "       [0.97854692],\n",
       "       [0.98093182],\n",
       "       [0.8227635 ],\n",
       "       [0.95135146],\n",
       "       [0.70105451],\n",
       "       [0.97460562],\n",
       "       [0.77951729],\n",
       "       [0.89057481],\n",
       "       [0.69204307],\n",
       "       [0.95280081],\n",
       "       [0.88505781],\n",
       "       [0.99300015],\n",
       "       [0.88890511],\n",
       "       [0.84057808],\n",
       "       [0.86718768],\n",
       "       [0.71842742],\n",
       "       [0.70370311],\n",
       "       [0.81468922],\n",
       "       [0.96556169],\n",
       "       [0.99914622],\n",
       "       [0.88261569],\n",
       "       [0.72722101],\n",
       "       [0.96605796],\n",
       "       [0.95136535],\n",
       "       [0.829835  ],\n",
       "       [0.99586123],\n",
       "       [0.97414827],\n",
       "       [0.80807179],\n",
       "       [0.87987566],\n",
       "       [0.9983961 ],\n",
       "       [0.97366661],\n",
       "       [0.99194545],\n",
       "       [0.99884272],\n",
       "       [0.7298609 ],\n",
       "       [0.89810723],\n",
       "       [0.99704868],\n",
       "       [0.77702379],\n",
       "       [0.80172145],\n",
       "       [0.99587327],\n",
       "       [0.99970156],\n",
       "       [0.85042554],\n",
       "       [0.83096266],\n",
       "       [0.99998069],\n",
       "       [0.94684857],\n",
       "       [0.81795377],\n",
       "       [0.96545458],\n",
       "       [0.88045645],\n",
       "       [0.70522064],\n",
       "       [0.9998548 ],\n",
       "       [0.89036983],\n",
       "       [0.99955755],\n",
       "       [0.83114207],\n",
       "       [0.72755992],\n",
       "       [0.93906528],\n",
       "       [0.98097014],\n",
       "       [0.99297661],\n",
       "       [0.99348736],\n",
       "       [0.95286936],\n",
       "       [0.99014074],\n",
       "       [0.99955994],\n",
       "       [0.77171785],\n",
       "       [0.93147558],\n",
       "       [0.99999642],\n",
       "       [0.9070797 ],\n",
       "       [0.99722016],\n",
       "       [0.70107222],\n",
       "       [0.87031692],\n",
       "       [0.98866254],\n",
       "       [0.99801528],\n",
       "       [0.99993348],\n",
       "       [0.97273195],\n",
       "       [0.99975926],\n",
       "       [0.87817293],\n",
       "       [0.93901449],\n",
       "       [0.96103805],\n",
       "       [0.88090366],\n",
       "       [0.99551779],\n",
       "       [0.99838448],\n",
       "       [0.99997973],\n",
       "       [0.99958116],\n",
       "       [0.99516857],\n",
       "       [0.99459076],\n",
       "       [0.9935813 ],\n",
       "       [0.74729383],\n",
       "       [0.6570856 ],\n",
       "       [0.9992767 ],\n",
       "       [0.99597663],\n",
       "       [0.99454194],\n",
       "       [0.99478936],\n",
       "       [0.99020785],\n",
       "       [0.98828375],\n",
       "       [0.99833578],\n",
       "       [0.9999814 ],\n",
       "       [0.99458647],\n",
       "       [0.99766088],\n",
       "       [0.97959799],\n",
       "       [0.99728656],\n",
       "       [0.99938333],\n",
       "       [0.99996984],\n",
       "       [0.99970919],\n",
       "       [0.99991143],\n",
       "       [0.99619675],\n",
       "       [0.97069395],\n",
       "       [0.87267756],\n",
       "       [0.82238108],\n",
       "       [0.9973231 ],\n",
       "       [0.99835855],\n",
       "       [0.99784565],\n",
       "       [0.99473369],\n",
       "       [0.97478545],\n",
       "       [0.99370122],\n",
       "       [0.99172872],\n",
       "       [0.99996734],\n",
       "       [0.98377788],\n",
       "       [0.99974769],\n",
       "       [0.99978417],\n",
       "       [0.95656669],\n",
       "       [0.98699772],\n",
       "       [0.96125787],\n",
       "       [0.96293783],\n",
       "       [0.99622166],\n",
       "       [0.98535639],\n",
       "       [0.99998844],\n",
       "       [0.99947089],\n",
       "       [0.99197632],\n",
       "       [0.99838424],\n",
       "       [0.99993443],\n",
       "       [0.99270731],\n",
       "       [0.9436363 ],\n",
       "       [0.99834907],\n",
       "       [0.99191391],\n",
       "       [0.97745776],\n",
       "       [0.96233082],\n",
       "       [0.99575931],\n",
       "       [0.97703075],\n",
       "       [0.99729854],\n",
       "       [0.99891758],\n",
       "       [0.96790999],\n",
       "       [0.99515605],\n",
       "       [0.92224371],\n",
       "       [0.98539644],\n",
       "       [0.99624312],\n",
       "       [0.69787008],\n",
       "       [0.97593212],\n",
       "       [0.92616206],\n",
       "       [0.99816656],\n",
       "       [0.98826164],\n",
       "       [0.77523363],\n",
       "       [0.81408191],\n",
       "       [0.9878704 ],\n",
       "       [0.95706666],\n",
       "       [0.99758017],\n",
       "       [0.99777907],\n",
       "       [0.85698897],\n",
       "       [0.87585753],\n",
       "       [0.7965098 ],\n",
       "       [0.78369421],\n",
       "       [0.89874893],\n",
       "       [0.77733761],\n",
       "       [0.94534153],\n",
       "       [0.99997365],\n",
       "       [0.95359516],\n",
       "       [0.99985754],\n",
       "       [0.79218858],\n",
       "       [0.97796941],\n",
       "       [0.789276  ],\n",
       "       [0.99999964],\n",
       "       [0.80230242],\n",
       "       [0.99401629],\n",
       "       [0.83816159],\n",
       "       [0.97145778],\n",
       "       [0.98338944],\n",
       "       [0.91435117],\n",
       "       [0.95841765],\n",
       "       [0.99991393],\n",
       "       [0.94956023],\n",
       "       [0.89946622],\n",
       "       [0.74682122],\n",
       "       [0.93854886],\n",
       "       [0.90144974],\n",
       "       [0.89121383],\n",
       "       [0.93839598],\n",
       "       [0.84864765],\n",
       "       [0.99118137],\n",
       "       [0.97064936],\n",
       "       [0.89262474],\n",
       "       [0.93919712],\n",
       "       [0.83618051],\n",
       "       [0.99812502],\n",
       "       [0.90348917],\n",
       "       [0.99955481],\n",
       "       [0.9713859 ],\n",
       "       [0.76508373],\n",
       "       [0.84439284],\n",
       "       [0.99724752],\n",
       "       [0.84135896],\n",
       "       [0.9945094 ],\n",
       "       [0.66930956],\n",
       "       [0.67516047],\n",
       "       [0.96434307],\n",
       "       [0.78617638],\n",
       "       [0.85417134],\n",
       "       [0.9272871 ],\n",
       "       [0.72732228],\n",
       "       [0.78185248],\n",
       "       [0.86784399],\n",
       "       [0.91151422],\n",
       "       [0.99224663],\n",
       "       [0.88638794],\n",
       "       [0.93713808],\n",
       "       [0.86041451],\n",
       "       [0.8212105 ],\n",
       "       [0.69722438],\n",
       "       [0.99051917],\n",
       "       [0.98670548],\n",
       "       [0.78069389]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제값과 예측값 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 실제 값과 예측값을 시각화하는 코드\n",
    "plt.figure(figsize=(10, 6))  # 그래프 크기 설정\n",
    "plt.plot(actual, label='Actual Values')  # 실제 값 그래프\n",
    "plt.plot(pred, label='Predicted Values', alpha=0.7)  # 예측 값 그래프, 투명도를 주어 구분하기 쉽게 함\n",
    "plt.title('Actual vs Predicted Values')  # 그래프 제목\n",
    "plt.xlabel('Sample Index')  # x축 라벨\n",
    "plt.ylabel('Value')  # y축 라벨\n",
    "plt.legend()  # 범례 표시\n",
    "plt.show()  # 그래프 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'../model_b_ar1_l24_v5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('rnn.weight_ih_l0',\n",
       "              tensor([[ 0.1039,  0.0838, -0.1345,  ...,  0.1262,  0.1138, -0.0422],\n",
       "                      [-0.0543, -0.0084, -0.0415,  ..., -0.0728, -0.1225,  0.0680],\n",
       "                      [ 0.0125,  0.0145, -0.0224,  ..., -0.0779, -0.1662, -0.0740],\n",
       "                      ...,\n",
       "                      [-0.0314,  0.1238,  0.0245,  ..., -0.1601, -0.0749, -0.0538],\n",
       "                      [-0.0404, -0.0957, -0.0752,  ..., -0.0707,  0.1202,  0.0264],\n",
       "                      [-0.0030, -0.0952,  0.1127,  ...,  0.0591,  0.0041,  0.2346]])),\n",
       "             ('rnn.weight_hh_l0',\n",
       "              tensor([[ 0.1060,  0.0003, -0.0118,  ..., -0.1039,  0.0170, -0.0597],\n",
       "                      [-0.0659,  0.1109,  0.0839,  ...,  0.0375, -0.1409,  0.0122],\n",
       "                      [ 0.0917, -0.0679,  0.0214,  ...,  0.0198, -0.1272,  0.1387],\n",
       "                      ...,\n",
       "                      [-0.0450,  0.1325,  0.0458,  ..., -0.1018, -0.1425,  0.0820],\n",
       "                      [-0.0392, -0.0093,  0.1395,  ..., -0.1023,  0.1221, -0.0839],\n",
       "                      [ 0.0990,  0.0605,  0.0761,  ..., -0.1361, -0.0224,  0.0116]])),\n",
       "             ('rnn.bias_ih_l0',\n",
       "              tensor([-0.0037,  0.0312,  0.0539,  0.0643,  0.1277,  0.0605,  0.0769,  0.1090,\n",
       "                      -0.0556, -0.0756,  0.1205, -0.0755,  0.1024,  0.1186,  0.0924, -0.0084,\n",
       "                      -0.0669, -0.0561, -0.0450, -0.1032, -0.0319,  0.0933,  0.1793, -0.0767,\n",
       "                      -0.0375,  0.1492, -0.0085,  0.1753,  0.0811,  0.0369, -0.0654,  0.0604,\n",
       "                       0.0154, -0.0514, -0.1335, -0.0039,  0.1102,  0.0200,  0.0563,  0.0997,\n",
       "                       0.1455,  0.1694,  0.0616,  0.0283, -0.0132,  0.0214,  0.0646,  0.0603])),\n",
       "             ('rnn.bias_hh_l0',\n",
       "              tensor([-1.0784e-01,  4.2007e-02, -1.2580e-02, -1.1866e-01,  1.0509e-01,\n",
       "                       8.5107e-02,  1.0648e-01,  1.0165e-01,  3.8517e-02, -5.6606e-02,\n",
       "                       1.6515e-01,  8.3611e-02, -3.3045e-02, -6.2630e-02,  2.8664e-02,\n",
       "                      -3.7135e-02, -1.6434e-01, -8.6093e-02, -1.8117e-02,  1.1077e-01,\n",
       "                       1.0686e-01,  1.5095e-01, -7.7680e-02,  1.0488e-04,  1.1316e-01,\n",
       "                       7.7217e-02,  1.8302e-01,  7.1912e-02, -4.4372e-02, -2.8372e-02,\n",
       "                      -7.7770e-02,  5.7588e-02, -1.1700e-01, -5.2887e-02,  5.7090e-03,\n",
       "                       7.5478e-02,  1.0270e-01,  7.2330e-03, -7.3424e-02,  5.3600e-02,\n",
       "                      -9.7085e-02, -5.4941e-02, -5.2750e-02,  7.9572e-02, -3.2405e-02,\n",
       "                       4.1013e-02,  1.1329e-01,  1.4532e-01])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[ 0.1325, -0.0130,  0.1937,  ...,  0.0800,  0.0522,  0.1722],\n",
       "                      [-0.0499,  0.0932, -0.0726,  ..., -0.0284,  0.1155,  0.0531],\n",
       "                      [ 0.0416,  0.1179, -0.0252,  ..., -0.0807, -0.1477,  0.0646],\n",
       "                      ...,\n",
       "                      [-0.0390, -0.0289,  0.0032,  ...,  0.1320,  0.0763,  0.1007],\n",
       "                      [-0.0333,  0.1091, -0.0336,  ...,  0.1638, -0.0195, -0.0662],\n",
       "                      [-0.0675, -0.0892,  0.0013,  ..., -0.0589,  0.0989,  0.1555]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([ 0.0095,  0.1371,  0.0002,  0.0683, -0.0356, -0.0709,  0.0499, -0.0458,\n",
       "                      -0.0937,  0.1347, -0.0998,  0.1184,  0.0302,  0.0367,  0.1485,  0.0396,\n",
       "                      -0.0826,  0.1648,  0.2015, -0.0005,  0.1356,  0.1795,  0.0589,  0.0074])),\n",
       "             ('fc.2.weight',\n",
       "              tensor([[-4.2658e-02,  7.6899e-03, -1.7645e-01, -1.7158e-01, -6.8894e-02,\n",
       "                        1.7346e-01,  8.8389e-02, -2.3191e-01, -1.6541e-01, -8.4349e-02,\n",
       "                       -3.2460e-02,  1.0028e-01, -8.6095e-02, -1.2055e-01,  2.0459e-02,\n",
       "                        1.3739e-01,  1.1566e-01, -9.1514e-02, -8.3178e-03, -1.6544e-01,\n",
       "                        1.3756e-01, -6.1832e-02, -1.3105e-02,  8.0097e-03],\n",
       "                      [ 1.3233e-01,  1.2428e-01, -4.1316e-03, -7.9042e-02,  5.7000e-03,\n",
       "                       -2.4020e-03, -2.0786e-01, -6.2599e-02, -1.6619e-01,  1.2915e-01,\n",
       "                       -2.5884e-02, -9.1523e-02, -1.7845e-01,  2.3209e-01,  8.7000e-02,\n",
       "                        8.4414e-02, -3.0459e-02,  2.8908e-01,  1.7285e-01, -1.7614e-01,\n",
       "                       -3.5117e-02,  2.3344e-01,  2.7625e-01, -1.7339e-02],\n",
       "                      [ 2.3337e-01,  2.1416e-01,  1.4910e-01,  2.5619e-01,  2.2657e-01,\n",
       "                       -6.8492e-02, -6.1098e-02, -8.0705e-02, -1.8785e-01,  1.4961e-01,\n",
       "                       -2.4026e-01,  2.4812e-01, -1.9880e-01,  1.7969e-01, -2.8121e-01,\n",
       "                       -1.9857e-01,  2.1188e-01,  2.0458e-01,  1.8623e-01, -1.0888e-01,\n",
       "                       -1.5931e-01,  2.5041e-01,  2.2636e-01,  1.0797e-01],\n",
       "                      [ 1.6303e-01, -8.0404e-02, -1.1640e-01,  2.5743e-02,  8.8867e-02,\n",
       "                        1.1958e-01, -3.2281e-02,  1.5581e-01,  8.6279e-02, -1.0220e-01,\n",
       "                        1.6767e-01, -5.0617e-02, -1.6695e-01, -1.0098e-01, -4.9249e-02,\n",
       "                        1.7235e-01,  1.6691e-01, -1.2717e-01, -1.0216e-02, -7.1111e-02,\n",
       "                        1.5406e-01, -1.9141e-01,  6.6167e-02, -1.8161e-01],\n",
       "                      [-5.5843e-02, -7.6326e-02,  9.4495e-02, -9.9457e-02, -1.6917e-01,\n",
       "                       -1.1479e-01, -3.7129e-02, -1.3651e-01, -7.6244e-02, -1.8002e-01,\n",
       "                        1.3919e-01,  1.2408e-01,  1.8459e-01, -1.0066e-01,  2.2637e-01,\n",
       "                        3.8248e-02,  4.5887e-03,  6.8322e-02,  1.5170e-01,  2.3446e-01,\n",
       "                        3.1075e-02, -8.1138e-02,  5.6462e-04, -3.6161e-02],\n",
       "                      [ 1.6301e-01,  7.8322e-02, -2.0734e-01, -1.5362e-01, -1.5742e-01,\n",
       "                       -1.6136e-02,  1.8048e-01, -2.5187e-01,  3.5080e-02, -1.8119e-01,\n",
       "                       -1.6786e-01, -7.6718e-02, -9.4987e-02, -4.2595e-02,  1.0366e-01,\n",
       "                        8.9761e-02,  3.9195e-02,  9.2326e-02, -9.0185e-02,  1.0484e-01,\n",
       "                       -1.1034e-01, -1.4677e-01, -5.5357e-02,  1.5811e-01],\n",
       "                      [ 6.7065e-02,  2.3267e-02, -7.6890e-02,  1.8220e-01,  2.3520e-01,\n",
       "                       -5.0590e-02,  1.3568e-01, -9.9617e-02, -1.0743e-01, -2.4358e-02,\n",
       "                       -4.6113e-02,  2.2570e-01, -1.6856e-01,  1.1687e-01, -1.8312e-01,\n",
       "                       -8.3036e-02, -7.0093e-02,  2.3943e-01,  1.6811e-01,  7.0535e-02,\n",
       "                       -1.5317e-01,  1.4111e-03,  1.3320e-01, -5.7317e-02],\n",
       "                      [-1.4191e-01, -1.5495e-02, -2.3231e-01,  1.3201e-01,  9.7178e-02,\n",
       "                       -5.9363e-02,  9.3116e-02, -8.3420e-02, -7.6604e-02, -1.1555e-01,\n",
       "                        5.0793e-02,  6.6265e-02,  1.7606e-01, -1.9072e-01, -1.2688e-01,\n",
       "                        1.2649e-01, -5.3891e-05, -1.5884e-01,  1.3416e-01, -2.5659e-02,\n",
       "                        4.3759e-02, -6.8823e-02,  2.6793e-02, -1.9855e-01],\n",
       "                      [-9.4707e-02, -1.5068e-01, -2.3682e-02,  5.7564e-02,  8.7299e-02,\n",
       "                       -7.1006e-02,  1.6913e-01, -1.0283e-01, -2.2505e-02, -1.6118e-01,\n",
       "                       -5.5046e-02,  8.1270e-03,  1.5342e-01, -3.2953e-02,  2.3394e-02,\n",
       "                       -1.3895e-01, -1.5568e-01,  5.3653e-03, -1.0776e-01,  1.9250e-01,\n",
       "                       -1.1937e-01,  1.5895e-02,  1.6764e-01, -6.7086e-02],\n",
       "                      [ 2.3933e-02,  6.2345e-02, -5.6899e-02,  1.8599e-01,  2.1935e-01,\n",
       "                       -1.0596e-01,  5.1392e-02, -2.0495e-02,  5.9616e-03, -2.5902e-02,\n",
       "                       -1.4818e-01,  8.3595e-02, -2.9082e-01,  1.8535e-01,  9.1882e-02,\n",
       "                       -2.5840e-01,  4.1967e-02,  2.6787e-01, -1.9745e-02, -1.2030e-01,\n",
       "                       -8.2776e-02, -4.6634e-02,  3.0699e-02,  1.6218e-01],\n",
       "                      [ 1.0836e-01,  1.2471e-02, -8.2938e-02,  5.6783e-02, -1.7797e-01,\n",
       "                        3.7639e-02,  1.4122e-01, -1.6814e-01, -1.6422e-01, -2.2235e-02,\n",
       "                       -6.4634e-02, -1.2813e-01,  2.1049e-01,  1.3838e-01,  1.3511e-01,\n",
       "                        1.6921e-02, -1.4637e-01, -6.8470e-02,  1.1303e-01, -1.1185e-01,\n",
       "                        1.9891e-01, -1.2666e-01, -1.1372e-01, -5.1833e-02],\n",
       "                      [-7.8791e-02,  1.8960e-02,  8.5233e-02, -8.1813e-02,  1.3774e-01,\n",
       "                       -1.5695e-01, -5.4859e-02, -7.9619e-02,  1.5228e-01, -9.8516e-02,\n",
       "                        1.4903e-01, -1.0942e-01,  9.3161e-03, -5.5590e-03,  1.7969e-02,\n",
       "                       -7.6087e-02, -1.8950e-02, -2.4601e-01,  4.9151e-02,  5.8709e-02,\n",
       "                        1.7602e-01,  1.0581e-01,  1.6478e-01,  1.5677e-02]])),\n",
       "             ('fc.2.bias',\n",
       "              tensor([ 0.1036,  0.0294,  0.1622, -0.0303, -0.0832,  0.0359, -0.1727,  0.0992,\n",
       "                      -0.0459, -0.0022, -0.0097, -0.0845])),\n",
       "             ('fc.4.weight',\n",
       "              tensor([[-0.0538,  0.0840, -0.3015, -0.1766, -0.0140,  0.2495,  0.2232, -0.0164,\n",
       "                        0.1533,  0.1278,  0.2782,  0.0377],\n",
       "                      [-0.2858,  0.3217,  0.3355, -0.1941, -0.2749,  0.0015,  0.0910, -0.2345,\n",
       "                       -0.1096,  0.1175, -0.1051, -0.2978],\n",
       "                      [ 0.0644,  0.1511, -0.2423, -0.1396, -0.0271,  0.2869,  0.1483, -0.0613,\n",
       "                       -0.1562,  0.0501, -0.0916,  0.0909],\n",
       "                      [-0.0409,  0.2069,  0.0178, -0.1562,  0.1461, -0.1962,  0.1300, -0.1441,\n",
       "                       -0.2581,  0.2949,  0.1510,  0.2026],\n",
       "                      [-0.1115, -0.0249,  0.0986, -0.2173, -0.1568, -0.0122,  0.1537, -0.3385,\n",
       "                       -0.1845,  0.2123,  0.0708, -0.0232],\n",
       "                      [-0.0704,  0.0708, -0.0773, -0.3248,  0.2295, -0.0791, -0.1005, -0.2757,\n",
       "                        0.0693, -0.1615, -0.2271,  0.1938]])),\n",
       "             ('fc.4.bias',\n",
       "              tensor([ 0.2215,  0.2283, -0.2497,  0.1263,  0.2904,  0.0796])),\n",
       "             ('fc.6.weight',\n",
       "              tensor([[-0.3009,  0.4854, -0.1870,  0.3498,  0.2415, -0.0058]])),\n",
       "             ('fc.6.bias', tensor([0.3929]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "l = 24\n",
    "\n",
    "# 관리상태 / 이상상태 데이터 생성 함수\n",
    "def argen(ar, psi, delta,gamma, length) :\n",
    "\n",
    "    e = np.random.normal(loc=0, scale = 1,size = length)\n",
    "    sigma = math.sqrt(1 / (1 - pow(ar, 2)))\n",
    "    x = np.array(np.repeat(0, length), dtype= np.float64)\n",
    "    x[0] = e[0]\n",
    "    z = np.array(np.repeat(0, length), dtype=np.float64)\n",
    "\n",
    "    for i in range(1, psi):\n",
    "        x[i] = ar * x[i-1] + e[i]\n",
    "        z[i] = x[i]\n",
    "    for i in range(psi,len(x)):\n",
    "        x[i] = ar * x[i - 1] + gamma*e[i]\n",
    "        z[i] = x[i]\n",
    "    for i in range(psi,len(z)):\n",
    "        z[i] = z[i] + delta * sigma\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arl(ar,delta,gamma, run, length,cl) :\n",
    "    rl = np.array([], dtype=np.float64)\n",
    "\n",
    "    for i in tqdm(range(run)) :\n",
    "        y = argen(ar=ar, psi=l-1, delta=delta, gamma = gamma,length=length)\n",
    "        a = np.array([length-l])\n",
    "        x = np.zeros(shape=(length-l, l))\n",
    "        for j in range(length-l):\n",
    "            x[j] = y[j: j + l]\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j in range(0,len(x)):\n",
    "                input = x[[j]]\n",
    "\n",
    "                output = model(input)\n",
    "\n",
    "                if output[0] > cl :\n",
    "\n",
    "                    a = np.array([j + 1])\n",
    "                    break\n",
    "                elif j == len(x):\n",
    "                    a = len(x)\n",
    "\n",
    "            rl = np.append(rl,a)\n",
    "\n",
    "    arl = np.mean(rl)\n",
    "    return arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arl0(ar,delta,gamma, run, length,cl) :\n",
    "    rl = np.array([], dtype=np.float64)\n",
    "\n",
    "    for i in tqdm(range(run)) :\n",
    "        y = argen(ar=ar, psi=l-1, delta=delta, gamma = gamma,length=length)\n",
    "        a = np.array([length-l])\n",
    "        x = np.zeros(shape=(length-l, l))\n",
    "        for j in range(length-l):\n",
    "            x[j] = y[j: j + l]\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j in range(0,len(x)):\n",
    "                input = x[[j]]\n",
    "\n",
    "                output = model(input)\n",
    "\n",
    "                if output[0] > cl :\n",
    "\n",
    "                    a = np.array([j + 1])\n",
    "                    break\n",
    "                elif j == len(x):\n",
    "                    a = len(x)\n",
    "\n",
    "            rl = np.append(rl,a)\n",
    "\n",
    "    arl = np.mean(rl)\n",
    "    return arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arl0(ar,delta,gamma, run, length,cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arl1(ar,run,length,cl):\n",
    "    a5 = arl(ar, 0.5, 1, run, length, cl)\n",
    "    a1 = arl(ar, 1, 1, run, length, cl)\n",
    "    a2 = arl(ar, 2, 1, run, length, cl)\n",
    "    a3 = arl(ar, 3, 1, run, length, cl)\n",
    "    b5 = arl(ar, 0.5, 1.5,run, length, cl)\n",
    "    b1 = arl(ar, 1, 1.5, run, length, cl)\n",
    "    b2 = arl(ar, 2, 1.5, run, length, cl)\n",
    "    b3 = arl(ar, 3, 1, run, length, cl)\n",
    "    c1 = arl(ar, 0, 1.5, run, length, cl)\n",
    "    c2 = arl(ar, 0, 2, run, length, cl)\n",
    "    c3 = arl(ar, 0, 3, run, length, cl)\n",
    "    print(f'0.5: {a5}, 1:{a1},2:{a2},3:{a3}')\n",
    "    print(f'0.5:{b5},1:{b1},2:{b2},3:{b3}')\n",
    "    print(f'1.5:{c1},2:{c2},3:{c3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:56<00:00, 56.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "312.3063"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:58<00:00, 41.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423.4017"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:22<00:00, 49.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "356.1153"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:42<00:00, 44.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "391.1835"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9437)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:34<00:00, 46.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "375.4041"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:30<00:00, 47.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "370.5614"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9428)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:25<00:00, 389.52it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 791.72it/s]\n",
      "100%|██████████| 10000/10000 [00:09<00:00, 1109.36it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1244.64it/s]\n",
      "100%|██████████| 10000/10000 [00:15<00:00, 663.99it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 875.29it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1123.38it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1240.47it/s]\n",
      "100%|██████████| 10000/10000 [00:27<00:00, 366.79it/s]\n",
      "100%|██████████| 10000/10000 [00:16<00:00, 621.30it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 893.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 34.1666, 1:10.6933,2:4.2828,3:2.4727\n",
      "0.5:15.2481,1:8.4831,2:4.0109,3:2.484\n",
      "1.5:36.467,2:16.5166,3:7.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(ar=0, run=10000, length=1000, cl=0.9428)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.25 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:56<00:00, 56.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "310.5583"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.25, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:20<00:00, 31.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "564.8062"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.25, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:29<00:00, 47.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "365.472"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.25, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9525)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:31<00:00, 47.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "370.573"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.25, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9528)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:30<00:00, 331.73it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 718.76it/s]\n",
      "100%|██████████| 10000/10000 [00:09<00:00, 1062.49it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1225.60it/s]\n",
      "100%|██████████| 10000/10000 [00:16<00:00, 594.14it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 815.99it/s]\n",
      "100%|██████████| 10000/10000 [00:09<00:00, 1078.42it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1225.56it/s]\n",
      "100%|██████████| 10000/10000 [00:30<00:00, 323.16it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 565.47it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 830.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 42.4329, 1:13.1817,2:5.039,3:2.8608\n",
      "0.5:18.391,1:10.1903,2:4.7964,3:2.8483\n",
      "1.5:43.9206,2:19.9373,3:9.7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(ar=0.25, run=10000, length=1000, cl=0.9528)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.5 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:47<00:00, 93.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182.8035"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:31<00:00, 47.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "372.183"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:08<00:00, 53.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "330.7466"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:15<00:00, 51.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "339.638"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.969)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:24<00:00, 48.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "355.9155"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9695)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:26<00:00, 48.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "360.8606"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:29<00:00, 47.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "364.4805"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:30<00:00, 47.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "368.4389"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.5, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9699)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:40<00:00, 248.63it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 579.64it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 982.53it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1156.64it/s]\n",
      "100%|██████████| 10000/10000 [00:21<00:00, 475.81it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 683.36it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 971.74it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1160.14it/s]\n",
      "100%|██████████| 10000/10000 [00:38<00:00, 261.59it/s]\n",
      "100%|██████████| 10000/10000 [00:21<00:00, 471.19it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 723.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 59.1936, 1:18.5505,2:6.3225,3:3.4699\n",
      "0.5:25.0616,1:13.7797,2:6.1389,3:3.4785\n",
      "1.5:55.233,2:25.8266,3:12.9047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(ar=0.5, run=10000, length=1000, cl=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.75 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:15<00:00, 132.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125.8499"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.75, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:22<00:00, 49.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "353.9779"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.75, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:48<00:00, 34.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "509.4377"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.75, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:08<00:00, 40.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "436.7106"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.75, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9915)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:49<00:00, 43.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "417.2016"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.75, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9912)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:35<00:00, 46.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "380.2168"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.75, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9905)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:24<00:00, 48.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "370.2725"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.75, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9903)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:55<00:00, 180.75it/s]\n",
      "100%|██████████| 10000/10000 [00:24<00:00, 410.39it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 849.34it/s]\n",
      "100%|██████████| 10000/10000 [03:17<00:00, 50.70it/s] \n",
      "100%|██████████| 10000/10000 [27:02<00:00,  6.16it/s] \n",
      "100%|██████████| 10000/10000 [09:05<00:00, 18.34it/s] \n",
      "100%|██████████| 10000/10000 [18:00<00:00,  9.26it/s] \n",
      "100%|██████████| 10000/10000 [09:02<00:00, 18.43it/s] \n",
      "100%|██████████| 10000/10000 [14:31<00:00, 11.48it/s]  \n",
      "100%|██████████| 10000/10000 [00:57<00:00, 175.38it/s]\n",
      "100%|██████████| 10000/10000 [00:43<00:00, 229.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 90.3146, 1:31.6137,2:9.1701,3:4.7853\n",
      "0.5:39.1702,1:22.4344,2:9.2316,3:4.7507\n",
      "1.5:79.5065,2:40.6724,3:21.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(ar=0.75, run=10000, length=1000, cl=0.9903)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi = 0.95 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 (threshold 임의 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:15<00:00, 73.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.0806"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [11:25<00:00, 14.58it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.2365"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:00<00:00, 164.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103.1307"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:13<00:00, 136.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123.9279"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [23:42<00:00,  7.03it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "165.144"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:41<00:00, 45.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404.3203"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:07<00:00, 78.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "223.0914"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:37<00:00, 63.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "273.3544"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:21<00:00, 49.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "354.469"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.99985)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:29<00:00, 47.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "368.2834"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.99987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:38<00:00, 45.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384.7888"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.99988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:32<00:00, 47.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "374.5534"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.999875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:27<00:00, 48.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "366.579"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.999873)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [50:00<00:00,  3.33it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "378.8789"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.999874)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:27<00:00, 48.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "376.3259"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9998735)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:34<00:00, 46.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "376.1174"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9998734)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:33<00:00, 46.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "375.5979"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9998733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:30<00:00, 47.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "377.4521"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9998732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:25<00:00, 48.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "371.4345"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9998731)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:35<00:00, 46.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "375.0694"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.99987305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:26<00:00, 48.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "370.9911"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arl(ar = 0.95, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 0.9998737)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:33<00:00, 107.13it/s]\n",
      "100%|██████████| 10000/10000 [00:46<00:00, 215.85it/s]\n",
      "100%|██████████| 10000/10000 [00:16<00:00, 600.29it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 947.48it/s]\n",
      "100%|██████████| 10000/10000 [00:53<00:00, 186.69it/s]\n",
      "100%|██████████| 10000/10000 [00:34<00:00, 292.11it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 572.89it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 949.25it/s]\n",
      "100%|██████████| 10000/10000 [01:29<00:00, 111.95it/s]\n",
      "100%|██████████| 10000/10000 [00:58<00:00, 171.90it/s]\n",
      "100%|██████████| 10000/10000 [00:38<00:00, 262.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 155.3962, 1:71.3895,2:17.5483,3:6.9382\n",
      "0.5:85.0779,1:49.9915,2:19.0496,3:6.9832\n",
      "1.5:146.6575,2:90.3462,3:55.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(ar=0.95, run=10000, length=1000, cl=0.9998731)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
